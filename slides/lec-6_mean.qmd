---
title: "Week 06: Comparing Means"
subtitle: "Date: September 29, 2025"
footer:  "[course-website](https://dharaden.github.io/psyc640/)"
logo: "images/640_hex.png"
format: 
  revealjs:
    theme: clean.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    overview: false
    scrollable: true
    code-line-numbers: true
editor: visual
execute:
  echo: true
  freeze: auto
---

## Today...

HIGHLIGHT THE USE OF THE `rm()` FUNCTION TO GET RID OF THINGS IN YOUR ENVIRONMENT

```{r, results = 'hide', message = F, warning = F}
# File management
library(here)
# for dplyr, ggplot2
library(tidyverse)
# Pretty tables
library(sjPlot)
library(ggstatsplot)
# making tests easier
library(lsr) ## NEW

#Remove Scientific Notation 
options(scipen=999)
```

# Comparing Means

Converting to a t-score and checking against the t-distribution

-   One-Sample

-   Independent Sample

-   Paired Sample

------------------------------------------------------------------------

## but first...Degrees of Freedom

**Degrees of Freedom:** *the number of values in the final calculation of a statistic that are free to vary*

**Example**: Drawing a triangle

**‚ùìQuestion:** If you have a sample of 5 scores and you know the mean is 3, how many of those scores can you *freely* change?

------------------------------------------------------------------------

## One Sample t-test

*t*-tests were developed by William Sealy Gosset, who was a chemist studying the grains used in making beer. (He worked for Guinness.)

-   Specifically, he wanted to know whether particular strains of grain made better or worse beer than the standard.

-   He developed the *t*-test, to test small samples of beer against a population with an unknown standard deviation.

    -   Probably had input from Karl Pearson and Ronald Fisher

-   Published this as "Student" because Guinness didn't want these tests tied to the production of beer.

------------------------------------------------------------------------

***One-sample tests compare your given sample with a "known" population***

Research question: does this sample come from this population?

**Hypotheses**

-   $H_0$: Yes, this sample comes from this population.

-   $H_1$: No, this sample comes from a different population.

------------------------------------------------------------------------

To calculate the t-statistic, we generally use this formula:

$$t_{df=N-1} = \frac{\bar{X}-\mu}{\frac{\hat{\sigma}}{\sqrt{N}}}$$

The heavier tails of the t-distribution, especially for small N, are the penalty we pay for having to estimate the population standard deviation from the sample.

------------------------------------------------------------------------

![](/images/t-to-z.png){fig-align="center"}

------------------------------------------------------------------------

## Example

For examples today, we will use a dataset from Cards Against Humanity's Pulse of the Nation survey (<https://thepulseofthenation.com/>)

-   <a href="/files/data/CAH.csv" download="CAH.csv">Cards Against Humanity Data (.csv)</a>

```{r}
## We are using read_csv() this time
## import() was doing something strange with missing values
cah <- read_csv(here("files", "data", "CAH.csv")) %>% 
  janitor::clean_names() 

head(cah)
```

------------------------------------------------------------------------

### **Assumptions of the one-sample *t*-test**

**Normality.** We assume the sampling distribution of the mean is normally distributed. Under what two conditions can we be assured that this is true?

**Independence.** Observations in the dataset are not associated with one another. Put another way, collecting a score from Participant A doesn't tell me anything about what Participant B will say. How can we be safe in this assumption?

------------------------------------------------------------------------

### A brief example

Using the Cards Against Humanity data, we find that participants identified having approximately 22.33 ( $sd = 75.87$ ) books in their home. We know that the average household has approximately 50 books. How does this sample represent the larger united states?

**Hypotheses**

$H_0: \mu = 50$

$H_1: \mu \neq 50$

------------------------------------------------------------------------

::::: columns
::: {.column width="50%"}
$$\mu = 50$$

$$N = 1000$$

$$ \bar{X} = 22.33 $$

$$ s = 75.87 $$
:::

::: {.column width="50%"}
```{r}
t.test(x = cah$books, mu = 50,        
       alternative = "two.sided")
```
:::
:::::

------------------------------------------------------------------------

```{r}
lsr::oneSampleTTest(x = cah$books, mu = 50, 
                    one.sided = FALSE)
```

------------------------------------------------------------------------

## Cohen's D

Cohen suggested one of the most common effect size estimates---the standardized mean difference---useful when comparing a group mean to a population mean or two group means to each other.

$$\delta = \frac{\mu_1 - \mu_0}{\sigma} \approx d = \frac{\bar{X}-\mu}{\hat{\sigma}}$$

Cohen's d is in the standard deviation (Z) metric.

------------------------------------------------------------------------

Cohens's d for these data is $0.365$. In other words, the sample mean differs from the population mean by $0.365$ standard deviation units.

Cohen (1988) suggests the following guidelines for interpreting the size of d:

::: nonincremental
-   .2 = Small

-   .5 = Medium

-   .8 = Large
:::

[Cohen, J. (1988), Statistical power analysis for the behavioral sciences (2nd Ed.). Hillsdale: Lawrence Erlbaum.]{style="font-size:30px;"}

------------------------------------------------------------------------

### The usefulness of the one-sample *t*-test

How often will you conducted a one-sample *t*-test on raw data?

-   (Probably) never

How often will you come across one-sample *t*-tests?

-   (Probably) a lot!

The one-sample *t*-test is used to test coefficients in a model.

# YOUR TURN üíª

# Independent Samples t-test

Two different types: Student's & Welch's

-   Start with Student's t-test which assumes equal variances between the groups

$$ t = \frac{\bar{X_1} - \bar{X_2}}{SE(\bar{X_1} - \bar{X_2})} $$

------------------------------------------------------------------------

## Student's t-test

$$ H_0 : \mu_1 = \mu_2  \ \  H_1 : \mu_1 \neq \mu_2 $$

![](/images/student_H.png){fig-align="center"}

------------------------------------------------------------------------

### Student's t-test: Calculate SE

Are able to use a pooled variance estimate

Both variances/standard deviations are assumed to be equal

Therefore:

$$ SE(\bar{X_1} - \bar{X_2}) = \hat{\sigma} \sqrt{\frac{1}{N_1} + \frac{1}{N_2}} $$

We are calculating the **Standard Error of the Difference between means**

Degrees of Freedom: Total N - 2

------------------------------------------------------------------------

### Student's t-test

Let's try it out using the traditional `t.test()` function

Difference in Books by Belief in Ghosts

```{r}
t.test(formula = books ~ ghosts,
       data = cah, var.equal = TRUE )
```

------------------------------------------------------------------------

## Welch's t-test

$$ H_0 : \mu_1 = \mu_2  \ \  H_1 : \mu_1 \neq \mu_2 $$

![](/images/welch_H.png){fig-align="center"}

------------------------------------------------------------------------

### Welch's t-test: Calculate SE

Since the variances are not equal, we have to estimate the SE differently

$$ SE(\bar{X_1} - \bar{X_2}) = \sqrt{\frac{\hat{\sigma_1^2}}{N_1} + \frac{\hat{\sigma_2^2}}{N_2}} $$

Degrees of Freedom is also very different:

![](/images/welch_df.png){fig-align="center" width="380"}

------------------------------------------------------------------------

### Welch's t-test: In R (classic)

Let's try it out using the traditional `t.test()` function...turns out it is pretty straightforward

```{r}
t.test(formula = books ~ ghosts,
       data = cah, var.equal = FALSE )
```

------------------------------------------------------------------------

## Cool Visualizations

The library [ggstatsplot](https://indrajeetpatil.github.io/ggstatsplot/) has some wonderful visualizations of various tests

```{r}
#| code-fold: true

ggstatsplot::ggbetweenstats(   
  data  = cah,   
  x     = ghosts,   
  y     = books,   
  title = "Distribution of books by belief in ghosts" )

```

------------------------------------------------------------------------

### Interpreting and writing up an independent samples t-test

::: {style="font-size: 30px"}
The first sentence usually conveys some descriptive information about the two groups you were comparing. Then you identify the type of test you conducted and what was determined (be sure to include the "stat block" here as well with the t-statistic, df, p-value, CI and Effect size). Finish it up by putting that into person words and saying what that means.
:::

> ::: {style="font-size: 30px"}
> The mean amount of books in the household for the group who did not believe in ghosts was 19.9 (SD = 61.2), while the mean for those who believed in ghosts was 26.8 (SD = 96.4). A Student's independent samples t-test showed that there was not a significant mean difference (*t*(951)=-1.364, *p*=.17, $CI_{95}$=\[-16.98, 3.05\]). This suggests that there is no difference in amount of books in their household as a function of belief in ghosts.
> :::

# YOUR TURN üíª

------------------------------------------------------------------------

## Paired Samples $t$-Test

[Chapter 13.5 - Learning Stats with R](https://learningstatisticswithr.com/book/ttest.html#pairedsamplesttest)

Also called "Dependent Samples t-test"

-   We have been testing means between two *independent* samples. Participants may be randomly assigned to the separate groups

    -   This is limited to those types of study designs, but what if we have repeated measures?

-   We will then need to compare scores across people...the samples we are comparing now *depend* on one another and are *paired*

------------------------------------------------------------------------

### Paired Samples $t$-test

Each of the repeated measures (or pairs) can be viewed as a *difference score*

This reduces the analysis to a one-sample t-test of the *difference score*

-   We are comparing the sample (i.e., difference scores) to a population $\mu$ = 0

------------------------------------------------------------------------

### Assumptions: Paired Samples

The variable of interest (*difference scores*):

-   Continuous (Interval/Ratio)

-   Have 2 groups (and only two groups) that are matched

-   Normally Distributed

------------------------------------------------------------------------

### Why paired samples??

Previously, we looked at independent samples $t$-tests, which we *could* do here as well (nobody will yell at you)

-   However, this would violate the assumption that the data points are independent of one another!

-   Within vs. Between-subjects

------------------------------------------------------------------------

### Within vs. Between Subjects

![](/images/between-subjects-design.avif){fig-align="center"}

------------------------------------------------------------------------

### Paired Samples: Single Sample

Instead of focusing on these variables as being separate/independent, we need to be able to account for their dependency on one another

This is done by calculating a *difference* or *change* score for each participant

$$ D_i = X_{i1} - X_{i2} $$

Notice: The equation is set up as `variable1` minus `variable2`. This will be important when we interpret the results

------------------------------------------------------------------------

### Paired Samples: Hypotheses & $t$-statistic

The hypotheses would then be:

$$ H_0:  \mu_D = 0; H_1: \mu_D \neq 0 $$

And to calculate our t-statistic: ¬†¬†¬†¬†¬†¬†$t_{df=n-1} = \frac{\bar{D}}{SE(D)}$

where the Standard Error of the difference score is: ¬†¬†¬†¬†¬†¬† $\frac{\hat{\sigma_D}}{\sqrt{N}}$

------------------------------------------------------------------------

## Review of the t-test process

1.  Collect Sample and define hypotheses

2.  Set alpha level

3.  Determine the sampling distribution ($t$ distribution for now)

4.  Identify the critical value that corresponds to alpha and *df*

5.  Calculate test statistic for sample collected

6.  Inspect & compare statistic to critical value; Calculate probability

# Example 1: Simple (by hand)

Participants are placed in two differently colored rooms (counterbalanced) and are asked to rate overall happiness levels after spending 5 minutes inside the rooms. There are no windows, but there is a nice lamp and chair.

Hypotheses:

-   $H_0:$ There is no difference in ratings of happiness between the rooms ( $\mu = 0$ )

-   $H_1:$ There is a difference in ratings of happiness between the rooms ( $\mu \neq 0$ )

------------------------------------------------------------------------

| Participant | Blue Room Score | Orange Room Score | Difference ($X_{iB} - X_{iO}$) |
|:----------------:|:----------------:|:----------------:|:------------------:|
| 1 | 3 | 6 | -3 |
| 2 | 9 | 9 | 0 |
| 3 | 2 | 10 | -8 |
| 4 | 9 | 6 | 3 |
| 5 | 5 | 2 | 3 |
| 6 | 5 | 7 | -2 |

```{r}
#| code-fold: true 

ex1 <- data.frame(id = c(1,2,3,4,5,6),
                  blue = c(3,9,2,9,5,5),    
                  orange = c(6,9,10,6,2,7),   
                  diff_score = c(-3, 0, -8, 3, 3, -2))
  head(ex1)
```

------------------------------------------------------------------------

## Determining $t$-crit

Can look things up using a [t-table](https://www.tdistributiontable.com/) where you need the degrees of freedom and the alpha

But we have R to do those things for us:

```{r}
#the qt() function is for a 1 tailed test, so we are having to divide it in half to get both tails 

alpha <- 0.05 
n <- nrow(ex1) 
t_crit <- qt(alpha/2, n-1) 
t_crit
```

## Calculating t

Let's get all of the information for the sample we are focusing on (difference scores):

```{r}
d <- mean(ex1$diff_score) 
d 
sd_diff <- sd(ex1$diff_score) 
sd_diff
```

## Calculating t

Now we can calculate our $t$-statistic: $$t_{df=n-1} = \frac{\bar{D}}{\frac{sd_{diff}}{\sqrt{n}}}$$

```{r}
t_stat <- d/(sd_diff/(sqrt(n))) 
t_stat  

#Probability of this t-statistic  
p_val <- pt(t_stat, n-1)*2 
p_val
```

## Make a decision

Hypotheses:

::: nonincremental
-   $H_0:$ There is no difference in ratings of happiness between the rooms ( $\mu = 0$ )

-   $H_1:$ There is a difference in ratings of happiness between the rooms ( $\mu \neq 0$ )
:::

| $alpha$ |          $t-crit$          |    $t-statistic$     |      $p-value$      |
|:----------------:|:-----------------:|:----------------:|:----------------:|
|  0.05   | $\pm$ `r round(t_crit, 2)` | `r round(t_stat, 2)` | `r round(p_val, 2)` |

**What can we conclude??**

# Example 2: Data in R

```{r}

state_school <- read_csv("https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/data/NM-NY_CAS.csv") %>% 
  #create an ID variable
  rowid_to_column("id")
```

------------------------------------------------------------------------

## Let's Look at the data

Research Question: Is there a difference between school nights and weekend nights for amount of time slept?

Only looking at the variables that we are potentially interested in:

```{r}
state_school %>%    
  select(id, Gender, Ageyears, Sleep_Hours_Schoolnight, Sleep_Hours_Non_Schoolnight) %>%    
  head() #look at first few observations
```

------------------------------------------------------------------------

### Difference Score

::: {style="font-size: 30px"}
This can be done in a couple different ways and sometimes you will see things computed this way:
:::

```{r, eval = FALSE}

state_school$sleep_diff <- state_school$Sleep_Hours_Schoolnight - state_school$Sleep_Hours_Non_Schoolnight
```

::: {style="font-size: 30px"}
However, we like the `tidyverse` so why don't we use the `mutate()` function

And I always overdo things, so I am going to make a new dataset that only has the variables that I'm interested in (`sleep_state_school`)
:::

```{r}
#| code-fold: true 

sleep_state_school <- state_school %>%    
  mutate(sleep_diff = Sleep_Hours_Schoolnight - Sleep_Hours_Non_Schoolnight) %>%   
  select(id, Gender, Ageyears, Sleep_Hours_Schoolnight,          Sleep_Hours_Non_Schoolnight, sleep_diff)  

head(sleep_state_school)
```

------------------------------------------------------------------------

### Visualizing

Now that we have our variable of interest, let's take a look at it!

```{r}
sleep_state_school %>%   
  ggplot(aes(sleep_diff)) +    
  geom_histogram()
```

## Doing the test in R: One Sample

Since we have calculated the difference scores, we can basically just do a one-sample t-test with the `lsr` library

```{r}
oneSampleTTest(sleep_state_school$sleep_diff, mu = 0)
```

## Doing the test in R: Paired Sample

Maybe we want to keep things separate and don't want to calculate separate values. We can use `pairedSamplesTTest()` instead! 

But this isn't working and is making me mad...

```{r, eval= FALSE}
lsr::pairedSamplesTTest(
  formula = ~ Sleep_Hours_Schoolnight + Sleep_Hours_Non_Schoolnight,
  data = sleep_state_school
)
```


## Doing the test in R: Classic Edition

As you Google around to figure things out, you will likely see folks using `t.test()`

```{r}
t.test(x = sleep_state_school$Sleep_Hours_Schoolnight,    
       y = sleep_state_school$Sleep_Hours_Non_Schoolnight,
       paired = TRUE )
```

## Reporting $t$-test

The first sentence usually conveys some descriptive information about the sample you were comparing (e.g., pre & post test).

Then you identify the type of test you conducted and what was determined (be sure to include the "stat block" here as well with the t-statistic, df, p-value, CI and Effect size).

Finish it up by putting that into person words and saying what that means.

# YOUR TURN üíª

# ANOVA (Analysis of Variance)

## What is ANOVA? ([LSR Ch. 14](https://learningstatisticswithr.com/book/anova.html))

::: incremental
-   ANOVA stands for [***An***]{.underline}alysis [***o***]{.underline}f [***Va***]{.underline}riance
-   Comparing means between two or more groups (usually 3 or more)
    -   Continuous outcome and grouping variable with 2 or more levels
-   Under the larger umbrella of general linear models
    -   ANOVA is basically a regression with only categorical predictors
-   Likely the most widely used tool in Psychology
:::

## Different Types of ANOVA

-   ::: {.fragment .highlight-green}
    One-Way ANOVA
    :::

-   Two-Way ANOVA

-   Repeated Measures ANOVA

-   ANCOVA

-   MANOVA

## One-Way ANOVA

Goal: Inform of differences among the levels of our variable of interest (Omnibus Test)

-   But cannot tell us more than that...

Hypotheses:

$$ H_0: it\: is\: true\: that\: \mu_1 = \mu_2 = \mu_3 =\: ...\mu_k  \\  H_1: it\: is\: \boldsymbol{not}\: true\: that\: \mu_1 = \mu_2 = \mu_3 =\: ...\mu_k $$

## Wait...Means or Variance?

We are using the variance to create a ratio (within group versus between group variance) to determine differences in means

-   We are not directly investigating variance, but operationalize variance to create the ratio:

$$ F_{df_b, \: df_w} = \frac{MS_{between}}{MS_{within}} $$

------------------------------------------------------------------------

![](/images/ANOVA.png){width="729"}

------------------------------------------------------------------------

![](/images/ANOVA.png){width="729"}

::::: columns
::: {.column width="50%"}
$F = \frac{MS_{between}}{MS_{within}} = \frac{small}{large} < 1$
:::

::: {.column width="50%"}
$F = \frac{MS_{between}}{MS_{within}} = \frac{large}{small} > 1$
:::
:::::

## ANOVA: Assumptions

::: {style="font-size: 30px"}
**Independence**

-   Observations between and within groups should be independent. No autocorrelation

**Homogeneity of Variance**

-   The variances within each group should be roughly equal
    -   Levene's test --\> Welch's ANOVA for unequal variances

**Normality**

-   The data within each group should follow a normal distribution
    -   Shapiro-Wilk test --\> can transform the data or use non-parametric tests
:::

# NHST with ANOVA

## Review of the NHST process

::: incremental
1.  Collect Sample and define hypotheses

2.  Set alpha level

3.  Determine the sampling distribution (will be using $F$-distribution now)

4.  Identify the critical value

5.  Calculate test statistic for sample collected

6.  Inspect & compare statistic to critical value; Calculate probability
:::

## Steps to calculating F-ratio

1.  Capture variance both between and within groups
2.  Variance to Sum of Squares
3.  Degrees of Freedom
4.  Mean squares values
5.  F-Statistic

## Capturing Variance

We have calculated variance before!

$$ Var = \frac{1}{N}\sum(x_i - \bar{x})^2 $$

Now we have to take into account the variance between and within the groups:

$$ Var(Y) = \frac{1}{N} \sum^G_{k=1}\sum^{N_k}_{i=i}(Y_{ik} - \bar{Y})^2 $$

::: {style="font-size: 20px; text-align: center"}
Notice that we have the summation across each group ( $G$ ) and the person in the group ( $N_k$ )
:::

------------------------------------------------------------------------

## Variance to Sum of Squares

**Total Sum of Squares -** Adding up the sum of squares instead of getting the average (notice the removal of $\frac{1}{N}$)

$$ SS_{total} = \sum^G_{k=1}\sum^{N_k}_{i=i}(Y_{ik} - \bar{Y})^2 $$

Can be broken up to see what is the variation ***between*** the groups AND the variation ***within*** the groups

$$ SS_{total}=SS_{between}+SS_{within} $$

::: {style="font-size: 20px; text-align: center"}
This gets us closer to understanding the difference between means
:::

# Sum of Squares

------------------------------------------------------------------------

### Sum of Squares

$$ SS_{total}=SS_{between}+SS_{within} $$

![](/images/var_ss.PNG){fig-align="center"}

------------------------------------------------------------------------

### Example Data

![](/images/table1.PNG){fig-align="center"}

------------------------------------------------------------------------

### Example Data

![](/images/table2.PNG){fig-align="center"}

------------------------------------------------------------------------

### Example Data

![](/images/table3.PNG){fig-align="center"}

------------------------------------------------------------------------

### Example Data

![](/images/table4.PNG){fig-align="center"}

------------------------------------------------------------------------

## Sum of Squares - Between

The difference between the *group mean* and *grand mean*

$$ SS_{between} = \sum^G_{k=1}N_k(\bar{Y_k} - \bar{Y})^2 $$

| Group  | Group Mean $\bar{Y_k}$ | Grand Mean $\bar{Y}$ |
|:------:|:----------------------:|:--------------------:|
|  Cool  |           32           |         41.8         |
| Uncool |          56.5          |         41.8         |

------------------------------------------------------------------------

## Sum of Squares - Between

The difference between the *group mean* and *grand mean*

$$ SS_{between} = \sum^G_{k=1}N_k(\bar{Y_k} - \bar{Y})^2 $$

| Group | Group Mean $\bar{Y_k}$ | Grand Mean $\bar{Y}$ | Sq. Dev. | N | Weighted Sq. Dev. |
|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|
| Cool | 32 | 41.8 | 96.04 | 3 | 288.12 |
| Uncool | 56.5 | 41.8 | 216.09 | 2 | 432.18 |

------------------------------------------------------------------------

## Sum of Squares - Between

The difference between the *group mean* and *grand mean*

$$ SS_{between} = \sum^G_{k=1}N_k(\bar{Y_k} - \bar{Y})^2 $$

Now we can sum the Weighted Squared Deviations together to get our Sum of Squares Between:

```{r}
ssb <- 288.12 + 432.18 
ssb
```

------------------------------------------------------------------------

## Sum of Squares - Within

The difference between the [*individual*]{.underline} and their [*group mean*]{.underline}

$$ SS_{within} = \sum^G_{k=1}\sum^{N_k}_{i=i}(Y_{ik} - \bar{Y_k})^2 $$

|    Name    | Grumpiness $Y_{ik}$ | Group Mean $\bar{Y_K}$ |
|:----------:|:-------------------:|:----------------------:|
|   Frodo    |         20          |           32           |
|    Sam     |         55          |           32           |
|   Bandit   |         21          |           32           |
| Dolores U. |         91          |          56.5          |
|   Dustin   |         22          |          56.5          |

------------------------------------------------------------------------

## Sum of Squares - Within

The difference between the [*individual*]{.underline} and their [*group mean*]{.underline}

$$ SS_{within} = \sum^G_{k=1}\sum^{N_k}_{i=i}(Y_{ik} - \bar{Y_k})^2 $$

|    Name    | Grumpiness $Y_{ik}$ | Group Mean $\bar{Y_K}$ | Sq. Dev |
|:----------:|:-------------------:|:----------------------:|---------|
|   Frodo    |         20          |           32           | 144     |
|    Sam     |         55          |           32           | 529     |
|   Bandit   |         21          |           32           | 121     |
| Dolores U. |         91          |          56.5          | 1190.25 |
|   Dustin   |         22          |          56.5          | 1190.25 |

```{r}
#| code-fold: true 

score <- c(20, 55, 21, 91, 22) 
group_m <- c(32, 32, 32, 56.5, 56.5) 
sq_dev <- (score - group_m)^2
```

------------------------------------------------------------------------

## Sum of Squares - Within

The difference between the [*individual*]{.underline} and their [*group mean*]{.underline}

$$ SS_{within} = \sum^G_{k=1}\sum^{N_k}_{i=i}(Y_{ik} - \bar{Y_k})^2 $$ Now we can sum the Squared Deviations together to get our Sum of Squares Within:

```{r}
sum(sq_dev)
```

## Sum of Squares

Can start to have an idea of what this looks like

$$ SS_{between} = \sum^G_{k=1}N_k(\bar{Y_k} - \bar{Y})^2 = 720.3 $$

$$ SS_{within} = \sum^G_{k=1}\sum^{N_k}_{i=i}(Y_{ik} - \bar{Y_k})^2 = 3174.5 $$

Next we have to take into account the degrees of freedom

# Degrees of Freedom - ANOVA

------------------------------------------------------------------------

## Degrees of Freedom

Since we have 2 types of variations that we are examining, this needs to be reflected in the degrees of freedom

1.  Take the number of groups and subtract 1\
    $df_{between} = G - 1$

2.  Take the total number of observations and subtract the number of groups

    $df_{within} = N - G$

# Mean Squares

------------------------------------------------------------------------

## Calculating Mean Squares

Next we convert our summed squares value into a "mean squares"

This is done by dividing by the respective degrees of freedom

$$ MS_b = \frac{SS_b}{df_b} $$

$$ MS_W = \frac{SS_w}{df_w} $$

------------------------------------------------------------------------

## Calculating Mean Squares - Example

Let's take a look at how this applies to our example: $$ MS_b = \frac{SS_b}{G-1} = \frac{720.3}{2-1} = 720.3 $$

$$ MS_W = \frac{SS_w}{N-G} = \frac{3174.5}{5-2} = 1058.167  $$

# $F$-Statistic

------------------------------------------------------------------------

## Calculating the F-Statistic

$$F = \frac{MS_b}{MS_w}$$

If the null hypothesis is true, $F$ has an expected value close to 1 (numerator and denominator are estimates of the same variability)

If it is false, the numerator will likely be larger, because systematic, between-group differences contribute to the variance of the means, but not to variance within group.

------------------------------------------------------------------------

```{r, fig.width = 10, fig.height = 6}
#| code-fold: true 

data.frame(F = c(0,8)) %>%   
  ggplot(aes(x = F)) +   
  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196),                  geom = "line") +   
  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196),                  geom = "area", xlim = c(2.65, 8), fill = "purple") +   geom_vline(aes(xintercept = 2.65), color = "purple") +   scale_y_continuous("Density") + scale_x_continuous("F statistic", breaks = NULL) +   theme_bw(base_size = 20)
```

If data are normally distributed, then the variance is $\chi^2$ distributed

$F$-distributions are one-tailed tests. Recall that we're interested in how far away our test statistic from the null $(F = 1).$

------------------------------------------------------------------------

## Calculating F-statistic: Example

$$F = \frac{MS_b}{MS_w} = \frac{720.3}{1058.167} = 0.68$$

[Link](http://courses.atlas.illinois.edu/spring2016/STAT/STAT200/pf.html) to probability calculator

------------------------------------------------------------------------

```{r fig.width = 10, fig.height = 6}
#| code-fold: true #|  

data.frame(F = c(0,8)) %>%   ggplot(aes(x = F)) +   stat_function(fun = function(x) df(x, df1 = 3, df2 = 196),                  geom = "line") +   stat_function(fun = function(x) df(x, df1 = 3, df2 = 196),                  geom = "area", xlim = c(2.65, 8), fill = "purple") +   geom_vline(aes(xintercept = 2.65), color = "purple") +    geom_vline(aes(xintercept = 0.68), color = "red") +    annotate("text",            label = "F=0.68",             x = 1.1, y = 0.65, size = 8, color = "red") +    scale_y_continuous("Density") + scale_x_continuous("F statistic", breaks = NULL) +   theme_bw(base_size = 20)
```

What can we conclude?

------------------------------------------------------------------------

## Contrasts/Post-Hoc Tests

Performed when there is a significant difference among the groups to examine which groups are different

1.  **Contrasts**: When we have *a priori* hypotheses
2.  **Post-hoc Tests**: When we want to test everything

# Reporting Results

------------------------------------------------------------------------

## Tables

Often times the output will be in the form of a table and then it is often reported this way in the manuscript

| Source of Variation | df | Sum of Squares | Mean Squares | F-statistic | p-value |
|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|
| Group | $G-1$ | $SS_b$ | $MS_b = \frac{SS_b}{df_b}$ | $F = \frac{MS_b}{MS_w}$ | $p$ |
| Residual | $N-G$ | $SS_w$ | $MS_w = \frac{SS_w}{df_w}$ |  |  |
| Total | $N-1$ | $SS_{total}$ |  |  |  |

------------------------------------------------------------------------

## In-Text

> A one-way analysis of variance was used to test for differences in the \[variable of interest/outcome variable\] as a function of \[whatever the factor is\]. Specifically, differences in \[variable of interest\] were assessed for the \[list different levels and be sure to include (M= , SD= )\] . The one-way ANOVA revealed a significant/nonsignificant effect of \[factor\] on scores on the \[variable of interest\] (F(dfb, dfw) = f-ratio, p = p-value, Œ∑2 = effect size).
>
> Planned comparisons were conducted to compare expected differences among the \[however many groups\] means. Planned contrasts revealed that participants in the \[one of the conditions\] had a greater/fewer \[variable of interest\] and then include the p-value. This same type of sentence is repeated for whichever contrasts you completed. Descriptive statistics were reported in Table 1.

---
title: "Week 05: Correlation & Effect Sizes"
subtitle: "Date: September 22, 2025"
footer:  "[course-website](https://dharaden.github.io/psyc640/)"
logo: "images/640_hex.png"
format: 
  revealjs:
    theme: clean.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    overview: false
    scrollable: true
    code-line-numbers: true
editor: visual
execute:
  echo: true
  freeze: auto
---

## Today...

Explore hypotheses, correlations and effect sizes

```{r, results = 'hide', message = F, warning = F}
# File management
library(here)
# for dplyr, ggplot2
library(tidyverse)
# Loading data
library(rio)
# Pretty tables
library(sjPlot)
library(kableExtra)

#Remove Scientific Notation 
options(scipen=999)
```

# The Lady Tasting Tea 🍵

How A Simple Experiment Paved the Way for Modern Experimental Design 📊

Taken from [Tweet](https://x.com/selcukorkmaz/status/1690348343363334144){target="_blank"} by Selçuk Korkmaz

------------------------------------------------------------------------

## Setting:

In 1925, during a summer afternoon on campus, a lady, Dr. Muriel Bristol ([who has an algae species named after her](https://en.wikipedia.org/wiki/Chlamydomonas)), was handed a cup of tea by Ronald Fisher. She declined saying:

*"I prefer the flavor when the milk is poured first. I can tell when there is a difference"*

Fisher, being a statistician and a white man:

*"Prove it"*

::: callout-note
*Sometimes, groundbreaking insights arise from everyday claims!*
:::

------------------------------------------------------------------------

## Designing the Test:

Fisher prepared 8 cups of tea; 4 with milk first & 4 with tea first

Dr. Muriel Bristol *correctly* identified 3 out of 4 of each! (some reports claim she identified all correctly)

*Was it just chance or genuine ability?*

::: callout-note
Always consider both outcomes. In hypothesis testing, this means setting up null and alternative hypotheses.
:::

------------------------------------------------------------------------

## Birth of Hypothesis Testing:

Fisher framed it as a combinatorial problem

If it was mere luck/chance, the *probability* of getting all 8 correct was low

This way of thinking is the groundwork for the concept of the p-value

::: callout-important
The p-value gives the probability of observing data (or something more extreme) given that the null hypothesis is true.
:::

# NHST

The core idea of Null Hypothesis Significance Testing (NHST) is a bit backward, but powerful:

> We can't prove our research hypothesis is true. Instead, we gather evidence to show that the alternative explanation (the null hypothesis) is incredibly unlikely

It's a process of elimination and inference

------------------------------------------------------------------------

## The Tortured Logic of NHST

We create two hypotheses, $H_0$ and $H_1$. Usually, we care about $H_1$, not $H_0$. In fact, what we really want to know is how likely $H_1$, given our data.

$$P(H_1|Data)$$ Instead, we're going to test our null hypothesis. Well, not really. We're going to assume our null hypothesis is true, and test how likely we would be to get these data.

$$P(Data|H_0)$$

------------------------------------------------------------------------

## NHST Analogy: The Legal System

-   The **Null Hypothesis (** $H_0$ **)** is the starting assumption: "**presumed innocent**." In research, this means assuming there is *no effect*, *no relationship*, or *no difference*.

-   **You, the researcher**, are the prosecution, gathering evidence (data) to challenge this presumption.

-   Your **p-value** reflects the strength of your evidence. A small p-value *likely* means your evidence is strong.

-   **Rejecting the null** is like a "guilty" verdict. You have enough evidence to say the initial presumption of innocence (no effect) is unlikely to be true.

------------------------------------------------------------------------

## p-value: Formal Definition

This is one of the most important—and misunderstood—concepts in statistics.

The **p-value** is:

> The probability of observing a result **as extreme as, or more extreme than**, the one we actually observed, **assuming the null hypothesis is true.**

------------------------------------------------------------------------

## p-value: Formal Definition

This is one of the most important—and misunderstood—concepts in statistics.

**It is NOT:**

-   The probability that the null hypothesis is true

-   The probability that our research hypothesis is true

-   A measure of the size or importance of an effect

------------------------------------------------------------------------

## What are the steps of NHST?

::::: columns
::: {.column width="50%"}
1.  Define null and alternative hypothesis.

2.  Set and justify alpha level (usually $\alpha$ = .05)

3.  Determine which sampling distribution ( $z$, $t$, or $\chi^2$ for now)

4.  Calculate parameters of your sampling distribution under the null.

-   If $z$, calculate $\mu$ and $\sigma_M$
:::

::: {.column width="50%"}
5.  Calculate test statistic under the null.

-   If $z$, $\frac{\bar{X} - \mu}{\sigma_M}$

6.  Calculate probability of that test statistic or more extreme under the null, and compare to alpha.
:::
:::::

------------------------------------------------------------------------

## 🧠 Think about...

> You run a correlational analysis to see if levels of anxiety are related to caffeine consumption. You get a correlation of 0.38 and a p-value of *p =* .045.

Turn to someone next to you and in your own words, explain what the p-value is telling you.

-   Looking at the p-value, what can you conclude?

-   Looking at the p-value, what can't you conclude?

------------------------------------------------------------------------

## 🧠More thinking...

What if you ran the same study and analyses, but found that your p-value was *p =* .052?

# Visualizations of Concepts

[NHST Visualizations](https://rpsychologist.com/d3/nhst/){target="_blank"}

[Sampling Distributions - OG](https://onlinestatbook.com/stat_sim/sampling_dist/){target="_blank"}

[Sampling Distributions & p-values](https://rpsychologist.com/pvalue/){target="_blank"}

# Break ☕stopped here at the break

------------------------------------------------------------------------

## Import cleaned up file

Last class we created a new .csv file named `named_Sleep_Data`. We will continue to use that! Let's make sure to import that data.

```{r}
# Remember that your path will probably be different
#sleep_data <- read.csv(here("lectures", 
#                            "data", 
#                            "named_Sleep_Data.csv"))

```

# Relationships between variables

## Association - Correlation

Examine the relationship between two continuous variables

Similar to the mean and standard deviation, but it is *between* two variables

Typically displayed as a `scatterplot`

------------------------------------------------------------------------

```{r}
#| code-fold: true

set.seed(42)

n <- 200
x <- rnorm(n, mean = 10, sd = 2)
y <- 2 * x + rnorm(n, mean = 0, sd = 2) 
corr_data <- data.frame(x,y)

corr_data %>% 
  ggplot(aes(x,y)) + 
  geom_point() + 
  geom_smooth(method="lm", 
              se = FALSE) + 
  labs(
    x = "Number of Houses", 
    y = "Amount of Candy"
  )


```

------------------------------------------------------------------------

## Association - Covariance

Before we talk about correlation, we need to take a look at covariance

$$
cov_{xy} = \frac{\sum(x-\bar{x})(y-\bar{y})}{N-1}
$$

::: incremental
-   Covariance can be thought of as the "average cross product" between two variables

-   It captures the *raw/unstandardized* relationship between two variables

-   Covariance matrix is the basis for many statistical analyses
:::

------------------------------------------------------------------------

## Covariance to Correlation

The Pearson correlation coefficient $r$ addresses this by standardizing the covariance

It is done in the same way that we would create a $z-score$...by dividing by the standard deviation

$$
r_{xy} = \frac{Cov(x,y)}{sd_x sd_y}
$$

------------------------------------------------------------------------

## Correlations

-   Tells us: How much 2 variables are *linearly* related

-   Range: -1 to +1

-   Most common and basic effect size measure

-   Is used to build the regression model

------------------------------------------------------------------------

### Interpreting Correlations ([5.7.5](https://learningstatisticswithr.com/book/descriptives.html#interpretingcorrelations))

| Correlation  |  Strength   | Direction |
|:------------:|:-----------:|:---------:|
| -1.0 to -0.9 | Very Strong | Negative  |
| -0.9 to -0.7 |   Strong    | Negative  |
| -0.7 to -0.4 |  Moderate   | Negative  |
| -0.4 to -0.2 |    Weak     | Negative  |
|  -0.2 to 0   | Negligible  | Negative  |
|   0 to 0.2   | Negligible  | Positive  |
|  0.2 to 0.4  |    Weak     | Positive  |
|  0.4 to 0.7  |  Moderate   | Positive  |
|  0.7 to 0.9  |   Strong    | Positive  |
|  0.9 to 1.0  | Very Strong | Positive  |

# Pearson Correlations in R

## Calculating Correlation in R

Now how do we get a correlation value in R?

```{r}
cor(corr_data$x, corr_data$y)

```

That will give us the correlation, but we also want to know how to get our p-value

------------------------------------------------------------------------

### Correlation Test

To get the test of a single pair of variables, we will use the `cor.test()` function:

```{r}
cor.test(x, y, data = corr_data)
```

------------------------------------------------------------------------

## Using real data - Epworth Sleepiness Scale

So far we have been looking at single variables, but we often care about the relationships between multiple variables in a dataset

```{r}
#| code-fold: true

#sleep_data %>% 
#  select(ESS1m1:ESS8m1) %>% 
#  cor() %>% 
#  kable()
```

# Missing Values - `na.rm = TRUE`?

------------------------------------------------------------------------

## Handling Missing - Correlation

-   Listwise Deletion (complete cases)

    ::: incremental
    -   Removes participants completely if they are missing a value being compared
    -   Smaller Sample Sizes
    -   Doesn't bias correlation estimate
    :::

-   Pairwise Deletion

    ::: incremental
    -   Removes participants for that single pair, but leaves information in when there are complete information
    -   Larger Sample Sizes
    -   Could bias estimates if there is a systematic reason things are missing
    :::

------------------------------------------------------------------------

```{r}
#sleep_data %>% 
#  select(ESS1m1:ESS8m1) %>% 
#  cor(use = "complete") %>% 
#  kable()
```

------------------------------------------------------------------------

```{r}
#sleep_data %>% 
#  select(ESS1m1:ESS8m1) %>% 
#  cor(use = "pairwise") %>% 
#  kable()
```

# Spearman's Rank Correaltion

------------------------------------------------------------------------

## Spearman's Rank Correlation

We need to be able to capture this different (ordinal) "relationship"

-   If student 1 works more hours than student 2, then we can guarantee that student 1 will get a better grade

Instead of using the amount given by the variables ("hours studied"), we rank the variables based on least (rank = 1) to most (rank = 10)

Then we correlate the rankings with one another

# Foundations of Statistics

Who were those white dudes that started this?

------------------------------------------------------------------------

## Statistics and Eugenics

The concept of the correlation is primarily attributed to Sir Frances Galton

-   He was also the founder of the [concept of eugenics](https://www.theguardian.com/commentisfree/2019/oct/03/eugenics-francis-galton-science-ideas)

The correlation coefficient was developed by his student, [Karl Pearson](https://www.britannica.com/biography/Karl-Pearson), and adapted into the ANOVA framework by [Sir Ronald Fisher](https://statmodeling.stat.columbia.edu/2020/08/01/ra-fisher-and-the-science-of-hatred/)

-   Both were prominent advocates for the eugenics movement

------------------------------------------------------------------------

## What do we do with this info?

::: incremental
-   Never use the correlation or the later techniques developed on it? Of course not.

-   Acknowledge this history? Certainly.

-   [Understand how the perspectives](https://medium.com/swlh/is-statistics-racist-59cd4ddb5fa9) of Galton, Fisher, Pearson and others [shaped our practices](http://gppreview.com/2019/12/16/eugenics-ethics-statistical-analysis/)? We must! -- these are not set in stone, [nor are they necessarily the best way](https://www.forbes.com/sites/jerrybowyer/2016/01/06/beer-vs-eugenics-the-good-and-the-bad-uses-of-statistics/?sh=3114a0c82a14) to move forward.
:::

------------------------------------------------------------------------

## Be aware of the assumptions

::: incremental
-   Statistics are often thought of as being absent of bias...they are just numbers

-   Statistical significance was a way to avoid talking about nuance or degree.

-   "Correlation does not imply causation" was a refutation of work demonstrating associations between environment and poverty.

-   Need to be particularly mindful of our goals as scientists and how they can influence the way we interpret the findings
:::

# Fancy Tables

------------------------------------------------------------------------

## Correlation Tables

Before we used the `cor()` function to create a correlation matrix of our variables

***But what is missing?***

```{r}
#sleep_data %>% 
#  select(ESS1m1:ESS8m1) %>% 
#  cor(use = "complete") %>% 
#  kable()
```

## Correlation Tables - [sjPlot](https://strengejacke.github.io/sjPlot/reference/tab_corr.html)

```{r}
#sleep_data %>% 
#  select(ESS1m1:ESS8m1) %>% 
#  cor(use = "complete") %>% 
#  tab_corr(na.deletion = "listwise", triangle = "lower")
```

------------------------------------------------------------------------

## Correlation Tables - [sjPlot](https://strengejacke.github.io/sjPlot/reference/tab_corr.html)

So many different cusomizations for this type of plot

Can add titles, indicate what missingness and method

Saves you a TON of time when putting it into a manuscript

------------------------------------------------------------------------

## Writing up a Correlation

Template: *r*(degress of freedom) = the *r* statistic, *p* = *p* value.

Imagine we have conducted a study of 40 students that looked at whether IQ scores and GPA are correlated. We might report the results like this:

> IQ and GPA were found to be moderately positively correlated, *r*(38) = .34, *p* = .032.

Other example:

> Among the students of Hogwarts University, the number of hours playing Fortnite per week and midterm exam results were negatively correlated, *r*(78) = -.45, *p* \< .001.

And another:

> Table 1 reports descriptive statistics and correlations among variables of interest. Knowledge of Weird Al Songs was positively correlated with perceptions of humor for Dr. Haraden (*r*(49) = .79, *p \<*.001), such that the more Weird Al songs a student knew, the more they thought Dr. Haraden was funny.

---
title: "Week 12: Categorical & Logistic Regression"
subtitle: "Date: November 10, 2025"
footer:  "[course-website](https://dharaden.github.io/psyc640/)"
logo: "images/640_hex.png"
format: 
  revealjs:
    theme: clean.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    overview: false
    scrollable: true
    code-line-numbers: true
editor: visual
execute:
  echo: true
  freeze: auto
---

## Today...

-   Some helpful tools (autosave & new library)

-   Regression Review

-   Categorical Predictors Review

-   Logistic Regression

-   Model Diagnostics

------------------------------------------------------------------------

```{r, results = 'hide', message = F, warning = F}

library(tidyverse)
library(rio)
library(here)
library(lm.beta)
library(easystats)
library(sjPlot)


#Remove Scientific Notation 
options(scipen=999)

#Cards Against Humanity Data
cah_data <- import(here("files", "data", 
                        "CAH.csv")) %>% 
  mutate(across(where(is.character), ~na_if(., "")))
```

------------------------------------------------------------------------

## Autosave

**Tools \>\> Global Options \>\> Code \>\> Saving**

![](images/clipboard-747997202.png)

------------------------------------------------------------------------

## New Library: `genzplyr`

![](images/clipboard-1232526481.png)

> dplyr but make it bussin fr fr no cap

[genzplyr üíÖ](https://hadley.github.io/genzplyr/){target="_blank"}

------------------------------------------------------------------------

------------------------------------------------------------------------

## Regression  {.center}

***What is the equation for a regression??***

------------------------------------------------------------------------

## Regression 

$$ Y_i = b_0 + b_1X_{1i} + b_2X_{2i} + ... + b_nX_{ni}+ e_i $$

------------------------------------------------------------------------

## Regression  {.center}

***How do we interpret the regression coefficients (Intercepts & slopes)?***

------------------------------------------------------------------------

## Regression

[**INTERCEPT**]{.underline}: When all predictor variables are set to 0, our expected value (predicted $\hat{Y}$) will be this value.

[**SLOPES:**]{.underline}For every 1 unit change in our $X_n$ variable, there will be beta (b or $\beta$) units increase in our Y (outcome) variable, *[**holding all other variables constant**]{.underline}**.***

------------------------------------------------------------------------

### Explaining Variance

![](images/clipboard-3448148790.png){fig-align="center"}

------------------------------------------------------------------------

### Explaining Variance

![](images/clipboard-4102854469.png){fig-align="center"}

------------------------------------------------------------------------

# What do we do with categorical variables as predictors?

![](images/clipboard-1169136778.jpeg){fig-align="center"}

------------------------------------------------------------------------

## Categorical Predictors = factors

Typically identified by a grouping variable that may be a `character`

```{r}
glimpse(cah_data$political_affiliation)
```

Need to change the variable from `character` to `factor` which will assign a number to each group

```{r}
cah_data <- cah_data %>%
  mutate(
    pol = as.factor(political_affiliation),
    ghosts = as.factor(ghosts)
    )

glimpse(cah_data$pol)

```

------------------------------------------------------------------------

## Dummy Coding (replacing factors)

Numerical placeholders used to represent categorical variables

Taking a categorical variable with $k$ levels (e.g., `Democratic`, `Independent`, `Republican`) into $k-1$ binary variables.

| political_affiliation | *becomes* | Ind (binary1) | Rep (binary2) |
|-----------------------|-----------|---------------|---------------|
| Democrat              | --\>      | 0             | 0             |
| Independent           | --\>      | 1             | 0             |
| Republican            | --\>      | 0             | 1             |
| ...                   |           | ...           | ...           |

------------------------------------------------------------------------

### Dummy Coding

For the variable with $k$ levels, we use $k-1$ binary variables...why not use all of them?

::: incremental
-   Including all three variables would result in perfect multicollinearity. All Democrats would be related to other Democrats and unrelated to everything else

-   By including 2 binary variables, we are able to obtain all information about group membership
:::

------------------------------------------------------------------------

### Dummy Coding

The group that has 0's for all the binary variables is the **reference group**

For interpretation, we will make our statements in reference to this group

| political_affiliation | *becomes* | Ind (binary1) | Rep (binary2) |
|-----------------------|-----------|---------------|---------------|
| Democrat              | --\>      | 0             | 0             |
| Independent           | --\>      | 1             | 0             |
| Republican            | --\>      | 0             | 1             |
| ...                   |           | ...           | ...           |

------------------------------------------------------------------------

## Categorical Regression

Going back to our Transformers dataset, let's see how our political affiliation variable can predict \# of transformers movies

Usually you have to create the separate dummy variables, but not in R

```{r}
cat1 <- lm(transformers ~ pol,
           data = cah_data)
```

You can also double check the dummy/contrast coding

```{r}
contrasts(cah_data$pol)
```

------------------------------------------------------------------------

### Interpretation

We are shown the different levels of our predictor variable, but you will not see a predictor for the reference group

::: callout-tip
Think about how we interpret the intercept of a regression model. What are all values of our predictors set to be when looking at the intercept estimate?
:::

```{r}
summary(cat1)
```

------------------------------------------------------------------------

### Refactor Reference Group

Maybe we want to compare to a specific group

We need to then update the reference group

```{r}
cah_data$pol2 <- relevel(cah_data$pol, ref = "Independent")
cat2 <- lm(transformers ~ pol2,
           data = cah_data)
summary(cat2)
```

------------------------------------------------------------------------

### `lm()` or `aov()`

Didn't this look similar to something we've done before?

We are examining group differences in Transformer Movies (continuous) by Political Affiliation (group/categorical)

IT IS JUST AN ANOVA

```{r}
aov1 <- aov(transformers ~ pol2,
           data = cah_data)
summary(aov1)
```

------------------------------------------------------------------------

### Everything is a linear model

We are able to come to the same conclusions that there are no significant differences between groups

```{r}
# Regression F-value
summary(cat2)$fstatistic[[1]]

# ANOVA F-value
summary(aov1)[[1]][1, 4]
```

# ![](images/clipboard-1177388101.webp)

# Break ‚òïüçµü•ê

# Activity Time üé®

------------------------------------------------------------------------

## Categories in Action

**Goal:** Gain greater familiarity with dummy coding and categorical predictors

**Scenario:** We are researchers examining the impact of a new intervention on reducing the vocalization "6Ô∏è‚É£7Ô∏è‚É£" in the youths. We have done classroom observations to collect data on how many times students say "6Ô∏è‚É£7Ô∏è‚É£" after receiving the intervention. This is the data you have in front of you. Youths have been randomly assigned to one of 3 groups, and we need to determine which had the biggest impact.

::: callout-note
You will submit **one** Rmd file for the whole group. Be sure to include all group member's names.
:::

------------------------------------------------------------------------

### Part 1

1.  Get into groups of 4
2.  Download the data <a href="/files/data/cat_reg.csv" download="categorical_reg.csv">\[Download Categorical data (.csv)\]</a>
3.  Enter the data into the spreadsheet with the chips that you have
    1.  Each chip has an ID number (three digits) and a score (between 5 - 20)
    2.  These are the scores for the "post"
    3.  Be sure to identify their Group (the color of the chip)

------------------------------------------------------------------------

### Part 2

1.  Import the data into R

    1.  You will only have data for one school (be sure to `filter()` to include your data)

2.  On the board, put a portion of your data in a table along with your Dummy Code Scheme

    1.  Dr. Haraden should have different sections designated, if not, please remind him nicely (he's doing his best)

------------------------------------------------------------------------

### Part 3

1.  Calculate the average 6Ô∏è‚É£7Ô∏è‚É£ Score for your reference group:\
    Average Score for Group 1 (Reference) = \_\_\_\_\_\_\_\_\_\_\_\_\_\_ = $\beta_0$

2.  Calculate the average 6Ô∏è‚É£7Ô∏è‚É£ Score for your other two groups:\
    Average Score for Group 2 = \_\_\_\_\_\_\_\_\_\_\_\_\_\_\
    Average Score for Group 3 = \_\_\_\_\_\_\_\_\_\_\_\_\_\_

3.  Calculate the "Coefficients" by finding the mean difference from the reference group:\
    $\beta_1$ = (Avg. Group 2 Score) - (Avg. Group 1 Score) = \_\_\_\_\_\_\_\_\_\_\
    $\beta_2$ = (Avg. Group 3 Score) - (Avg. Group 1 Score) = \_\_\_\_\_\_\_\_\_\_

------------------------------------------------------------------------

### Part 4

Write out the regression equation for the model in which we are predicting 6Ô∏è‚É£7Ô∏è‚É£ Scores with our Dummy Variables. It should follow a format that we've used before ( $Y = b_0 + b_1X_1 + b_2X_2 + e$ ), but replacing your estimates in the correct places with the appropriate variables.

Once your group agrees on the equation, write it on the board for your group

------------------------------------------------------------------------

## Categories in Action: Using R

Now that we have the data entered appropriately, let's double check, and include some additional variables!

Import dataset, confirm predictions, include pre-scores to see how that changes interpretation

::: callout-warning
Dr. Haraden is going to start opening up R and doing a follow-along thing. You have been warned
:::

# Everything is a linear model

![](/images/reg_precious.gif)

# Maybe another break?

# Introducing Generalized Linear Model

------------------------------------------------------------------------

## 6Ô∏è‚É£7Ô∏è‚É£ Status

In our last example, we were able to see how these categorical variables could predict a continuous variable. This is perfect for linear regression.

*What if we want to see if students have "recovered" from* 6Ô∏è‚É£7Ô∏è‚É£?

We would then ask: "‚ÄúWhich participants dropped below the clinical threshold for 6Ô∏è‚É£7Ô∏è‚É£ at follow-up? Now, our outcome is either recovered or not recovered.‚Äù

------------------------------------------------------------------------

## Recovery Status

Now we have a binary outcome; Yes/No recovery

What happens when we fit a linear regression? What are the chances of someone with a pre-score of 12 recovering by the follow-up?

```{r}
#| code-fold: true

cat_reg <- import(here("files", "data", 
                       "cat_reg_complete.xlsx")) %>% 
  janitor::clean_names() %>% 
  # 1 = recovered; 0 = not recovered
  mutate(recover = if_else(post_score > 12, 0, 1))

cat_reg %>% 
  ggplot(aes(pre_score, recover)) +
  geom_jitter(width = 0, height = 0.05, alpha = 0.5) + 
  geom_smooth(method = "lm", se = FALSE) + 
  labs(
    title = "Pre-Score predicting Recovery"
  )
```

------------------------------------------------------------------------

## Probability to Odds to Log-Odds

**Probability (*p*)**: The chance of an event happening. Ranges from 0 to 1

**Odds**: The ratio of the probability of an event happening to it *not* happening.

-   $Odds = \frac{p}{1-p}$

-   Ranges from 0 to ‚àû. An odds of 4 means the event is 4 times more likely to happen than not.

**Log-Odds (logit)**: The natural log of the odds

-   $Logit(p) = ln(\frac{p}{1-p})$

-   Ranges from -‚àû to +‚àû

::: callout-important
This step transforms our bounded outcome variable (0/1) to an unbound one!
:::

------------------------------------------------------------------------

## Generalized Linear Model (GLM)

A generalization of a linear model (duh) that is used when the response variable has a non-normal error distribution

Most commonly used when there is a binary (0-1) or count variable as the outcome (we will focus on the binary)

Ultimately, we are trying to identify the ***probability*** of the outcome taking the value 1 ("success") that is being modeled in relation to the predictor variables

------------------------------------------------------------------------

### GLM: Logistic Regression

$$
transformation(p_i)=\beta_0+\beta_1x_{1,i} + \beta_2x_{2,i} + \cdots+\beta_lx_{k,i}
$$

We have to apply a transformation to the left side so that it can take variables beyond just 0 & 1

A common transformation is the $logit\ transformation$

$$
\log_{e}\left( \frac{p_i}{1-p_i} \right) = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \cdots + \beta_k x_{k,i}
$$

------------------------------------------------------------------------

### GLM - CAH data üëª

Research Question: *Does age, the number of books and the number of transformer movies predict the belief in ghosts?*

```{r}
## Using glm() instead of lm()
ln1 <- glm(ghosts ~ age + books + transformers, 
           data = cah_data, 
           ## This is new
           ##Tells the model we are doing logistic regression with a binary outcome
           family = "binomial")
```

------------------------------------------------------------------------

### GLM - CAH data Interpretation

::: callout-note
Interpretation is still the same as linear regression, ***except*** we are dealing with [**log-odds** of the outcome.]{.underline} What does that mean??
:::

```{r}
summary(ln1)
```

------------------------------------------------------------------------

### GLM - CAH data Interpretation (tables)

::: callout-note
We want to be able to interpret these coefficients more easily so we put them into **Odds Ratios**
:::

`sjPlot` is always coming in with the good tables

```{r}
tab_model(ln1)
```

------------------------------------------------------------------------

## Odds Ratios (OR)

**OR \> 1:** The predictor increases the odds of the outcome. (e.g., OR of 2.5 means the odds of believing in ghosts are 2.5 times higher).

**OR \< 1:** The predictor decreases the odds of the outcome. (e.g., OR of 0.4 means the odds of believing in ghosts are 60% lower).

**OR = 1:** The predictor has no effect on the odds of the outcome.

------------------------------------------------------------------------

## Visualization

Extract the model implied probabilities for each individual

```{r}
probs <- broom::augment(ln1, type.predict = "response")
```

Plotting the predicted probabilities

```{r}
probs %>% 
ggplot(aes(age, .fitted)) +
  geom_line(color = "blue", linewidth = 1.5) +
  labs(
    title = "Predicted Probability of Believing in Ghosts by Age",
    subtitle = "Holding # of Books and Transformers Movies at their mean",
    x = "Age (years)",
    y = "Predicted Probability of Believing in Ghosts"
  ) +
  ylim(0, 1) + # Keep the y-axis bounded at 0 and 1
  theme_minimal()
```

------------------------------------------------------------------------

## Logistic Regression: Summary

| **Feature** | **Linear Regression** | **Logistic Regression** |
|:-----------------------|:-----------------------|:-----------------------|
| Outcome Variable | Continuous | Categorical (Binary) |
| Equation | $Y=Œ≤_0+Œ≤_1X$ | $ln‚Å°(\frac{p}{1-p})=Œ≤_0+Œ≤_1X$ |
| Key Interpretation | $\beta_1$ is the change in the mean of Y | $\exp(\beta_1)$ is theodds ratio |
| R Function | `lm()` | `glm(..., family="binomial")` |

------------------------------------------------------------------------

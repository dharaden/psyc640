{
  "hash": "bd60857c81bfde65465ca31f9ecaa3db",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 09: Model Selection & Variability\"\nsubtitle: \"Date: October 19, 2025\"\nfooter:  \"[course-website](https://dharaden.github.io/psyc640/)\"\nlogo: \"images/640_hex.png\"\nformat: \n  revealjs:\n    theme: clean.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    overview: false\n    scrollable: true\n    code-line-numbers: true\neditor: visual\nexecute:\n  echo: true\n  freeze: auto\n---\n\n## Today...\n\nRegression & Model Selection\n\n![](/images/linear_regression_2x.png){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Don't know if I'm using all of these, but including theme here anyways\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(broom)\nlibrary(psych)\nlibrary(here)\n\n\n#Remove Scientific Notation \noptions(scipen=999)\n```\n:::\n\n\n------------------------------------------------------------------------\n\n### Regressions\n\nWith regression, we are ***building a model*** that we think best represents the data, and the broader world\n\n$$\nData = Model + error\n$$\n\n------------------------------------------------------------------------\n\n### Regression Equation\n\nOverall, we are providing a model to give us a \"best guess\" on predicting our outcome\n\n$$\nY_i = b_0 + b_1X_i + e_i\n$$\n\nThis equation is capturing how we are able to calculate each observation ( $Y_i$ )\n\n| Term | Meaning |\n|:--:|----|\n| $b_0$ | **Intercept** - when X = 0 |\n| $b_1$ | **Slope** - for every 1 unit change in X, there are $b_1$ unit change in Y |\n| $X_i$ | Predictor Variable |\n| $e_i$ | Error/Residuals |\n\n------------------------------------------------------------------------\n\n### Unstandardized vs. Standardized\n\n::: r-stack\n***What is the Difference?*** (other than the runes)\n:::\n\n**Unstandardized**:\n\n$$\nY_i = b_0 + b_1X_i + e_i\n$$\n\n**Standardized**:\n\n$$\nY_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\n$$\n\n$$\nZ_y = \\beta_0 + \\beta_1Z_x + \\epsilon_i\n$$\n\n------------------------------------------------------------------------\n\n## Example - LEGO Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlego <- import(here(\"files\", \"data\", \"LEGO_data.csv\")) %>% \n  janitor::clean_names()\n\ndescribe(lego)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 vars  n     mean       sd   median  trimmed      mad      min\nset_name*           1 59    30.00    17.18    30.00    30.00    22.24     1.00\nset_number          2 59 37023.29 25089.50 31167.00 35693.88 30827.70 10294.00\nnumber_of_pieces    3 59  2416.58  2384.39  1503.00  2073.37  1906.62   104.00\nprice               4 59   218.19   214.72   129.99   189.81   163.09     9.99\n                      max range skew kurtosis      se\nset_name*           59.00    58 0.00    -1.26    2.24\nset_number       76974.00 66680 0.55    -1.21 3266.37\nnumber_of_pieces 10001.00  9897 1.32     1.37  310.42\nprice              999.99   990 1.28     1.48   27.95\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Visualization\n\nGood to see the scatterplots of this relationship\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlego %>% \n  ggplot(aes(number_of_pieces, price)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](lec-9_models_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Estimating Coefficients\n\n-   We are bringing in 2 sources of data (information we don't need to estimate)\n\n    -   \\# of Pieces (Mean = 2416.5762712, SD = 2384.3877997)\n\n    -   Set Price (Mean = 218.1935593, SD = 214.7204998)\n\n-   Need to estimate $b_0$ and $b_1$\n\n------------------------------------------------------------------------\n\n## Regression coefficient, $b_{1}$\n\nCalculating the **slope**\n\n$$\\Large b_{1} = \\frac{cov_{XY}}{s_{x}^{2}}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Covariance\ncov(lego$number_of_pieces, lego$price)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 469162.5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance of # of Pieces\nvar(lego$number_of_pieces)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5685305\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### ðŸš§ Brief Detour - Calculating (Co)Variances\n\n**Variance:** The average squared difference between each data point and the mean; *How far each observation is from the mean, on average, in squared units*\n\n$$\nVar(X) = \\frac1{N-1} \\sum (X_i - \\bar X)^2 \n$$\n\n**Covariance**: Average cross product of deviations; *How related are the* *deviations of two different variables across observations*\n\n$$\nCov(X, Y) = \\frac 1{N-1} \\sum (X_i - \\bar{X}) (Y_i - \\bar{Y}) \n$$\n\n------------------------------------------------------------------------\n\n**Example on the board** *(reminder to Dr. Haraden to do something here)*\n\n-   Note where the regression line passes through\n\n-   Look at deviations (variance) to covariance\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmprice <- mean(lego$price)\nmnum <- mean(lego$number_of_pieces)\n\nlego %>% \n  ggplot(aes(number_of_pieces, price)) + \n  geom_point() + \n  geom_hline(yintercept = mprice, \n             color = \"orange\", \n             linewidth = 1.5) + \n  geom_vline(xintercept = mnum, \n             color = \"purple\", \n             linewidth = 1.5) + \n  geom_smooth(method = \"lm\", se = FALSE, \n              color = \"black\", linewidth = 1.5)\n```\n\n::: {.cell-output-display}\n![](lec-9_models_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Regression coefficient, $b_{1}$\n\n$$\\Large b_{1} = \\frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \\frac{s_{y}}{s_{x}}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#SD of # of Pieces\nsd(lego$number_of_pieces) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2384.388\n```\n\n\n:::\n\n```{.r .cell-code}\n# SD of Price\nsd(lego$price) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 214.7205\n```\n\n\n:::\n\n```{.r .cell-code}\n# Correlation\ncor(lego$number_of_pieces, lego$price)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9163742\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Regression Coefficient, $b_0$\n\nCalculating the Intercept:\n\n$$\\Large b_0 = \\bar{Y} - b_1\\bar{X}$$\n\nThe intercept adjusts the location of the regression line to ensure that it runs through the point $\\large (\\bar{X}, \\bar{Y}).$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(lego$number_of_pieces)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2416.576\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(lego$price)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 218.1936\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## What is the Regression Model?\n\nUsing the calculations, identify what the linear model would be.\n\n------------------------------------------------------------------------\n\n## What is the Regression Model?\n\nUsing the calculations, identify what the linear model would be.\n\nYou hear about 3 new sets coming out that you are interested in. Since you have this regression model, you can calculate the total price for each. How much money do you need to save (excluding taxes) to buy all 3 sets?\n\n|                    |            |             |\n|--------------------|------------|-------------|\n| Set 1: 1031 pieces | Set 2: 357 | Set 3: 4154 |\n\n# Before the Break\n\nTake the [Stroop Task](https://www.psytoolkit.org/experiment-library/experiment_stroop.html){target=\"_blank\"} and record your incongruent, congruent and stroop effect on your worksheet\n\n# Back to the Show\n\nLast time on \"Stats Island\", Dr. Haraden asked everyone to look at colors and words. Plus, Rupert was voted off the island. Who could be next??\n\n------------------------------------------------------------------------\n\n## Explaining Variance\n\n![](/images/predict_var.png){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n![](/images/varianceexplained.gif){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n## Stroop Data Modeling\n\n**Research Question**: Sleep Quality is negatively related to the Stroop Effect. Lower sleep quality will predict a greater Stroop effect.\n\n[Data Set (I hope I entered all the data in appropriately)](https://docs.google.com/spreadsheets/d/1VSWDBci5MlMmXnFDq-rau3GzlqJEZAeIrEb3fD1ye9g/edit?usp=sharing){target=\"_blank\"}\n\n------------------------------------------------------------------------\n\n### Explaining Variance\n\nVisualize the Stroop Effect data\n\n-   What is the best \"model\" to explain this data? If you had no information, what would be your best guess for someone's score?\n\n. . .\n\nWe are looking to explain variance in our outcome (Stroop Performance)\n\n::: callout-note\nThe total distance from the mean to each point is the Total Sum of Squares - The total amount of \"error\" we are trying to explain\n:::\n\nThe mean can be considered the \"null model\" in that it is the most basic\n\n------------------------------------------------------------------------\n\n### Null to Sleep model\n\nWe can then introduce another variable to try explain more of the variance in the Stroop Effect\n\n::: callout-note\nThere should be some visualizations on the board with mean and regression lines\n:::\n\nThe distance from each point down to the original 'mean' is the total deviation.\n\nThe portion from the 'mean' up to our new 'sleep model' is the explained deviation (**SSM**). This is the improvement we gained by considering sleep!\n\nThe leftover bit from our 'sleep model' to the actual data point is the unexplained residual (**SSE**). This is the error our model couldn't account for.\n\n------------------------------------------------------------------------\n\n## Linear Regression\n\nUse the information and let's do it in R instead of on the board\n\nVisualize the relationship\n\nRun a linear regression\n\n------------------------------------------------------------------------\n\n## Model Fit\n\nTake a look at the $R^2$ value\n\nIs there any other data that we have that could impact overall Stroop Effects?\n\nAfter including it in the model, we can examine the change in $R^2$\n\nWhat would happen if we put the `random number` variable in there?\n\n------------------------------------------------------------------------\n\n### Comparing Models\n\nCan compare models using an ANOVA when one model is **nested** within another\n\n::: callout-note\n**Nested Models**: where one model (Model 0) contains a subset of the predictors from the other one (Model 1). Model 1 contains *all* of the predictors included in Model 0, plus one or more additional predictors. When this happens we say that Model 0 is nested within Model 1.\n:::\n\n------------------------------------------------------------------------\n\n### Model Diagnostics\n\nAs we get more complex, we want to be sure to check how our model is functioning. One way is to use $adjusted \\ R^2$ to get a sense of the amount of variance each predictor is bringing.\n\nNeed to be careful of **overfitting** the model to the data\n\n**Multicollinearity** - when predictors are highly related to one another\n\n-   Checked with the **Variance Inflation Factor** (VIF)\n\n**Homogeneity of Variance** - the variance of the residuals is assumed to be constant\n\n------------------------------------------------------------------------\n\n### `check_models`\n\nThis will check everything for you in regressions, let's try it out\n\n<https://easystats.github.io/performance/reference/check_model.html>\n\n------------------------------------------------------------------------\n\n## Selecting a Model\n\nOne of the most daunting tasks can be identifying the \"right\" statistical model for the data you have\n\nYou may see flowcharts like this:\n\n![](/images/flow.png){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Selecting a Model\n\nIf you are asking which model is \"right\", you are doing it backwards\n\n> \"All models are wrong, some models are useful.\" -George Box\n\nRemember: $Data (The\\ Truth) = Model + Error$\n\nInstead, think about identifying a model as *\"Model Based Reasoning\"* in which you will need to defend the choices you made\n\n------------------------------------------------------------------------\n\n## Example: t-test vs. ANOVA vs. Regression\n\nThis might be something that we are considering between. How do we pick the \"right\" one?\n\nStart with some data where we have 2 groups and we identify a 2 point difference in the groups scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\n\n# Two independent groups with equal variances\nn0 <- 80; n1 <- 80\nmu0 <- 12; mu1 <- 10      # true mean difference = -2\nsigma <- 3\ny0 <- rnorm(n0, mu0, sigma)\ny1 <- rnorm(n1, mu1, sigma)\n\n# Put that in a data frame\ndat <- data.frame(\n  dep = c(y0, y1),\n  group01 = c(rep(0, n0), rep(1, n1))\n) %>% \n  mutate(group = factor(group01, levels = c(0,1), labels = c(\"control\",\"treatment\")))\n```\n:::\n\n\n------------------------------------------------------------------------\n\n### t-test\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntt_fit <- t.test(dep ~ group, data = dat, var.equal = TRUE)\ntt_tidy <- tidy(tt_fit)\ntt_tidy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 10\n  estimate estimate1 estimate2 statistic    p.value parameter conf.low conf.high\n     <dbl>     <dbl>     <dbl>     <dbl>      <dbl>     <dbl>    <dbl>     <dbl>\n1     2.68      12.1      9.46      5.75    4.48e-8       158     1.76      3.60\n# â„¹ 2 more variables: method <chr>, alternative <chr>\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### ANOVA\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_aov <- aov(dep ~ group, data = dat)\nsummary(fit_aov)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Df Sum Sq Mean Sq F value       Pr(>F)    \ngroup         1  286.6  286.64   33.07 0.0000000448 ***\nResiduals   158 1369.6    8.67                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\naov_tidy <- tidy(fit_aov)\n```\n:::\n\n\n------------------------------------------------------------------------\n\n### Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg_fit <- lm(dep ~ group01, data = dat)\nreg_tidy <- tidy(reg_fit)\nreg_tidy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    12.1      0.329     36.9  1.65e-79\n2 group01        -2.68     0.466     -5.75 4.48e- 8\n```\n\n\n:::\n\n```{.r .cell-code}\nreg_glance <- glance(reg_fit)\n```\n:::\n\n\n------------------------------------------------------------------------\n\n### Comparing Methods\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Equivalence 1: The regression slope for the dummy variable equals the difference in group means.\nmean_diff <- tt_tidy$estimate2 - tt_tidy$estimate1\nbeta1 <- reg_tidy %>% \n  filter(term == \"group01\") %>% \n  pull(estimate)\n\nmean_diff\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.676943\n```\n\n\n:::\n\n```{.r .cell-code}\nbeta1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.676943\n```\n\n\n:::\n\n```{.r .cell-code}\n# Equivalence 2: The F-statistic from the ANOVA is the regression square of the t-statistic.\nt_stat_reg <- reg_tidy %>% \n  filter(term == \"group01\") %>% \n  pull(statistic)\n\nt_stat_t <- tt_tidy %>% \n  pull(statistic)\n\nf_stat <- reg_glance$statistic\n\n\nt_stat_reg^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 33.06772\n```\n\n\n:::\n\n```{.r .cell-code}\nt_stat_t^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       t \n33.06772 \n```\n\n\n:::\n\n```{.r .cell-code}\nf_stat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   value \n33.06772 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Equivalence 3: The p-values are identical.\ntt_tidy$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.00000004475048\n```\n\n\n:::\n\n```{.r .cell-code}\nreg_tidy %>% \n  filter(term == \"group01\") %>% \n  pull(p.value)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.00000004475048\n```\n\n\n:::\n\n```{.r .cell-code}\naov_tidy %>% \n  filter(term == \"group\") %>% \n  pull(p.value)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.00000004475048\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n# Examples in `R`\n\nTry some live coding! Also known as \"Another opportunity for Dr. Haraden to potentially embarrass himself\"\n\n<https://archive.ics.uci.edu/dataset/320/student+performance>\n\n<https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success>\n\n<https://datahub.io/collections>\n\n<https://www.kaggle.com/datasets>\n",
    "supporting": [
      "lec-9_models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
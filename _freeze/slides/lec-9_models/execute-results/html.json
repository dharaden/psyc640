{
  "hash": "91c72901ca6b036bfb56450103f71850",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 09: Model Selection & Variability\"\nsubtitle: \"Date: October 19, 2025\"\nfooter:  \"[course-website](https://dharaden.github.io/psyc640/)\"\nlogo: \"images/640_hex.png\"\nformat: \n  revealjs:\n    theme: clean.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    overview: false\n    scrollable: true\n    code-line-numbers: true\neditor: visual\nexecute:\n  echo: true\n  freeze: auto\n---\n\n## Today...\n\nRegression & Model Selection\n\n![](/images/linear_regression_2x.png){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Don't know if I'm using all of these, but including theme here anyways\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(broom)\nlibrary(psych)\nlibrary(here)\n\n\n#Remove Scientific Notation \noptions(scipen=999)\n```\n:::\n\n\n------------------------------------------------------------------------\n\n### Regressions\n\nWith regression, we are ***building a model*** that we think best represents the data, and the broader world\n\n$$\nData = Model + error\n$$\n\n------------------------------------------------------------------------\n\n### Regression Equation\n\nOverall, we are providing a model to give us a \"best guess\" on predicting our outcome\n\n$$\nY_i = b_0 + b_1X_i + e_i\n$$\n\nThis equation is capturing how we are able to calculate each observation ( $Y_i$ )\n\n| Term | Meaning |\n|:-----------------------:|-----------------------------------------------|\n| $b_0$ | **Intercept** - when X = 0 |\n| $b_1$ | **Slope** - for every 1 unit change in X, there are $b_1$ unit change in Y |\n| $X_i$ | Predictor Variable |\n| $e_i$ | Error/Residuals |\n\n------------------------------------------------------------------------\n\n### Unstandardized vs. Standardized\n\n::: r-stack\n***What is the Difference?*** (other than the runes)\n:::\n\nUnstandardized:\n\n$$\nY_i = b_0 + b_1X_i + e_i\n$$\n\nStandardized:\n\n$$\nY_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\n$$\n\n$$\nZ_y = \\beta_0 + \\beta_1Z_x + \\epsilon_i\n$$\n\n------------------------------------------------------------------------\n\n## Example - LEGO Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlego <- import(here(\"files\", \"data\", \"LEGO_data.csv\")) %>% \n  janitor::clean_names()\n\ndescribe(lego)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 vars  n     mean       sd   median  trimmed      mad      min\nset_name*           1 59    30.00    17.18    30.00    30.00    22.24     1.00\nset_number          2 59 37023.29 25089.50 31167.00 35693.88 30827.70 10294.00\nnumber_of_pieces    3 59  2416.58  2384.39  1503.00  2073.37  1906.62   104.00\nprice               4 59   218.19   214.72   129.99   189.81   163.09     9.99\n                      max range skew kurtosis      se\nset_name*           59.00    58 0.00    -1.26    2.24\nset_number       76974.00 66680 0.55    -1.21 3266.37\nnumber_of_pieces 10001.00  9897 1.32     1.37  310.42\nprice              999.99   990 1.28     1.48   27.95\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Visualization\n\nGood to see the scatterplots of this relationship\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlego %>% \n  ggplot(aes(number_of_pieces, price)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](lec-9_models_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Estimating Coefficients\n\n-   We are bringing in 2 sources of data (information we don't need to estimate)\n\n    -   \\# of Pieces (Mean = 2416.5762712, SD = 2384.3877997)\n\n    -   Set Price (Mean = 218.1935593, SD = 214.7204998)\n\n-   Need to estimate $b_0$ and $b_1$\n\n------------------------------------------------------------------------\n\n## Regression coefficient, $b_{1}$\n\nCalculating the **slope**\n\n$$\\Large b_{1} = \\frac{cov_{XY}}{s_{x}^{2}}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Covariance\ncov(lego$number_of_pieces, lego$price)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 469162.5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variance of # of Pieces\nvar(lego$number_of_pieces)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5685305\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### ðŸš§ Brief Detour - Calculating (Co)Variances\n\n**Variance:** The average squared difference between each data point and the mean; *How far each observation is from the mean, on average, in squared units*\n\n$$\nVar(X) = \\frac1{N-1} \\sum (X_i - \\bar X)^2 \n$$\n\n**Covariance**: Average cross product of deviations; *How related are the* *deviations of two different variables across observations*\n\n$$\nCov(X, Y) = \\frac 1{N-1} \\sum (X_i - \\bar{X}) (Y_i - \\bar{Y}) \n$$\n\n------------------------------------------------------------------------\n\n**Example on the board** *(reminder to Dr. Haraden to do something here)*\n\n-   Note where the regression line passes through\n\n-   Look at deviations (variance) to covariance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmprice <- mean(lego$price)\nmnum <- mean(lego$number_of_pieces)\n\nlego %>% \n  ggplot(aes(number_of_pieces, price)) + \n  geom_point() + \n  geom_hline(yintercept = mprice, \n             color = \"orange\", \n             linewidth = 1.5) + \n  geom_vline(xintercept = mnum, \n             color = \"purple\", \n             linewidth = 1.5) + \n  geom_smooth(method = \"lm\", se = FALSE, \n              color = \"black\", linewidth = 1.5)\n```\n\n::: {.cell-output-display}\n![](lec-9_models_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Regression coefficient, $b_{1}$\n\n$$\\Large b_{1} = \\frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \\frac{s_{y}}{s_{x}}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#SD of # of Pieces\nsd(lego$number_of_pieces) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2384.388\n```\n\n\n:::\n\n```{.r .cell-code}\n# SD of Price\nsd(lego$price) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 214.7205\n```\n\n\n:::\n\n```{.r .cell-code}\n# Correlation\ncor(lego$number_of_pieces, lego$price)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9163742\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Regression Coefficient, $b_0$ \n\nCalculating the Intercept:\n\n$$\\Large b_0 = \\bar{Y} - b_1\\bar{X}$$\n\nThe intercept adjusts the location of the regression line to ensure that it runs through the point $\\large (\\bar{X}, \\bar{Y}).$\n\n```{r}s}\nmean(lego$number_of_pieces)\nmean(lego$price)\n```\n\n------------------------------------------------------------------------\n\n## What is the Regression Model?\n\nUsing the calculations, identify what the linear model would be.\n\n------------------------------------------------------------------------\n\n## What is the Regression Model?\n\nUsing the calculations, identify what the linear model would be.\n\nYou hear about 3 new sets coming out that you are interested in. Since you have this regression model, you can calculate the total price for each. How much money do you need to save (excluding taxes) to buy all 3 sets?\n\n|                    |            |             |\n|--------------------|------------|-------------|\n| Set 1: 1031 pieces | Set 2: 357 | Set 3: 4154 |\n\n# Before the Break\n\nTake the [Stroop Task](https://www.psytoolkit.org/experiment-library/experiment_stroop.html){target=\"_blank\"} and record your incongruent, congruent and stroop effect on your worksheet\n\n# Back to the Show\n\nLast time on \"Stats Island\", Dr. Haraden asked everyone to look at colors and words. Plus, Rupert was voted off the island. Who could be next??\n\n------------------------------------------------------------------------\n\n## Explaining Variance\n\n![](/images/predict_var.png){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n![](/images/varianceexplained.gif){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n## Stroop Data Modeling\n\n**Research Question**: Sleep Quality is negatively related to the Stroop Effect. Lower sleep quality will predict a greater Stroop effect.\n\n[Data Set (I hope I entered all the data in appropriately)](https://docs.google.com/spreadsheets/d/1VSWDBci5MlMmXnFDq-rau3GzlqJEZAeIrEb3fD1ye9g/edit?usp=sharing){target=\"_blank\"}\n\n------------------------------------------------------------------------\n\n### Explaining Variance\n\nVisualize the Stroop Effect data\n\n-   What is the best \"model\" to explain this data? If you had no information, what would be your best guess for someone's score?\n\n. . .\n\nWe are looking to explain variance in our outcome (Stroop Performance)\n\n::: callout-note\n## The total distance from the mean to each point is the Total Sum of Squares - The total amount of \"error\" we are trying to explain\n:::\n\nThe mean can be considered the \"null model\" in that it is the most basic\n\n------------------------------------------------------------------------\n\n### Null to Sleep model\n\nWe can then introduce another variable to try explain more of the variance in the Stroop Effect\n\n::: callout-note\n## There should be some visualizations on the board with mean and regression lines\n:::\n\nThe distance from each point down to the original 'mean' is the total deviation.\n\nThe portion from the 'mean' up to our new 'sleep model' is the explained deviation (**SSM**). This is the improvement we gained by considering sleep!\n\nThe leftover bit from our 'sleep model' to the actual data point is the unexplained residual (**SSE**). This is the error our model couldn't account for.\n\n------------------------------------------------------------------------\n\n## Linear Regression\n\nUse the information and let's do it in R instead of on the board\n\nVisualize the relationship\n\nRun a linear regression\n\n------------------------------------------------------------------------\n\n## Model Fit\n\nTake a look at the $R^2$ value\n\nIs there any other data that we have that could impact overall Stroop Effects?\n\nAfter including it in the model, we can examine the change in $R^2$\n\nWhat would happen if we put the `random number` variable in there?\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\n\n# 1) Simulate two independent groups with equal variances\nn0 <- 80; n1 <- 80\nmu0 <- 12; mu1 <- 10      # true mean difference = -2\nsigma <- 3\ny0 <- rnorm(n0, mu0, sigma)\ny1 <- rnorm(n1, mu1, sigma)\n\ndat <- data.frame(\n  dep = c(y0, y1),\n  group01 = c(rep(0, n0), rep(1, n1))\n) |>\n  transform(group = factor(group01, levels = c(0,1), labels = c(\"control\",\"program\")))\n\n# 2) Two-sample t-test (equal variances to match OLS)\ntt <- t.test(dep ~ group, data = dat, var.equal = TRUE)\ntt\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  dep by group\nt = 5.7505, df = 158, p-value = 0.00000004475\nalternative hypothesis: true difference in means between group control and group program is not equal to 0\n95 percent confidence interval:\n 1.757501 3.596386\nsample estimates:\nmean in group control mean in group program \n            12.135819              9.458875 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 3) Regression with 0/1 indicator\nm <- lm(dep ~ group01, data = dat)\nsm <- summary(m)\nsm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = dep ~ group01, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.4879 -2.2516 -0.1258  2.0889  8.4225 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)  12.1358     0.3292   36.87 < 0.0000000000000002 ***\ngroup01      -2.6769     0.4655   -5.75         0.0000000448 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.944 on 158 degrees of freedom\nMultiple R-squared:  0.1731,\tAdjusted R-squared:  0.1678 \nF-statistic: 33.07 on 1 and 158 DF,  p-value: 0.00000004475\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check: slope equals difference in means\ndiff_means <- with(dat, mean(dep[group01==1]) - mean(dep[group01==0]))\nc(beta1 = coef(m)[\"group01\"], diff_means = diff_means)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nbeta1.group01    diff_means \n    -2.676943     -2.676943 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 4) ANOVA from the regression fit and from aov()\nafrom_lm  <- anova(m)\nafrom_lm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: dep\n           Df  Sum Sq Mean Sq F value        Pr(>F)    \ngroup01     1  286.64 286.641  33.068 0.00000004475 ***\nResiduals 158 1369.59   8.668                          \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nfit_aov <- aov(dep ~ group, data = dat)\nsummary(fit_aov)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Df Sum Sq Mean Sq F value       Pr(>F)    \ngroup         1  286.6  286.64   33.07 0.0000000448 ***\nResiduals   158 1369.6    8.67                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# 5) Verify F = t^2 equivalence for single df test\nt_reg <- sm$coefficients[\"group01\",\"t value\"]\nF_lm  <- afrom_lm[\"group01\",\"F value\"]\nc(t_squared = t_reg^2, F_from_anova = F_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   t_squared F_from_anova \n    33.06772     33.06772 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 6) Compare p-values across methods\nc(p_ttest = tt$p.value,\n  p_lm_slope = sm$coefficients[\"group01\",\"Pr(>|t|)\"],\n  p_anova = afrom_lm[\"group01\",\"Pr(>F)\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         p_ttest       p_lm_slope          p_anova \n0.00000004475048 0.00000004475048 0.00000004475048 \n```\n\n\n:::\n:::\n\n\n# Example in `R`\n\nTry some live coding! Also known as \"Another opportunity for Dr. Haraden to potentially embarrass himself\"\n\n<https://archive.ics.uci.edu/dataset/320/student+performance>\n\n<https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success>\n\n<https://datahub.io/collections>\n\n<https://www.kaggle.com/datasets>\n",
    "supporting": [
      "lec-9_models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
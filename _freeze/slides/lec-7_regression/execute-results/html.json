{
  "hash": "4dfe949b4727e9f9fe52eba134ee9d38",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 07: Simple Regression\"\nsubtitle: \"Date: October 5, 2025\"\nfooter:  \"[course-website](https://dharaden.github.io/psyc640/)\"\nlogo: \"images/640_hex.png\"\nformat: \n  revealjs:\n    theme: clean.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    overview: false\n    scrollable: true\n    code-line-numbers: true\neditor: visual\nexecute:\n  echo: true\n  freeze: auto\n---\n\n## Today...\n\nRegression\n\n-   Why use regression?\n\n-   One equation to rule them all\n\n![](/images/reg_precious.gif){fig-align=\"center\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Don't know if I'm using all of these, but including theme here anyways\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(broom)\nlibrary(psych)\nlibrary(gapminder)\nlibrary(psychTools)\n\n\n#Remove Scientific Notation \noptions(scipen=999)\n```\n:::\n\n\n------------------------------------------------------------------------\n\n## Overview of Regression\n\nRegression is a general data analytic system\n\n-   Lots of things fall under the umbrella of regression\n\n-   This system can handle a variety of forms of relations and types of variables\n\nThe output of regression includes both effect sizes and statistical significance\n\nWe can also incorporate multiple influences (IVs) and account for their intercorrelations\n\n------------------------------------------------------------------------\n\n### Uses for regression\n\n::: incremental\n-   **Adjustment**: Take into account (*control*) known effects in a relationship\n\n-   **Prediction**: Develop a model based on what has happened previously to predict what will happen in the future\n\n-   **Explanation**: examining the influence of one or more variable on some outcome\n:::\n\n------------------------------------------------------------------------\n\n## Study Design & Collection\n\n::::: columns\n::: {.column width=\"50%\"}\nDesign - *When* data are collected\n\n-   Retrospective/Prospective\n\n-   Longitudinal\n\n-   Cross-Sectional\n:::\n\n::: {.column width=\"50%\"}\nCollection - *How* data are collected\n\n-   Experimental\n\n-   Field\n\n-   Observational\n\n-   Meta-analysis\n\n-   Neuroimaging/Psychophysiology\n\n-   Survey\n\n-   Quasi-Experimental\n:::\n:::::\n\n------------------------------------------------------------------------\n\n### Regression Equation\n\nWith regression, we are ***building a model*** that we think best represents the data, and the broader world\n\n$$\nData = Model + error\n$$\n\n------------------------------------------------------------------------\n\nAt the most simple form we are drawing a line to characterize the linear relationship between the variables so that for any value of `x` we can have an estimate of `y`\n\n$$\nY = mX + b\n$$\n\n::::::: columns\n:::: {.column width=\"50%\"}\n::: incremental\n-   Y = Outcome Variable (DV)\n\n-   m = Slope Term\n:::\n::::\n\n:::: {.column width=\"50%\"}\n::: incremental\n-   X = Predictor (IV)\n\n-   b = Intercept\n:::\n::::\n:::::::\n\n------------------------------------------------------------------------\n\n### Regression Equation\n\nOverall, we are providing a model to give us a \"best guess\" on predicting\n\nLet's \"science up\" the equation a little bit:\n\n$$\nY_i = b_0 + b_1X_i + e_i\n$$\n\nThis equation is capturing how we are able to calculate each observation ( $Y_i$ )\n\n$$\n\\hat{Y_i} = b_0 + b_1X_i\n$$\n\nThis one will give us the \"best guess\" or *expected* value of $Y$ given $X$\n\n------------------------------------------------------------------------\n\n### Regression Equation\n\nThere are two ways to think about our regression equation. They're similar to each other, but they produce different outputs.\\\n$$Y_i = b_{0} + b_{1}X_i +e_i$$\\\n$$\\hat{Y_i} = b_{0} + b_{1}X_i$$\\\nThe model we are building by including new variables is to ***explain variance*** in our outcome\n\n::: callout-note\n$\\hat{Y}$ signifies the fitted score -- no errorThe difference between the fitted and observed score is the residual ($e_i$)There is a different e value for each observation in the dataset\n:::\n\n------------------------------------------------------------------------\n\n### Expected vs. Actual\n\n$$Y_i = b_{0} + b_{1}X_i + e_i$$\n\n$$\\hat{Y_i} = b_{0} + b_{1}X_i$$\n\n$\\hat{Y}$ signifies that there is no error. Our line is predicting that exact value. We interpret it as being \"on average\"\n\nImportant to identify that that $Y_i - \\hat{Y_i} = e_i$.\n\n::: notes\n$\\hat{Y}$ signifies the fitted score -- no error\n\nThe difference between the fitted and observed score is the residual ($e_i$)\n\nThere is a different e value for each observation in the dataset\n:::\n\n------------------------------------------------------------------------\n\n## OLS\n\n::: incremental\n-   How do we find the regression estimates?\n\n-   Ordinary Least Squares (OLS) estimation\n\n-   Minimizes deviations\n\n    -   $$ min\\sum(Y_{i} - \\hat{Y} ) ^{2} $$\n\n-   Other estimation procedures possible (and necessary in some cases)\n:::\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(142)\nx.1 <- rnorm(10, 0, 1)\ne.1 <- rnorm(10, 0, 2)\ny.1 <- .5 + .55 * x.1 + e.1\nd.1 <- data.frame(x.1,y.1)\nm.1 <- lm(y.1 ~ x.1, data = d.1)\nd1.f<- augment(m.1)\n\n\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/plot1-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_point(aes(y = .fitted), shape = 1, size = 2) +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_point(aes(y = .fitted), shape = 1, size = 2) +\n  geom_segment(aes( xend = x.1, yend = .fitted))+\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_smooth(method = lm, se = FALSE) +\n  geom_point(aes(y = .fitted), shape = 1, size = 2) +\n  geom_segment(aes( xend = x.1, yend = .fitted))+\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## compare to bad fit\n\n::::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n:::\n:::::\n\n------------------------------------------------------------------------\n\n## OLS\n\nThe line that yields the smallest sum of squared deviations\n\n$$\\Sigma(Y_i - \\hat{Y_i})^2$$ $$= \\Sigma(Y_i - (b_0+b_{1}X_i))^2$$ $$= \\Sigma(e_i)^2$$\n\n. . .\n\nIn order to find the OLS solution, you could try many different coefficients $(b_0 \\text{ and } b_{1})$ until you find the one with the smallest sum squared deviation. Luckily, there are simple calculations that will yield the OLS solution every time.\n\n------------------------------------------------------------------------\n\n## Regression coefficient, $b_{1}$\n\n$$b_{1} = \\frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \\frac{s_{y}}{s_{x}}$$\n\nWhat units is the regression coefficient in?\n\n. . .\n\nThe regression coefficient (slope) equals the estimated change in Y for a 1-unit change in X\n\n------------------------------------------------------------------------\n\n$$\\Large b_{1} = r_{xy} \\frac{s_{y}}{s_{x}}$$\n\nIf the standard deviation of both X and Y is equal to 1:\n\n$$\\Large b_1 = r_{xy} \\frac{s_{y}}{s_{x}} = r_{xy} \\frac{1}{1} = r_{xy} = \\beta_{yx} = b_{yx}^*$$\n\n------------------------------------------------------------------------\n\n## Standardized regression equation\n\n$$\\Large Z_{y_i} = b_{yx}^*Z_{x_i}+e_i$$\n\n$$\\Large b_{yx}^* = b_{yx}\\frac{s_x}{s_y} = r_{xy}$$\n\n. . .\n\nAccording to this regression equation, when $X = 0, Y = 0$. Our interpretation of the coefficient is that a one-standard deviation increase in X is associated with a $b_{yx}^*$ standard deviation increase in Y. Our regression coefficient is equivalent to the correlation coefficient *when we have only one predictor in our model.*\n\n------------------------------------------------------------------------\n\n## Estimating the intercept, $b_0$\n\n-   intercept serves to adjust for differences in means between X and Y\n\n$$\\hat{Y_i} = \\bar{Y} + r_{xy} \\frac{s_{y}}{s_{x}}(X_i-\\bar{X})$$\n\n-   if standardized, intercept drops out\n\n-   otherwise, intercept is where regression line crosses the y-axis at X = 0\n\n::: notes\n-   Also, notice that when $X = \\bar{X}$ the regression line goes through $\\bar{Y}$\n\n$$b_0 = \\bar{Y} - b_1\\bar{X}$$\n:::\n\n------------------------------------------------------------------------\n\nThe intercept adjusts the location of the regression line to ensure that it runs through the point $\\large (\\bar{X}, \\bar{Y}).$ We can calculate this value using the equation:\n\n$$\\Large b_0 = \\bar{Y} - b_1\\bar{X}$$\n\n# Visuals of OLS Regression\n\n<https://setosa.io/ev/ordinary-least-squares-regression/>\n\n[https://observablehq.com/\\@yizhe-ang/interactive-visualization-of-linear-regression](https://observablehq.com/@yizhe-ang/interactive-visualization-of-linear-regression)\n\n------------------------------------------------------------------------\n\n## Example (by hand)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gapminder)\ngapminder %<>% filter(year == 2007 & continent == \"Asia\") %>% \n  mutate(log_gdp = log(gdpPercap))\n\ngapminder %>% \n  select(log_gdp, lifeExp) %>% \n  describe()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        vars  n  mean   sd median trimmed  mad   min   max range  skew kurtosis\nlog_gdp    1 33  8.74 1.24   8.41    8.73 1.42  6.85 10.76  3.91  0.21    -1.37\nlifeExp    2 33 70.73 7.96  72.40   71.31 7.70 43.83 82.60 38.77 -1.07     1.79\n          se\nlog_gdp 0.22\nlifeExp 1.39\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(gapminder$log_gdp, gapminder$lifeExp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8003474\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nIf we regress lifeExp onto log_gdp:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr = cor(gapminder$log_gdp, gapminder$lifeExp)\nm_log_gdp = mean(gapminder$log_gdp)\nm_lifeExp = mean(gapminder$lifeExp)\ns_log_gdp = sd(gapminder$log_gdp)\ns_lifeExp = sd(gapminder$lifeExp)\n\nb1 = r*(s_lifeExp/s_log_gdp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.157259\n```\n\n\n:::\n\n```{.r .cell-code}\nb0 = m_lifeExp - b1*m_log_gdp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 25.65011\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## In `R`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit.1 <- lm(lifeExp ~ log_gdp, data = gapminder)\nsummary(fit.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = lifeExp ~ log_gdp, data = gapminder)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.314  -1.650  -0.040   3.428   8.370 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(>|t|)    \n(Intercept)  25.6501     6.1234   4.189     0.000216 ***\nlog_gdp       5.1573     0.6939   7.433 0.0000000226 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.851 on 31 degrees of freedom\nMultiple R-squared:  0.6406,\tAdjusted R-squared:  0.629 \nF-statistic: 55.24 on 1 and 31 DF,  p-value: 0.00000002263\n```\n\n\n:::\n:::\n\n\n::: notes\n**Things to discuss**\n\n-   Coefficient estimates\n-   Statistical tests (covered in more detail soon)\n:::\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Data, fitted, and residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nmodel_info = augment(fit.1)\nhead(model_info)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 8\n  lifeExp log_gdp .fitted .resid   .hat .sigma .cooksd .std.resid\n    <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n1    43.8    6.88    61.1 -17.3  0.101    3.63 0.796       -3.76 \n2    75.6   10.3     78.8  -3.15 0.0802   4.89 0.0199      -0.676\n3    64.1    7.24    63.0   1.08 0.0765   4.93 0.00224      0.233\n4    59.7    7.45    64.1  -4.33 0.0646   4.86 0.0294      -0.923\n5    73.0    8.51    69.5   3.43 0.0314   4.89 0.00836      0.718\n6    82.2   10.6     80.3   1.94 0.100    4.92 0.00994      0.422\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescribe(model_info, fast = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           vars  n  mean   sd median    min   max range  skew kurtosis   se\nlifeExp       1 33 70.73 7.96  72.40  43.83 82.60 38.77 -1.07     1.79 1.39\nlog_gdp       2 33  8.74 1.24   8.41   6.85 10.76  3.91  0.21    -1.37 0.22\n.fitted       3 33 70.73 6.37  69.00  60.98 81.16 20.19  0.21    -1.37 1.11\n.resid        4 33  0.00 4.77  -0.04 -17.31  8.37 25.68 -1.37     3.29 0.83\n.hat          5 33  0.06 0.03   0.05   0.03  0.11  0.08  0.60    -0.98 0.00\n.sigma        6 33  4.84 0.23   4.90   3.63  4.93  1.30 -4.53    20.90 0.04\n.cooksd       7 33  0.04 0.14   0.01   0.00  0.80  0.80  5.08    25.12 0.02\n.std.resid    8 33  0.00 1.02  -0.01  -3.76  1.77  5.53 -1.44     3.56 0.18\n```\n\n\n:::\n:::\n\n\n::: notes\nPoint out the average of the residuals is 0, just like average deviation from the mean is 0.\n:::\n\n------------------------------------------------------------------------\n\n### The relationship between $X_i$ and $\\hat{Y_i}$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_info %>% ggplot(aes(x = log_gdp, y = .fitted)) +\n  geom_point() + geom_smooth(se = F, method = \"lm\") +\n  scale_x_continuous(\"X\") + scale_y_continuous(expression(hat(Y))) + theme_bw(base_size = 30)\n```\n\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-13-1.png){width=768}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### The relationship between $X_i$ and $e_i$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_info %>% ggplot(aes(x = log_gdp, y = .resid)) +\n  geom_point() + geom_smooth(se = F, method = \"lm\") + \n  scale_x_continuous(\"X\") + scale_y_continuous(\"e\") + theme_bw(base_size = 30)\n```\n\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-14-1.png){width=768}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### The relationship between $Y_i$ and $\\hat{Y_i}$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_info %>% ggplot(aes(x = lifeExp, y = .fitted)) +\n  geom_point() + geom_smooth(se = F, method = \"lm\") + \n  scale_x_continuous(\"Y\") + scale_y_continuous(expression(hat(Y))) + theme_bw(base_size = 30)\n```\n\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-15-1.png){width=768}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### The relationship between $Y_i$ and $e_i$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_info %>% ggplot(aes(x = lifeExp, y = .resid)) +\n  geom_point() + geom_smooth(se = F, method = \"lm\") + \n  scale_x_continuous(\"Y\") + scale_y_continuous(\"e\") + theme_bw(base_size = 25)\n```\n\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-16-1.png){width=768}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### The relationship between $\\hat{Y_i}$ and $e_i$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_info %>% ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() + geom_smooth(se = F, method = \"lm\") + \n  scale_y_continuous(\"e\") + scale_x_continuous(expression(hat(Y))) + theme_bw(base_size = 30)\n```\n\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-17-1.png){width=768}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Using `easystats`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(fit.1)\n```\n\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Using `sjPlot`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::tab_model(fit.1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">life Exp</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">25.65</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">13.16&nbsp;&ndash;&nbsp;38.14</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">log gdp</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">5.16</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">3.74&nbsp;&ndash;&nbsp;6.57</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">33</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.641 / 0.629</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Regression to the mean\n\nAn observation about heights was part of the motivation to develop the regression equation: If you selected a parent who was exceptionally tall (or short), their child was almost always not as tall (or as short).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(psychTools)\nlibrary(tidyverse)\nheights = psychTools::galton\nmod = lm(child~parent, data = heights)\npoint = 902\nheights = broom::augment(mod)\n\n\nheights %>%\n  ggplot(aes(x = parent, y = child)) +\n  geom_jitter(alpha = .3) +\n  geom_hline(aes(yintercept = 72), color = \"red\") +\n  geom_vline(aes(xintercept = 72), color = \"red\") +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Regression to the mean\n\nThis phenomenon is known as **regression to the mean.** This describes the phenomenon in which an random variable produces an extreme score on a first measurement, but a lower score on a second measurement.\n\n------------------------------------------------------------------------\n\n## Regression to the mean\n\nThis can be a threat to internal validity if interventions are applied based on first measurement scores.\n\n::::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n:::\n:::::\n\n------------------------------------------------------------------------\n\n![](/images/what.gif){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n## Another Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschool <- read_csv(\"https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/data/example2-chisq.csv\") %>% \n  mutate(Sleep_Hours_Non_Schoolnight = as.numeric(Sleep_Hours_Non_Schoolnight)) %>% \n  filter(Sleep_Hours_Non_Schoolnight < 24) #removing impossible values\n```\n:::\n\n\n------------------------------------------------------------------------\n\n## Statistical Inference\n\n-   The way the world is = our model + error\n\n-   How good is our model? Does it \"fit\" the data well?\n\n. . .\n\nTo assess how well our model fits the data, we will examine the proportion of variance in our outcome variable that can be \"explained\" by our model.\n\nTo do so, we need to partition the variance into different categories. For now, we will partition it into two categories: the variability that is captured by (explained by) our model, and variability that is not.\n\n------------------------------------------------------------------------\n\n## Partitioning variation\n\nLet's start with the formula defining the relationship between observed $Y$ and fitted $\\hat{Y}$:\n\n$$Y_i = \\hat{Y}_i + e_i$$\n\n. . .\n\nOne of the properties that we love about variance is that variances are additive when two variables are independent. In other words, if we create some variable, C, by adding together two other variables, A and B, then the variance of C is equal to the sum of the variances of A and B.\n\n. . .\n\nWhy can we use that rule in this case?\n\n::: notes\nStudents must recognize that Y-hat and e are uncorrelated, they must be the way that we've built the OLS function.\n:::\n\n------------------------------------------------------------------------\n\n## Partitioning variation\n\n::::: columns\n::: {.column width=\"50%\"}\n[$\\hat{Y}_i$ and $e_i$ must be independent from each other. Thus, the variance of $Y$ is equal to the sum of the variance of $\\hat{Y}$ and $e$.]{style=\"font-size:80%\"}\n\n$$\\large s^2_Y = s^2_{\\hat{Y}} + s^2_{e}$$\n:::\n\n::: {.column width=\"50%\"}\n[Recall that variances are sums of squares divided by N-1. Thus, all variances have the same sample size, so we can also note the following:]{style=\"font-size:80%\"}\n\n$$\\large SS_Y = SS_{\\hat{Y}} + SS_{\\text{e}}$$\n:::\n:::::\n\n------------------------------------------------------------------------\n\n[A quick note about terminology: I demonstrated these calculations using $Y$, $\\hat{Y}$ and $e$. However, you may also see the same terms written differently, to more clearly indicate the source of the variance...]{style=\"font-size:80%\"}\n\n$$ SS_Y = SS_{\\hat{Y}} + SS_{\\text{e}}$$ $$ SS_Y = SS_{\\text{Model}} + SS_{\\text{Residual}}$$\n\n[The relative magnitudes of sums of squares provides a way of identifying particularly large and important sources of variability.]{style=\"font-size:80%\"}\n\n------------------------------------------------------------------------\n\n## Coefficient of Determination\n\n$$\\Large \\frac{SS_{Model}}{SS_{Y}} = \\frac{s_{Model}^2}{s_{y}^2} = R^2$$\n\n$R^2$ represents the proportion of variance in Y that is explained by the model.\n\n. . .\n\n$\\sqrt{R^2} = R$ is the correlation between the predicted values of Y from the model and the actual values of Y\n\n$$\\large \\sqrt{R^2} = r_{Y\\hat{Y}}$$\n\n------------------------------------------------------------------------\n\n### Example\n\n\n::: {.cell highlight-output='10'}\n\n```{.r .cell-code}\nfit.1 <- lm(Sleep_Hours_Non_Schoolnight ~ Ageyears, \n           data = school)\nsummary(fit.1) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Sleep_Hours_Non_Schoolnight ~ Ageyears, data = school)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3947 -0.7306  0.3813  1.2694  4.5974 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(>|t|)    \n(Intercept) 10.52256    0.90536  11.623 <0.0000000000000002 ***\nAgeyears    -0.11199    0.05887  -1.902              0.0585 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.204 on 204 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.01743,\tAdjusted R-squared:  0.01261 \nF-statistic: 3.619 on 1 and 204 DF,  p-value: 0.05854\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_info <- augment(fit.1)\nsummary(fit.1)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01742975\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschool %>% \n  ggplot(aes(x = Ageyears, y = Sleep_Hours_Non_Schoolnight)) + \n  geom_point() + geom_smooth(method = \"lm\", se = F)\n```\n\n::: {.cell-output-display}\n![](lec-7_regression_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Example\n\nThe correlation between X and Y is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(school$Sleep_Hours_Non_Schoolnight, school$Ageyears, use = \"pairwise\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.1320218\n```\n\n\n:::\n:::\n\n\n. . .\n\nIf we square the correlation, we get:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(school$Sleep_Hours_Non_Schoolnight, school$Ageyears, use = \"pairwise\")^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01742975\n```\n\n\n:::\n:::\n\n\n. . .\n\nTa da!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit.1)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01742975\n```\n\n\n:::\n:::\n\n\n# Example in `R`\n\nTry some live coding! Also known as \"Another opportunity for Dr. Haraden to potentially embarrass himself\"\n\n<https://archive.ics.uci.edu/dataset/320/student+performance>\n\n<https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success>\n\n<https://datahub.io/collections>\n\n<https://www.kaggle.com/datasets>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
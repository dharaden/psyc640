{
  "hash": "2cadfdaa0d6a952f33e6a8d0f66ab54a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 05: Correlation & Effect Sizes\"\nsubtitle: \"Date: September 22, 2025\"\nfooter:  \"[course-website](https://dharaden.github.io/psyc640/)\"\nlogo: \"images/640_hex.png\"\nformat: \n  revealjs:\n    theme: clean.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    overview: false\n    scrollable: true\n    code-line-numbers: true\neditor: visual\nexecute:\n  echo: true\n  freeze: auto\n---\n\n## Today...\n\nExplore hypotheses, correlations and effect sizes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# File management\nlibrary(here)\n# for dplyr, ggplot2\nlibrary(tidyverse)\n# Loading data\nlibrary(rio)\n# Pretty tables\nlibrary(sjPlot)\nlibrary(kableExtra)\nlibrary(ggstatsplot)\n\n#Remove Scientific Notation \noptions(scipen=999)\n```\n:::\n\n\n# The Lady Tasting Tea 🍵\n\nHow A Simple Experiment Paved the Way for Modern Experimental Design 📊\n\nTaken from [Tweet](https://x.com/selcukorkmaz/status/1690348343363334144){target=\"_blank\"} by Selçuk Korkmaz\n\n------------------------------------------------------------------------\n\n## Setting:\n\nIn 1925, during a summer afternoon on campus, a lady, Dr. Muriel Bristol ([who has an algae species named after her](https://en.wikipedia.org/wiki/Chlamydomonas)), was handed a cup of tea by Ronald Fisher. She declined saying:\n\n*\"I prefer the flavor when the milk is poured first. I can tell when there is a difference\"*\n\nFisher, being a statistician and a white man:\n\n*\"Prove it\"*\n\n::: callout-note\n*Sometimes, groundbreaking insights arise from everyday claims!*\n:::\n\n------------------------------------------------------------------------\n\n## Designing the Test:\n\nFisher prepared 8 cups of tea; 4 with milk first & 4 with tea first\n\nDr. Muriel Bristol *correctly* identified 3 out of 4 of each! (some reports claim she identified all correctly)\n\n*Was it just chance or genuine ability?*\n\n::: callout-note\nAlways consider both outcomes. In hypothesis testing, this means setting up null and alternative hypotheses.\n:::\n\n------------------------------------------------------------------------\n\n## Birth of Hypothesis Testing:\n\nFisher framed it as a combinatorial problem\n\nIf it was mere luck/chance, the *probability* of getting all 8 correct was low\n\nThis way of thinking is the groundwork for the concept of the p-value\n\n::: callout-important\nThe p-value gives the probability of observing data (or something more extreme) given that the null hypothesis is true.\n:::\n\n# NHST\n\nThe core idea of Null Hypothesis Significance Testing (NHST) is a bit backward, but powerful:\n\n> We can't prove our research hypothesis is true. Instead, we gather evidence to show that the alternative explanation (the null hypothesis) is incredibly unlikely\n\nIt's a process of elimination and inference\n\n------------------------------------------------------------------------\n\n## The Tortured Logic of NHST\n\nWe create two hypotheses, $H_0$ and $H_1$. Usually, we care about $H_1$, not $H_0$. In fact, what we really want to know is how likely $H_1$, given our data.\n\n$$P(H_1|Data)$$ Instead, we're going to test our null hypothesis. Well, not really. We're going to assume our null hypothesis is true, and test how likely we would be to get these data.\n\n$$P(Data|H_0)$$\n\n------------------------------------------------------------------------\n\n## NHST Analogy: The Legal System\n\n-   The **Null Hypothesis (** $H_0$ **)** is the starting assumption: \"**presumed innocent**.\" In research, this means assuming there is *no effect*, *no relationship*, or *no difference*.\n\n-   **You, the researcher**, are the prosecution, gathering evidence (data) to challenge this presumption.\n\n-   Your **p-value** reflects the strength of your evidence. A small p-value *likely* means your evidence is strong.\n\n-   **Rejecting the null** is like a \"guilty\" verdict. You have enough evidence to say the initial presumption of innocence (no effect) is unlikely to be true.\n\n------------------------------------------------------------------------\n\n## p-value: Formal Definition\n\nThis is one of the most important—and misunderstood—concepts in statistics.\n\nThe **p-value** is:\n\n> The probability of observing a result **as extreme as, or more extreme than**, the one we actually observed, **assuming the null hypothesis is true.**\n\n------------------------------------------------------------------------\n\n## p-value: Formal Definition\n\nThis is one of the most important—and misunderstood—concepts in statistics.\n\n**It is NOT:**\n\n-   The probability that the null hypothesis is true\n\n-   The probability that our research hypothesis is true\n\n-   A measure of the size or importance of an effect\n\n------------------------------------------------------------------------\n\n## What are the steps of NHST?\n\n::::: columns\n::: {.column width=\"50%\"}\n1.  Define null and alternative hypothesis.\n\n2.  Set and justify alpha level (usually $\\alpha$ = .05)\n\n3.  Determine which sampling distribution ( $z$, $t$, or $\\chi^2$ for now)\n\n4.  Calculate parameters of your sampling distribution under the null.\n\n-   If $z$, calculate $\\mu$ and $\\sigma_M$\n:::\n\n::: {.column width=\"50%\"}\n5.  Calculate test statistic under the null.\n\n-   If $z$, $\\frac{\\bar{X} - \\mu}{\\sigma_M}$\n\n6.  Calculate probability of that test statistic or more extreme under the null, and compare to alpha.\n:::\n:::::\n\n------------------------------------------------------------------------\n\n## 🧠 Think about...\n\n> You run a correlational analysis to see if levels of anxiety are related to caffeine consumption. You get a correlation of 0.38 and a p-value of *p =* .045.\n\nTurn to someone next to you and in your own words, explain what the p-value is telling you.\n\n-   Looking at the p-value, what can you conclude?\n\n-   Looking at the p-value, what can't you conclude?\n\n------------------------------------------------------------------------\n\n## 🧠More thinking...\n\nWhat if you ran the same study and analyses, but found that your p-value was *p =* .052?\n\n# Visualizations of Concepts\n\n[NHST Visualizations](https://rpsychologist.com/d3/nhst/){target=\"_blank\"}\n\n[Sampling Distributions - OG](https://onlinestatbook.com/stat_sim/sampling_dist/){target=\"_blank\"}\n\n[Sampling Distributions & p-values](https://rpsychologist.com/pvalue/){target=\"_blank\"}\n\n# Break ☕\n\n# The \"How\" - Pearson's Correlation ($r$)\n\n------------------------------------------------------------------------\n\n## What is a Correlation? \n\nA statistical expression that quantifies the extent to which two continuous variables are linearly related\n\n**Association - Correlation**\n\nWill tell us 2 key pieces of information about the relationship:\n\n1.  Direction (Positive vs. Negative)\n2.  Strength (Magnitude from 0 to 1)\n\n------------------------------------------------------------------------\n\n## Example: Candy & \\# of Houses\n\n**Research Question:** Is there a relationship between the amount of candy we receive on Halloween and the \\# of houses that we go to?\n\n-   **Variables:**\n\n    -   `house_n`: Number of houses approached\n\n    -   `candy`: Amount of candy (# of pieces)\n\n-   **Hypothesis** $H_A$ : There will be a **positive correlation**. As a trick-or-treater goes to more houses, their amount of candy will increase ( $r \\neq 0$ .\n\n-   **Null Hypothesis** $H_0$: There is **no correlation** ( $r = 0$ ).\n\n------------------------------------------------------------------------\n\n### Step 1: ALWAYS visualize data\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(42)\n\nn <- 200\nhouse_n <- rnorm(n, mean = 10, sd = 2)\ncandy <- 2 * house_n + rnorm(n, mean = 0, sd = 2) \ncorr_data <- data.frame(house_n, candy)\n\ncorr_data %>% \n  ggplot(aes(house_n, candy)) + \n  geom_point(alpha = 0.7, size = 2, color = \"purple\") + \n  geom_smooth(method=\"lm\", \n              se = FALSE, \n              color = \"orange\") + \n  labs(\n    title = \"Relationship Between Num. of Houses and Amount of Candy on Halloween\",\n    x = \"Number of Houses\", \n    y = \"Amount of Candy\"\n  ) + \n  theme_grey(paper = \"black\",\n        ink = \"white\",)\n```\n\n::: {.cell-output-display}\n![](lec-5_correlation_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Association - Covariance\n\nBefore we talk about correlation, we need to take a look at covariance\n\n$$\ncov_{xy} = \\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{N-1}\n$$\n\n::: incremental\n-   Covariance can be thought of as the \"average cross product\" between two variables\n\n-   It captures the *raw/unstandardized* relationship between two variables\n\n-   Covariance matrix is the basis for many statistical analyses\n:::\n\n------------------------------------------------------------------------\n\n## Covariance to Correlation\n\nThe Pearson correlation coefficient $r$ addresses this by standardizing the covariance\n\nIt is done in the same way that we would create a $z-score$...by dividing by the standard deviation\n\n$$\nr_{xy} = \\frac{Cov(x,y)}{sd_x sd_y}\n$$\n\n------------------------------------------------------------------------\n\n## Pearson's $r$ \n\n**Range:** Varies from -1 (perfect negative correlation) and +1 (perfect positive correlation)\n\n**Assumptions:**\n\n1.  **Continuous Variables:** Both variables are measured on an interval or ratio scale.\n\n2.  **Linearity:** The relationship between the variables is linear. (This is why you **must** visualize your data!)\n\n3.  **Bivariate Normality:** Data points are normally distributed for both variables. (Pearson's $r$ is fairly robust to minor violations).\n\n------------------------------------------------------------------------\n\n### Interpreting Correlations ([5.7.5](https://learningstatisticswithr.com/book/descriptives.html#interpretingcorrelations))\n\n| Correlation  |  Strength   | Direction |\n|:------------:|:-----------:|:---------:|\n| -1.0 to -0.9 | Very Strong | Negative  |\n| -0.9 to -0.7 |   Strong    | Negative  |\n| -0.7 to -0.4 |  Moderate   | Negative  |\n| -0.4 to -0.2 |    Weak     | Negative  |\n|  -0.2 to 0   | Negligible  | Negative  |\n|   0 to 0.2   | Negligible  | Positive  |\n|  0.2 to 0.4  |    Weak     | Positive  |\n|  0.4 to 0.7  |  Moderate   | Positive  |\n|  0.7 to 0.9  |   Strong    | Positive  |\n|  0.9 to 1.0  | Very Strong | Positive  |\n\n# Pearson Correlations in R\n\n## Calculating Correlation in R\n\nNow how do we get a correlation value in R?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(corr_data$house_n, corr_data$candy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8929946\n```\n\n\n:::\n:::\n\n\nThat will give us the correlation, but we also want to know how to get our p-value\n\n------------------------------------------------------------------------\n\n### Correlation Test\n\nTo get the test of a single pair of variables, we will use the `cor.test()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(corr_data$house_n, corr_data$candy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  corr_data$house_n and corr_data$candy\nt = 27.919, df = 198, p-value < 0.00000000000000022\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8609168 0.9180000\nsample estimates:\n      cor \n0.8929946 \n```\n\n\n:::\n:::\n\n\n-   **`p-value`**: The probability of getting a correlation this strong if the null were true is virtually zero ( $p < .001$ ) . We **reject the null hypothesis**.\n\n-   **`95 percent confidence interval`**: We are 95% confident that the true population correlation is between 0.86 and 0.92. Importantly, this interval **does not contain 0**.\n\n    -   We cannot say \"There is a 95% chance that the population correlation falls in this range\"\n\n    -   CI's are based on the infinite running of the test (<https://rpsychologist.com/d3/ci/>)\n\n-   **`sample estimates: cor`**: This is our sample correlation coefficient, $r = 0.89$. This is our **effect size**.\n\n# The \"So What?\" - Effect Sizes & Practical Significance\n\n------------------------------------------------------------------------\n\n## Significance\n\nThe $p-value$ gives us the *statistical significance*. It will tell us that the effect we identified is **likely not to be 0**\n\nIt does not tell us *anything* about how large or meaningful the effect actually is. This is where the **effect size** comes in!\n\n::: callout-important\nWith a large enough sample size, even a tiny, trivial correlation (e.g., r \\< 0.1) can become statistically significant. [Think about what happens to sampling distribution when sample size increases.](https://onlinestatbook.com/stat_sim/sampling_dist/){target=\"_blank\"}\n:::\n\n**Statistical Significance** $\\neq$ **Practical Significance**\n\n------------------------------------------------------------------------\n\n## Effect Sizes: Correlation\n\nFor correlation, we have two related measures of effect size:\n\n1.  **Pearson's** $r$: The correlation coefficient itself. A standardized measure of the strength and direction of the linear association.\n\n    -   Cohen's Conventions (a rough guide): Small (∣.10∣), Medium (∣.30∣), Large (∣.50∣).\n\n    -   Our finding of $r = 0.89$ is a **large effect**.\n\n2.  **Coefficient of Determination** ($R^2$): This is simply r squared. It represents the **proportion of variance** in one variable that is \"explained\" or \"accounted for\" by the other.\n\n------------------------------------------------------------------------\n\n### Putting it Together\n\nFor our candy and housese data:\n\n-   Our correlation was $r = 0.89$\n\n-   The coefficient of determination is $R^2 = 0.89^2 = 0.792$\n\n**Interpretation:**\n\n> **\"Approximately 79% of the variance in the amount of candy that one gets on Halloween can be accounted for by the number of houses they go to.\"**\n\nThis is a powerful and practical statement about the strength of the association.\n\n# Correlations: Using real data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep_data <- import(here(\"files\", \"data\", \"Sleep_Data.csv\")) %>% \n  #create an ESS Sum Score\n  rowwise() %>% \n  mutate(ess_tot = sum(c_across(ESS1:ESS8), na.rm = TRUE))\n```\n:::\n\n\n------------------------------------------------------------------------\n\n## Age x ESS_total: Visualize\n\nLet's examine the overall correlation between Age and overall sleepiness\n\n**First: Visualize**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsleep_data %>% \n  ggplot(aes(age, ess_tot)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", se=FALSE)\n```\n\n::: {.cell-output-display}\n![](lec-5_correlation_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\nIt doesn't seem like there is much of a relationship here...\n\n------------------------------------------------------------------------\n\n## Age x ESS_total: Test\n\nLet's check the overall correlation just to see what we are finding\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(sleep_data$age, sleep_data$ess_tot)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  sleep_data$age and sleep_data$ess_tot\nt = -2.2489, df = 1450, p-value = 0.02467\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.110064846 -0.007534546\nsample estimates:\n        cor \n-0.05895518 \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Writing up a Correlation\n\nTemplate: *r*(degress of freedom) = the *r* statistic, *p* = *p* value.\n\nImagine we have conducted a study of 40 students that looked at whether IQ scores and GPA are correlated. We might report the results like this:\n\n> IQ and GPA were found to be moderately positively correlated, *r*(38) = .34, *p* = .032.\n\nOther example:\n\n> Among the students of Hogwarts University, the number of hours playing Fortnite per week and midterm exam results were negatively correlated, *r*(78) = -.45, *p* \\< .001.\n\nAnd another:\n\n> Table 1 reports descriptive statistics and correlations among variables of interest. Knowledge of Weird Al Songs was positively correlated with perceptions of humor for Dr. Haraden (*r*(49) = .79, *p \\<*.001), such that the more Weird Al songs a student knew, the more they thought Dr. Haraden was funny.\n\n------------------------------------------------------------------------\n\n## Writing up: Example \n\nWe have all of the pieces for writing up our correlation between age and sleepiness.\n\nTemplate: *r*(degress of freedom) = the *r* statistic, *p* = *p* value.\n\n> Among the students in the sample, age was negatively related to overall levels of sleepiness (*r*(1450) = -0.06, *p* = .024).\n\nWhat about our $R^2$ value?\n\n$R^2 = -0.06^2 = 0.0034$. Therefore, approximately 0.34% of the variability in the sleepiness scale (`ess_tot`) is explained by age. Is this meaningful?\n\n# Missing Values in Correlation\n\n------------------------------------------------------------------------\n\n## Handling Missing - Correlation\n\n-   Listwise Deletion (complete cases)\n\n    ::: incremental\n    -   Removes participants completely if they are missing a value being compared\n    -   Smaller Sample Sizes\n    -   Doesn't bias correlation estimate\n    :::\n\n-   Pairwise Deletion\n\n    ::: incremental\n    -   Removes participants for that single pair, but leaves information in when there are complete information\n    -   Larger Sample Sizes\n    -   Could bias estimates if there is a systematic reason things are missing\n    :::\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep_data %>% \n  select(age, ESS1:ESS8, ess_tot) %>% \n  cor(use = \"complete\") %>% \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|        |        age|       ESS1|      ESS2|       ESS3|       ESS4|       ESS5|      ESS6|      ESS7|       ESS8|    ess_tot|\n|:-------|----------:|----------:|---------:|----------:|----------:|----------:|---------:|---------:|----------:|----------:|\n|age     |  1.0000000| -0.0057641| 0.0039691| -0.0421214| -0.1059506| -0.0871016| 0.0484763| 0.0057761| -0.0206676| -0.0594733|\n|ESS1    | -0.0057641|  1.0000000| 0.3303236|  0.2784836|  0.1455593|  0.2295289| 0.1422071| 0.2206383|  0.0984444|  0.5981508|\n|ESS2    |  0.0039691|  0.3303236| 1.0000000|  0.1976108|  0.1387409|  0.2190331| 0.1748049| 0.2134999|  0.1010883|  0.5565852|\n|ESS3    | -0.0421214|  0.2784836| 0.1976108|  1.0000000|  0.2920448|  0.1948408| 0.2888370| 0.2778982|  0.2178073|  0.6121220|\n|ESS4    | -0.1059506|  0.1455593| 0.1387409|  0.2920448|  1.0000000|  0.2579808| 0.0897873| 0.2589136|  0.2409171|  0.5936136|\n|ESS5    | -0.0871016|  0.2295289| 0.2190331|  0.1948408|  0.2579808|  1.0000000| 0.0704590| 0.2821851|  0.0629151|  0.5718083|\n|ESS6    |  0.0484763|  0.1422071| 0.1748049|  0.2888370|  0.0897873|  0.0704590| 1.0000000| 0.2833070|  0.3493524|  0.4143559|\n|ESS7    |  0.0057761|  0.2206383| 0.2134999|  0.2778982|  0.2589136|  0.2821851| 0.2833070| 1.0000000|  0.2356524|  0.6088579|\n|ESS8    | -0.0206676|  0.0984444| 0.1010883|  0.2178073|  0.2409171|  0.0629151| 0.3493524| 0.2356524|  1.0000000|  0.4293874|\n|ess_tot | -0.0594733|  0.5981508| 0.5565852|  0.6121220|  0.5936136|  0.5718083| 0.4143559| 0.6088579|  0.4293874|  1.0000000|\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep_data %>% \n  select(age, ESS1:ESS8, ess_tot) %>% \n  cor(use = \"pairwise\") %>% \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|        |        age|       ESS1|      ESS2|       ESS3|       ESS4|       ESS5|      ESS6|      ESS7|       ESS8|    ess_tot|\n|:-------|----------:|----------:|---------:|----------:|----------:|----------:|---------:|---------:|----------:|----------:|\n|age     |  1.0000000| -0.0057641| 0.0039691| -0.0421214| -0.1059506| -0.0871016| 0.0484763| 0.0057761| -0.0206676| -0.0589552|\n|ESS1    | -0.0057641|  1.0000000| 0.3303236|  0.2784836|  0.1455593|  0.2295289| 0.1422071| 0.2206383|  0.0984444|  0.5981508|\n|ESS2    |  0.0039691|  0.3303236| 1.0000000|  0.1976108|  0.1387409|  0.2190331| 0.1748049| 0.2134999|  0.1010883|  0.5565852|\n|ESS3    | -0.0421214|  0.2784836| 0.1976108|  1.0000000|  0.2920448|  0.1948408| 0.2888370| 0.2778982|  0.2178073|  0.6121220|\n|ESS4    | -0.1059506|  0.1455593| 0.1387409|  0.2920448|  1.0000000|  0.2579808| 0.0897873| 0.2589136|  0.2409171|  0.5936136|\n|ESS5    | -0.0871016|  0.2295289| 0.2190331|  0.1948408|  0.2579808|  1.0000000| 0.0704590| 0.2821851|  0.0629151|  0.5718083|\n|ESS6    |  0.0484763|  0.1422071| 0.1748049|  0.2888370|  0.0897873|  0.0704590| 1.0000000| 0.2833070|  0.3493524|  0.4143559|\n|ESS7    |  0.0057761|  0.2206383| 0.2134999|  0.2778982|  0.2589136|  0.2821851| 0.2833070| 1.0000000|  0.2356524|  0.6088579|\n|ESS8    | -0.0206676|  0.0984444| 0.1010883|  0.2178073|  0.2409171|  0.0629151| 0.3493524| 0.2356524|  1.0000000|  0.4293874|\n|ess_tot | -0.0589552|  0.5981508| 0.5565852|  0.6121220|  0.5936136|  0.5718083| 0.4143559| 0.6088579|  0.4293874|  1.0000000|\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Example: Handling Missing Data\n\nImagine we've collected data from 5 students on their levels of **anxiety**, **depression**, and **sleep quality** (where a lower score means better sleep).\n\nOur goal is to run a correlation matrix to see how these three variables relate to each other. But notice, we have some missing data (`NA`).\n\n|        |         |            |               |\n|--------|---------|------------|---------------|\n| **ID** | Anxiety | Depression | Sleep Quality |\n| **1**  | 10      | 12         | 5             |\n| **2**  | 12      | 14         | 4             |\n| **3**  | 15      | `NA`       | 3             |\n| **4**  | 18      | 20         | 2             |\n| **5**  | 11      | 13         | `NA`          |\n\n------------------------------------------------------------------------\n\n### Listwise Deletion\n\n**The Rule:** \"All or nothing.\" If a participant is missing data on **any** variable in our list, their entire row is removed from the analysis.\n\n**Applying the Rule:**\n\n-   Participant 3 is missing `Depression`. They're out.\n\n-   Participant 5 is missing `Sleep Quality`. They're out.\n\n**The Resulting Data for Analysis:** This leaves us with only the participants who have complete data on all three variables. We lose 40% of our data\n\n|        |             |                |                   |\n|--------|-------------|----------------|-------------------|\n| **ID** | **Anxiety** | **Depression** | **Sleep Quality** |\n| **1**  | 10          | 12             | 5                 |\n| **2**  | 12          | 14             | 4                 |\n| **4**  | 18          | 20             | 2                 |\n\n------------------------------------------------------------------------\n\n### Pairwise Deletion\n\n**The Rule:** \"Case by case.\" For each individual correlation, we use all participants who have data for **that specific pair** of variables.\n\nLook at each pair to determine the sample sizes\n\n|        |             |                |                   |\n|--------|-------------|----------------|-------------------|\n| **ID** | **Anxiety** | **Depression** | **Sleep Quality** |\n| **1**  | 10          | 12             | 5                 |\n| **2**  | 12          | 14             | 4                 |\n| **3**  | 15          | `NA`           | 3                 |\n| **4**  | 18          | 20             | 2                 |\n| **5**  | 11          | 13             | `NA`              |\n\n------------------------------------------------------------------------\n\n### Summary\n\n|  |  |  |\n|----|----|----|\n| **Correlation Pair** | **N with Listwise Deletion** | **N with Pairwise Deletion** |\n| Anxiety & Depression | 3 | 4 |\n| Anxiety & Sleep Quality | 3 | 4 |\n| Depression & Sleep Quality | 3 | 3 |\n\n# Spearman's Rank Correaltion\n\n------------------------------------------------------------------------\n\n## Spearman's Rank Correlation\n\nWe need to be able to capture this different (ordinal) \"relationship\"\n\n-   If student 1 works more hours than student 2, then we can guarantee that student 1 will get a better grade\n\nInstead of using the amount given by the variables (\"hours studied\"), we rank the variables based on least (rank = 1) to most (rank = 10)\n\nThen we correlate the rankings with one another\n\n# Foundations of Statistics\n\nWho were those white dudes that started this?\n\n------------------------------------------------------------------------\n\n## Statistics and Eugenics\n\nThe concept of the correlation is primarily attributed to Sir Frances Galton\n\n-   He was also the founder of the [concept of eugenics](https://www.theguardian.com/commentisfree/2019/oct/03/eugenics-francis-galton-science-ideas)\n\nThe correlation coefficient was developed by his student, [Karl Pearson](https://www.britannica.com/biography/Karl-Pearson), and adapted into the ANOVA framework by [Sir Ronald Fisher](https://statmodeling.stat.columbia.edu/2020/08/01/ra-fisher-and-the-science-of-hatred/)\n\n-   Both were prominent advocates for the eugenics movement\n\n------------------------------------------------------------------------\n\n## What do we do with this info?\n\n::: incremental\n-   Never use the correlation or the later techniques developed on it? Of course not.\n\n-   Acknowledge this history? Certainly.\n\n-   [Understand how the perspectives](https://medium.com/swlh/is-statistics-racist-59cd4ddb5fa9) of Galton, Fisher, Pearson and others [shaped our practices](http://gppreview.com/2019/12/16/eugenics-ethics-statistical-analysis/)? We must! -- these are not set in stone, [nor are they necessarily the best way](https://www.forbes.com/sites/jerrybowyer/2016/01/06/beer-vs-eugenics-the-good-and-the-bad-uses-of-statistics/?sh=3114a0c82a14) to move forward.\n:::\n\n------------------------------------------------------------------------\n\n## Be aware of the assumptions\n\n::: incremental\n-   Statistics are often thought of as being absent of bias...they are just numbers\n\n-   Statistical significance was a way to avoid talking about nuance or degree.\n\n-   \"Correlation does not imply causation\" was a refutation of work demonstrating associations between environment and poverty.\n\n-   Need to be particularly mindful of our goals as scientists and how they can influence the way we interpret the findings\n:::\n\n# Fancy Tables\n\n------------------------------------------------------------------------\n\n## Correlation Tables\n\nBefore we used the `cor()` function to create a correlation matrix of our variables\n\n***But what is missing?***\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#sleep_data %>% \n#  select(ESS1m1:ESS8m1) %>% \n#  cor(use = \"complete\") %>% \n#  kable()\n```\n:::\n\n\n## Correlation Tables - [sjPlot](https://strengejacke.github.io/sjPlot/reference/tab_corr.html)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#sleep_data %>% \n#  select(ESS1m1:ESS8m1) %>% \n#  cor(use = \"complete\") %>% \n#  tab_corr(na.deletion = \"listwise\", triangle = \"lower\")\n```\n:::\n\n\n------------------------------------------------------------------------\n\n## Correlation Tables - [sjPlot](https://strengejacke.github.io/sjPlot/reference/tab_corr.html)\n\nSo many different cusomizations for this type of plot\n\nCan add titles, indicate what missingness and method\n\nSaves you a TON of time when putting it into a manuscript\n\n------------------------------------------------------------------------\n\n## Writing up a Correlation\n\nTemplate: *r*(degress of freedom) = the *r* statistic, *p* = *p* value.\n\nImagine we have conducted a study of 40 students that looked at whether IQ scores and GPA are correlated. We might report the results like this:\n\n> IQ and GPA were found to be moderately positively correlated, *r*(38) = .34, *p* = .032.\n\nOther example:\n\n> Among the students of Hogwarts University, the number of hours playing Fortnite per week and midterm exam results were negatively correlated, *r*(78) = -.45, *p* \\< .001.\n\nAnd another:\n\n> Table 1 reports descriptive statistics and correlations among variables of interest. Knowledge of Weird Al Songs was positively correlated with perceptions of humor for Dr. Haraden (*r*(49) = .79, *p \\<*.001), such that the more Weird Al songs a student knew, the more they thought Dr. Haraden was funny.\n",
    "supporting": [
      "lec-5_correlation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
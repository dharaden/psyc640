{
  "hash": "5a8917a3ddb7ef63a205d676d650562b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 05: Correlation & Effect Sizes\"\nsubtitle: \"Date: September 22, 2025\"\nfooter:  \"[course-website](https://dharaden.github.io/psyc640/)\"\nlogo: \"images/640_hex.png\"\nformat: \n  revealjs:\n    theme: clean.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    overview: false\n    scrollable: true\n    code-line-numbers: true\neditor: visual\nexecute:\n  echo: true\n  freeze: auto\n---\n\n## Today...\n\nExplore hypotheses, correlations and effect sizes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# File management\nlibrary(here)\n# for dplyr, ggplot2\nlibrary(tidyverse)\n# Loading data\nlibrary(rio)\n# Pretty tables\nlibrary(sjPlot)\nlibrary(kableExtra)\n\n#Remove Scientific Notation \noptions(scipen=999)\n```\n:::\n\n\n# The Lady Tasting Tea 🍵\n\nHow A Simple Experiment Paved the Way for Modern Experimental Design 📊\n\nTaken from [Tweet](https://x.com/selcukorkmaz/status/1690348343363334144){target=\"_blank\"} by Selçuk Korkmaz\n\n------------------------------------------------------------------------\n\n## Setting:\n\nIn 1925, during a summer afternoon on campus, a lady, Dr. Muriel Bristol ([who has an algae species named after her](https://en.wikipedia.org/wiki/Chlamydomonas)), was handed a cup of tea by Ronald Fisher. She declined saying:\n\n*\"I prefer the flavor when the milk is poured first. I can tell when there is a difference\"*\n\nFisher, being a statistician and a white man:\n\n*\"Prove it\"*\n\n::: callout-note\n*Sometimes, groundbreaking insights arise from everyday claims!*\n:::\n\n------------------------------------------------------------------------\n\n## Designing the Test:\n\nFisher prepared 8 cups of tea; 4 with milk first & 4 with tea first\n\nDr. Muriel Bristol *correctly* identified 3 out of 4 of each! (some reports claim she identified all correctly)\n\n*Was it just chance or genuine ability?*\n\n::: callout-note\nAlways consider both outcomes. In hypothesis testing, this means setting up null and alternative hypotheses.\n:::\n\n------------------------------------------------------------------------\n\n## Birth of Hypothesis Testing:\n\nFisher framed it as a combinatorial problem\n\nIf it was mere luck/chance, the *probability* of getting all 8 correct was low\n\nThis way of thinking is the groundwork for the concept of the p-value\n\n::: callout-important\nThe p-value gives the probability of observing data (or something more extreme) given that the null hypothesis is true.\n:::\n\n# NHST\n\nThe core idea of Null Hypothesis Significance Testing (NHST) is a bit backward, but powerful:\n\n> We can't prove our research hypothesis is true. Instead, we gather evidence to show that the alternative explanation (the null hypothesis) is incredibly unlikely\n\nIt's a process of elimination and inference\n\n------------------------------------------------------------------------\n\n## The Tortured Logic of NHST\n\nWe create two hypotheses, $H_0$ and $H_1$. Usually, we care about $H_1$, not $H_0$. In fact, what we really want to know is how likely $H_1$, given our data.\n\n$$P(H_1|Data)$$ Instead, we're going to test our null hypothesis. Well, not really. We're going to assume our null hypothesis is true, and test how likely we would be to get these data.\n\n$$P(Data|H_0)$$\n\n------------------------------------------------------------------------\n\n## NHST Analogy: The Legal System\n\n-   The **Null Hypothesis (** $H_0$ **)** is the starting assumption: \"**presumed innocent**.\" In research, this means assuming there is *no effect*, *no relationship*, or *no difference*.\n\n-   **You, the researcher**, are the prosecution, gathering evidence (data) to challenge this presumption.\n\n-   Your **p-value** reflects the strength of your evidence. A small p-value *likely* means your evidence is strong.\n\n-   **Rejecting the null** is like a \"guilty\" verdict. You have enough evidence to say the initial presumption of innocence (no effect) is unlikely to be true.\n\n------------------------------------------------------------------------\n\n## p-value: Formal Definition\n\nThis is one of the most important—and misunderstood—concepts in statistics.\n\nThe **p-value** is:\n\n> The probability of observing a result **as extreme as, or more extreme than**, the one we actually observed, **assuming the null hypothesis is true.**\n\n------------------------------------------------------------------------\n\n## p-value: Formal Definition\n\nThis is one of the most important—and misunderstood—concepts in statistics.\n\n**It is NOT:**\n\n-   The probability that the null hypothesis is true\n\n-   The probability that our research hypothesis is true\n\n-   A measure of the size or importance of an effect\n\n------------------------------------------------------------------------\n\n## What are the steps of NHST?\n\n::::: columns\n::: {.column width=\"50%\"}\n1.  Define null and alternative hypothesis.\n\n2.  Set and justify alpha level (usually $\\alpha$ = .05)\n\n3.  Determine which sampling distribution ( $z$, $t$, or $\\chi^2$ for now)\n\n4.  Calculate parameters of your sampling distribution under the null.\n\n-   If $z$, calculate $\\mu$ and $\\sigma_M$\n:::\n\n::: {.column width=\"50%\"}\n5.  Calculate test statistic under the null.\n\n-   If $z$, $\\frac{\\bar{X} - \\mu}{\\sigma_M}$\n\n6.  Calculate probability of that test statistic or more extreme under the null, and compare to alpha.\n:::\n:::::\n\n------------------------------------------------------------------------\n\n## 🧠 Think about...\n\n> You run a correlational analysis to see if levels of anxiety are related to caffeine consumption. You get a correlation of 0.38 and a p-value of *p =* .045.\n\nTurn to someone next to you and in your own words, explain what the p-value is telling you.\n\n-   Looking at the p-value, what can you conclude?\n\n-   Looking at the p-value, what can't you conclude?\n\n------------------------------------------------------------------------\n\n## 🧠More thinking...\n\nWhat if you ran the same study and analyses, but found that your p-value was *p =* .052?\n\n# Visualizations of Concepts\n\n[NHST Visualizations](https://rpsychologist.com/d3/nhst/){target=\"_blank\"}\n\n[Sampling Distributions - OG](https://onlinestatbook.com/stat_sim/sampling_dist/){target=\"_blank\"}\n\n[Sampling Distributions & p-values](https://rpsychologist.com/pvalue/){target=\"_blank\"}\n\n# Break ☕\n\n------------------------------------------------------------------------\n\n## Import cleaned up file\n\nLast class we created a new .csv file named `named_Sleep_Data`. We will continue to use that! Let's make sure to import that data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remember that your path will probably be different\n#sleep_data <- read.csv(here(\"lectures\", \n#                            \"data\", \n#                            \"named_Sleep_Data.csv\"))\n```\n:::\n\n\n# Relationships between variables\n\n## Association - Correlation\n\nExamine the relationship between two continuous variables\n\nSimilar to the mean and standard deviation, but it is *between* two variables\n\nTypically displayed as a `scatterplot`\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(42)\n\nn <- 200\nx <- rnorm(n, mean = 10, sd = 2)\ny <- 2 * x + rnorm(n, mean = 0, sd = 2) \ncorr_data <- data.frame(x,y)\n\ncorr_data %>% \n  ggplot(aes(x,y)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", \n              se = FALSE) + \n  labs(\n    x = \"Number of Houses\", \n    y = \"Amount of Candy\"\n  )\n```\n\n::: {.cell-output-display}\n![](lec-5_correlation_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Association - Covariance\n\nBefore we talk about correlation, we need to take a look at covariance\n\n$$\ncov_{xy} = \\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{N-1}\n$$\n\n::: incremental\n-   Covariance can be thought of as the \"average cross product\" between two variables\n\n-   It captures the *raw/unstandardized* relationship between two variables\n\n-   Covariance matrix is the basis for many statistical analyses\n:::\n\n------------------------------------------------------------------------\n\n## Covariance to Correlation\n\nThe Pearson correlation coefficient $r$ addresses this by standardizing the covariance\n\nIt is done in the same way that we would create a $z-score$...by dividing by the standard deviation\n\n$$\nr_{xy} = \\frac{Cov(x,y)}{sd_x sd_y}\n$$\n\n------------------------------------------------------------------------\n\n## Correlations\n\n-   Tells us: How much 2 variables are *linearly* related\n\n-   Range: -1 to +1\n\n-   Most common and basic effect size measure\n\n-   Is used to build the regression model\n\n------------------------------------------------------------------------\n\n### Interpreting Correlations ([5.7.5](https://learningstatisticswithr.com/book/descriptives.html#interpretingcorrelations))\n\n| Correlation  |  Strength   | Direction |\n|:------------:|:-----------:|:---------:|\n| -1.0 to -0.9 | Very Strong | Negative  |\n| -0.9 to -0.7 |   Strong    | Negative  |\n| -0.7 to -0.4 |  Moderate   | Negative  |\n| -0.4 to -0.2 |    Weak     | Negative  |\n|  -0.2 to 0   | Negligible  | Negative  |\n|   0 to 0.2   | Negligible  | Positive  |\n|  0.2 to 0.4  |    Weak     | Positive  |\n|  0.4 to 0.7  |  Moderate   | Positive  |\n|  0.7 to 0.9  |   Strong    | Positive  |\n|  0.9 to 1.0  | Very Strong | Positive  |\n\n# Pearson Correlations in R\n\n## Calculating Correlation in R\n\nNow how do we get a correlation value in R?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(corr_data$x, corr_data$y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8929946\n```\n\n\n:::\n:::\n\n\nThat will give us the correlation, but we also want to know how to get our p-value\n\n------------------------------------------------------------------------\n\n### Correlation Test\n\nTo get the test of a single pair of variables, we will use the `cor.test()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(x, y, data = corr_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  x and y\nt = 27.919, df = 198, p-value < 0.00000000000000022\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8609168 0.9180000\nsample estimates:\n      cor \n0.8929946 \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Using real data - Epworth Sleepiness Scale\n\nSo far we have been looking at single variables, but we often care about the relationships between multiple variables in a dataset\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#sleep_data %>% \n#  select(ESS1m1:ESS8m1) %>% \n#  cor() %>% \n#  kable()\n```\n:::\n\n\n# Missing Values - `na.rm = TRUE`?\n\n------------------------------------------------------------------------\n\n## Handling Missing - Correlation\n\n-   Listwise Deletion (complete cases)\n\n    ::: incremental\n    -   Removes participants completely if they are missing a value being compared\n    -   Smaller Sample Sizes\n    -   Doesn't bias correlation estimate\n    :::\n\n-   Pairwise Deletion\n\n    ::: incremental\n    -   Removes participants for that single pair, but leaves information in when there are complete information\n    -   Larger Sample Sizes\n    -   Could bias estimates if there is a systematic reason things are missing\n    :::\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#sleep_data %>% \n#  select(ESS1m1:ESS8m1) %>% \n#  cor(use = \"complete\") %>% \n#  kable()\n```\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#sleep_data %>% \n#  select(ESS1m1:ESS8m1) %>% \n#  cor(use = \"pairwise\") %>% \n#  kable()\n```\n:::\n\n\n# Spearman's Rank Correaltion\n\n------------------------------------------------------------------------\n\n## Spearman's Rank Correlation\n\nWe need to be able to capture this different (ordinal) \"relationship\"\n\n-   If student 1 works more hours than student 2, then we can guarantee that student 1 will get a better grade\n\nInstead of using the amount given by the variables (\"hours studied\"), we rank the variables based on least (rank = 1) to most (rank = 10)\n\nThen we correlate the rankings with one another\n\n# Foundations of Statistics\n\nWho were those white dudes that started this?\n\n------------------------------------------------------------------------\n\n## Statistics and Eugenics\n\nThe concept of the correlation is primarily attributed to Sir Frances Galton\n\n-   He was also the founder of the [concept of eugenics](https://www.theguardian.com/commentisfree/2019/oct/03/eugenics-francis-galton-science-ideas)\n\nThe correlation coefficient was developed by his student, [Karl Pearson](https://www.britannica.com/biography/Karl-Pearson), and adapted into the ANOVA framework by [Sir Ronald Fisher](https://statmodeling.stat.columbia.edu/2020/08/01/ra-fisher-and-the-science-of-hatred/)\n\n-   Both were prominent advocates for the eugenics movement\n\n------------------------------------------------------------------------\n\n## What do we do with this info?\n\n::: incremental\n-   Never use the correlation or the later techniques developed on it? Of course not.\n\n-   Acknowledge this history? Certainly.\n\n-   [Understand how the perspectives](https://medium.com/swlh/is-statistics-racist-59cd4ddb5fa9) of Galton, Fisher, Pearson and others [shaped our practices](http://gppreview.com/2019/12/16/eugenics-ethics-statistical-analysis/)? We must! -- these are not set in stone, [nor are they necessarily the best way](https://www.forbes.com/sites/jerrybowyer/2016/01/06/beer-vs-eugenics-the-good-and-the-bad-uses-of-statistics/?sh=3114a0c82a14) to move forward.\n:::\n\n------------------------------------------------------------------------\n\n## Be aware of the assumptions\n\n::: incremental\n-   Statistics are often thought of as being absent of bias...they are just numbers\n\n-   Statistical significance was a way to avoid talking about nuance or degree.\n\n-   \"Correlation does not imply causation\" was a refutation of work demonstrating associations between environment and poverty.\n\n-   Need to be particularly mindful of our goals as scientists and how they can influence the way we interpret the findings\n:::\n\n# Fancy Tables\n\n------------------------------------------------------------------------\n\n## Correlation Tables\n\nBefore we used the `cor()` function to create a correlation matrix of our variables\n\n***But what is missing?***\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#sleep_data %>% \n#  select(ESS1m1:ESS8m1) %>% \n#  cor(use = \"complete\") %>% \n#  kable()\n```\n:::\n\n\n## Correlation Tables - [sjPlot](https://strengejacke.github.io/sjPlot/reference/tab_corr.html)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#sleep_data %>% \n#  select(ESS1m1:ESS8m1) %>% \n#  cor(use = \"complete\") %>% \n#  tab_corr(na.deletion = \"listwise\", triangle = \"lower\")\n```\n:::\n\n\n------------------------------------------------------------------------\n\n## Correlation Tables - [sjPlot](https://strengejacke.github.io/sjPlot/reference/tab_corr.html)\n\nSo many different cusomizations for this type of plot\n\nCan add titles, indicate what missingness and method\n\nSaves you a TON of time when putting it into a manuscript\n\n------------------------------------------------------------------------\n\n## Writing up a Correlation\n\nTemplate: *r*(degress of freedom) = the *r* statistic, *p* = *p* value.\n\nImagine we have conducted a study of 40 students that looked at whether IQ scores and GPA are correlated. We might report the results like this:\n\n> IQ and GPA were found to be moderately positively correlated, *r*(38) = .34, *p* = .032.\n\nOther example:\n\n> Among the students of Hogwarts University, the number of hours playing Fortnite per week and midterm exam results were negatively correlated, *r*(78) = -.45, *p* \\< .001.\n\nAnd another:\n\n> Table 1 reports descriptive statistics and correlations among variables of interest. Knowledge of Weird Al Songs was positively correlated with perceptions of humor for Dr. Haraden (*r*(49) = .79, *p \\<*.001), such that the more Weird Al songs a student knew, the more they thought Dr. Haraden was funny.\n",
    "supporting": [
      "lec-5_correlation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
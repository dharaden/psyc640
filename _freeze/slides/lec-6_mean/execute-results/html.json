{
  "hash": "e20bb0f075b6f4aa44729409ce3a3701",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 06: Comparing Means\"\nsubtitle: \"Date: September 29, 2025\"\nfooter:  \"[course-website](https://dharaden.github.io/psyc640/)\"\nlogo: \"images/640_hex.png\"\nformat: \n  revealjs:\n    theme: clean.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    overview: false\n    scrollable: true\n    code-line-numbers: true\neditor: visual\nexecute:\n  echo: true\n  freeze: auto\n---\n\n## Today...\n\nHIGHLIGHT THE USE OF THE `rm()` FUNCTION TO GET RID OF THINGS IN YOUR ENVIRONMENT\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# File management\nlibrary(here)\n# for dplyr, ggplot2\nlibrary(tidyverse)\n# Pretty tables\nlibrary(sjPlot)\nlibrary(ggstatsplot)\n# making tests easier\nlibrary(lsr) ## NEW\n# Getting marginal means\nlibrary(emmeans)\n\n#Remove Scientific Notation \noptions(scipen=999)\n```\n:::\n\n\n# Comparing Means\n\nConverting to a t-score and checking against the t-distribution\n\n-   One-Sample\n\n-   Independent Sample\n\n-   Paired Sample\n\n------------------------------------------------------------------------\n\n## but first...Degrees of Freedom\n\n**Degrees of Freedom:** *the number of values in the final calculation of a statistic that are free to vary*\n\n**Example**: Drawing a triangle\n\n**‚ùìQuestion:** If you have a sample of 5 scores and you know the mean is 3, how many of those scores can you *freely* change?\n\n------------------------------------------------------------------------\n\n## One Sample t-test\n\n*t*-tests were developed by William Sealy Gosset, who was a chemist studying the grains used in making beer. (He worked for Guinness.)\n\n-   Specifically, he wanted to know whether particular strains of grain made better or worse beer than the standard.\n\n-   He developed the *t*-test, to test small samples of beer against a population with an unknown standard deviation.\n\n    -   Probably had input from Karl Pearson and Ronald Fisher\n\n-   Published this as \"Student\" because Guinness didn't want these tests tied to the production of beer.\n\n------------------------------------------------------------------------\n\n***One-sample tests compare your given sample with a \"known\" population***\n\nResearch question: does this sample come from this population?\n\n**Hypotheses**\n\n-   $H_0$: Yes, this sample comes from this population.\n\n-   $H_1$: No, this sample comes from a different population.\n\n------------------------------------------------------------------------\n\nTo calculate the t-statistic, we generally use this formula:\n\n$$t_{df=N-1} = \\frac{\\bar{X}-\\mu}{\\frac{\\hat{\\sigma}}{\\sqrt{N}}}$$\n\nThe heavier tails of the t-distribution, especially for small N, are the penalty we pay for having to estimate the population standard deviation from the sample.\n\n------------------------------------------------------------------------\n\n![](/images/t-to-z.png){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n## Example\n\nFor examples today, we will use a dataset from Cards Against Humanity's Pulse of the Nation survey (<https://thepulseofthenation.com/>)\n\n-   <a href=\"/files/data/CAH.csv\" download=\"CAH.csv\">Cards Against Humanity Data (.csv)</a>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## We are using read_csv() this time\n## import() was doing something strange with missing values\ncah <- read_csv(here(\"files\", \"data\", \"CAH.csv\")) %>% \n  janitor::clean_names() \n\nhead(cah)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 √ó 16\n     id income gender   age age_range political_affiliation education  ethnicity\n  <dbl>  <dbl> <chr>  <dbl> <chr>     <chr>                 <chr>      <chr>    \n1     1   8000 Female    64 55-64     Democrat              College d‚Ä¶ White    \n2     2  68000 Female    56 55-64     Democrat              High scho‚Ä¶ Black    \n3     3  46000 Male      63 55-64     Independent           Some coll‚Ä¶ White    \n4     4  51000 Male      48 45-54     Republican            High scho‚Ä¶ White    \n5     5 100000 Female    32 25-34     Democrat              Some coll‚Ä¶ White    \n6     6  54000 Female    64 55-64     Democrat              Some coll‚Ä¶ White    \n# ‚Ñπ 8 more variables: marital_status <chr>, climate_change <chr>,\n#   transformers <dbl>, books <dbl>, ghosts <chr>, spending <chr>,\n#   choice <chr>, shower_pee <chr>\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### **Assumptions of the one-sample *t*-test**\n\n**Normality.** We assume the sampling distribution of the mean is normally distributed. Under what two conditions can we be assured that this is true?\n\n**Independence.** Observations in the dataset are not associated with one another. Put another way, collecting a score from Participant A doesn't tell me anything about what Participant B will say. How can we be safe in this assumption?\n\n------------------------------------------------------------------------\n\n### A brief example\n\nUsing the Cards Against Humanity data, we find that participants identified having approximately 22.33 ( $sd = 75.87$ ) books in their home. We know that the average household has approximately 50 books. How does this sample represent the larger united states?\n\n**Hypotheses**\n\n$H_0: \\mu = 50$\n\n$H_1: \\mu \\neq 50$\n\n------------------------------------------------------------------------\n\n::::: columns\n::: {.column width=\"50%\"}\n$$\\mu = 50$$\n\n$$N = 1000$$\n\n$$ \\bar{X} = 22.33 $$\n\n$$ s = 75.87 $$\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(x = cah$books, mu = 50,        \n       alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  cah$books\nt = -11.399, df = 976, p-value < 0.00000000000000022\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 17.56785 27.09438\nsample estimates:\nmean of x \n 22.33112 \n```\n\n\n:::\n:::\n\n:::\n:::::\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlsr::oneSampleTTest(x = cah$books, mu = 50, \n                    one.sided = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One sample t-test \n\nData variable:   cah$books \n\nDescriptive statistics: \n             books\n   mean     22.331\n   std dev. 75.869\n\nHypotheses: \n   null:        population mean equals 50 \n   alternative: population mean not equal to 50 \n\nTest results: \n   t-statistic:  -11.399 \n   degrees of freedom:  976 \n   p-value:  <.001 \n\nOther information: \n   two-sided 95% confidence interval:  [17.568, 27.094] \n   estimated effect size (Cohen's d):  0.365 \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Cohen's D\n\nCohen suggested one of the most common effect size estimates---the standardized mean difference---useful when comparing a group mean to a population mean or two group means to each other.\n\n$$\\delta = \\frac{\\mu_1 - \\mu_0}{\\sigma} \\approx d = \\frac{\\bar{X}-\\mu}{\\hat{\\sigma}}$$\n\nCohen's d is in the standard deviation (Z) metric.\n\n------------------------------------------------------------------------\n\nCohens's d for these data is $0.365$. In other words, the sample mean differs from the population mean by $0.365$ standard deviation units.\n\nCohen (1988) suggests the following guidelines for interpreting the size of d:\n\n::: nonincremental\n-   .2 = Small\n\n-   .5 = Medium\n\n-   .8 = Large\n:::\n\n[Cohen, J. (1988), Statistical power analysis for the behavioral sciences (2nd Ed.). Hillsdale: Lawrence Erlbaum.]{style=\"font-size:30px;\"}\n\n------------------------------------------------------------------------\n\n### The usefulness of the one-sample *t*-test\n\nHow often will you conducted a one-sample *t*-test on raw data?\n\n-   (Probably) never\n\nHow often will you come across one-sample *t*-tests?\n\n-   (Probably) a lot!\n\nThe one-sample *t*-test is used to test coefficients in a model.\n\n# YOUR TURN üíª\n\n# Independent Samples t-test\n\nTwo different types: Student's & Welch's\n\n-   Start with Student's t-test which assumes equal variances between the groups\n\n$$ t = \\frac{\\bar{X_1} - \\bar{X_2}}{SE(\\bar{X_1} - \\bar{X_2})} $$\n\n------------------------------------------------------------------------\n\n## Student's t-test\n\n$$ H_0 : \\mu_1 = \\mu_2  \\ \\  H_1 : \\mu_1 \\neq \\mu_2 $$\n\n![](/images/student_H.png){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Student's t-test: Calculate SE\n\nAre able to use a pooled variance estimate\n\nBoth variances/standard deviations are assumed to be equal\n\nTherefore:\n\n$$ SE(\\bar{X_1} - \\bar{X_2}) = \\hat{\\sigma} \\sqrt{\\frac{1}{N_1} + \\frac{1}{N_2}} $$\n\nWe are calculating the **Standard Error of the Difference between means**\n\nDegrees of Freedom: Total N - 2\n\n------------------------------------------------------------------------\n\n### Student's t-test\n\nLet's try it out using the traditional `t.test()` function\n\nDifference in Books by Belief in Ghosts\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(formula = books ~ ghosts,\n       data = cah, var.equal = TRUE )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  books by ghosts\nt = -1.3642, df = 951, p-value = 0.1728\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n95 percent confidence interval:\n -16.98193   3.05404\nsample estimates:\n mean in group No mean in group Yes \n         19.86525          26.82920 \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Welch's t-test\n\n$$ H_0 : \\mu_1 = \\mu_2  \\ \\  H_1 : \\mu_1 \\neq \\mu_2 $$\n\n![](/images/welch_H.png){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Welch's t-test: Calculate SE\n\nSince the variances are not equal, we have to estimate the SE differently\n\n$$ SE(\\bar{X_1} - \\bar{X_2}) = \\sqrt{\\frac{\\hat{\\sigma_1^2}}{N_1} + \\frac{\\hat{\\sigma_2^2}}{N_2}} $$\n\nDegrees of Freedom is also very different:\n\n![](/images/welch_df.png){fig-align=\"center\" width=\"380\"}\n\n------------------------------------------------------------------------\n\n### Welch's t-test: In R (classic)\n\nLet's try it out using the traditional `t.test()` function...turns out it is pretty straightforward\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(formula = books ~ ghosts,\n       data = cah, var.equal = FALSE )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  books by ghosts\nt = -1.2319, df = 542.85, p-value = 0.2185\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n95 percent confidence interval:\n -18.068820   4.140927\nsample estimates:\n mean in group No mean in group Yes \n         19.86525          26.82920 \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Cool Visualizations\n\nThe library [ggstatsplot](https://indrajeetpatil.github.io/ggstatsplot/) has some wonderful visualizations of various tests\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggstatsplot::ggbetweenstats(   \n  data  = cah,   \n  x     = ghosts,   \n  y     = books,   \n  title = \"Distribution of books by belief in ghosts\" )\n```\n\n::: {.cell-output-display}\n![](lec-6_mean_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Interpreting and writing up an independent samples t-test\n\n::: {style=\"font-size: 30px\"}\nThe first sentence usually conveys some descriptive information about the two groups you were comparing. Then you identify the type of test you conducted and what was determined (be sure to include the \"stat block\" here as well with the t-statistic, df, p-value, CI and Effect size). Finish it up by putting that into person words and saying what that means.\n:::\n\n> ::: {style=\"font-size: 30px\"}\n> The mean amount of books in the household for the group who did not believe in ghosts was 19.9 (SD = 61.2), while the mean for those who believed in ghosts was 26.8 (SD = 96.4). A Student's independent samples t-test showed that there was not a significant mean difference (*t*(951)=-1.364, *p*=.17, $CI_{95}$=\\[-16.98, 3.05\\]). This suggests that there is no difference in amount of books in their household as a function of belief in ghosts.\n> :::\n\n# YOUR TURN üíª\n\n------------------------------------------------------------------------\n\n## Paired Samples $t$-Test\n\n[Chapter 13.5 - Learning Stats with R](https://learningstatisticswithr.com/book/ttest.html#pairedsamplesttest)\n\nAlso called \"Dependent Samples t-test\"\n\n-   We have been testing means between two *independent* samples. Participants may be randomly assigned to the separate groups\n\n    -   This is limited to those types of study designs, but what if we have repeated measures?\n\n-   We will then need to compare scores across people...the samples we are comparing now *depend* on one another and are *paired*\n\n------------------------------------------------------------------------\n\n### Paired Samples $t$-test\n\nEach of the repeated measures (or pairs) can be viewed as a *difference score*\n\nThis reduces the analysis to a one-sample t-test of the *difference score*\n\n-   We are comparing the sample (i.e., difference scores) to a population $\\mu$ = 0\n\n------------------------------------------------------------------------\n\n### Assumptions: Paired Samples\n\nThe variable of interest (*difference scores*):\n\n-   Continuous (Interval/Ratio)\n\n-   Have 2 groups (and only two groups) that are matched\n\n-   Normally Distributed\n\n------------------------------------------------------------------------\n\n### Why paired samples??\n\nPreviously, we looked at independent samples $t$-tests, which we *could* do here as well (nobody will yell at you)\n\n-   However, this would violate the assumption that the data points are independent of one another!\n\n-   Within vs. Between-subjects\n\n------------------------------------------------------------------------\n\n### Within vs. Between Subjects\n\n![](/images/between-subjects-design.avif){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Paired Samples: Single Sample\n\nInstead of focusing on these variables as being separate/independent, we need to be able to account for their dependency on one another\n\nThis is done by calculating a *difference* or *change* score for each participant\n\n$$ D_i = X_{i1} - X_{i2} $$\n\nNotice: The equation is set up as `variable1` minus `variable2`. This will be important when we interpret the results\n\n------------------------------------------------------------------------\n\n### Paired Samples: Hypotheses & $t$-statistic\n\nThe hypotheses would then be:\n\n$$ H_0:  \\mu_D = 0; H_1: \\mu_D \\neq 0 $$\n\nAnd to calculate our t-statistic: ¬†¬†¬†¬†¬†¬†$t_{df=n-1} = \\frac{\\bar{D}}{SE(D)}$\n\nwhere the Standard Error of the difference score is: ¬†¬†¬†¬†¬†¬† $\\frac{\\hat{\\sigma_D}}{\\sqrt{N}}$\n\n------------------------------------------------------------------------\n\n## Review of the t-test process\n\n1.  Collect Sample and define hypotheses\n\n2.  Set alpha level\n\n3.  Determine the sampling distribution ($t$ distribution for now)\n\n4.  Identify the critical value that corresponds to alpha and *df*\n\n5.  Calculate test statistic for sample collected\n\n6.  Inspect & compare statistic to critical value; Calculate probability\n\n# Example 1: Simple (by hand)\n\nParticipants are placed in two differently colored rooms (counterbalanced) and are asked to rate overall happiness levels after spending 5 minutes inside the rooms. There are no windows, but there is a nice lamp and chair.\n\nHypotheses:\n\n-   $H_0:$ There is no difference in ratings of happiness between the rooms ( $\\mu = 0$ )\n\n-   $H_1:$ There is a difference in ratings of happiness between the rooms ( $\\mu \\neq 0$ )\n\n------------------------------------------------------------------------\n\n| Participant | Blue Room Score | Orange Room Score | Difference ($X_{iB} - X_{iO}$) |\n|:--:|:--:|:--:|:--:|\n| 1 | 3 | 6 | -3 |\n| 2 | 9 | 9 | 0 |\n| 3 | 2 | 10 | -8 |\n| 4 | 9 | 6 | 3 |\n| 5 | 5 | 2 | 3 |\n| 6 | 5 | 7 | -2 |\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nex1 <- data.frame(id = c(1,2,3,4,5,6),\n                  blue = c(3,9,2,9,5,5),    \n                  orange = c(6,9,10,6,2,7),   \n                  diff_score = c(-3, 0, -8, 3, 3, -2))\n  head(ex1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id blue orange diff_score\n1  1    3      6         -3\n2  2    9      9          0\n3  3    2     10         -8\n4  4    9      6          3\n5  5    5      2          3\n6  6    5      7         -2\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Determining $t$-crit\n\nCan look things up using a [t-table](https://www.tdistributiontable.com/) where you need the degrees of freedom and the alpha\n\nBut we have R to do those things for us:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#the qt() function is for a 1 tailed test, so we are having to divide it in half to get both tails \n\nalpha <- 0.05 \nn <- nrow(ex1) \nt_crit <- qt(alpha/2, n-1) \nt_crit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.570582\n```\n\n\n:::\n:::\n\n\n## Calculating t\n\nLet's get all of the information for the sample we are focusing on (difference scores):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- mean(ex1$diff_score) \nd \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.166667\n```\n\n\n:::\n\n```{.r .cell-code}\nsd_diff <- sd(ex1$diff_score) \nsd_diff\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.167333\n```\n\n\n:::\n:::\n\n\n## Calculating t\n\nNow we can calculate our $t$-statistic: $$t_{df=n-1} = \\frac{\\bar{D}}{\\frac{sd_{diff}}{\\sqrt{n}}}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt_stat <- d/(sd_diff/(sqrt(n))) \nt_stat  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.6857474\n```\n\n\n:::\n\n```{.r .cell-code}\n#Probability of this t-statistic  \np_val <- pt(t_stat, n-1)*2 \np_val\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5233677\n```\n\n\n:::\n:::\n\n\n## Make a decision\n\nHypotheses:\n\n::: nonincremental\n-   $H_0:$ There is no difference in ratings of happiness between the rooms ( $\\mu = 0$ )\n\n-   $H_1:$ There is a difference in ratings of happiness between the rooms ( $\\mu \\neq 0$ )\n:::\n\n| $alpha$ |          $t-crit$          |    $t-statistic$     |      $p-value$      |\n|:-------:|:--------------------------:|:--------------------:|:-------------------:|\n|  0.05   | $\\pm$ -2.57 | -0.69 | 0.52 |\n\n**What can we conclude??**\n\n# Example 2: Data in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstate_school <- read_csv(\"https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/data/NM-NY_CAS.csv\") %>% \n  #create an ID variable\n  rowid_to_column(\"id\")\n```\n:::\n\n\n------------------------------------------------------------------------\n\n## Let's Look at the data\n\nResearch Question: Is there a difference between school nights and weekend nights for amount of time slept?\n\nOnly looking at the variables that we are potentially interested in:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstate_school %>%    \n  select(id, Gender, Ageyears, Sleep_Hours_Schoolnight, Sleep_Hours_Non_Schoolnight) %>%    \n  head() #look at first few observations\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 √ó 5\n     id Gender Ageyears Sleep_Hours_Schoolnight Sleep_Hours_Non_Schoolnight\n  <int> <chr>     <dbl>                   <dbl>                       <dbl>\n1     1 Female       16                     8                            13\n2     2 Male         17                     8                             9\n3     3 Female       19                     8                             7\n4     4 Male         17                     8                             9\n5     5 Male         16                     8.5                           5\n6     6 Female       11                    11                            12\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Difference Score\n\n::: {style=\"font-size: 30px\"}\nThis can be done in a couple different ways and sometimes you will see things computed this way:\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstate_school$sleep_diff <- state_school$Sleep_Hours_Schoolnight - state_school$Sleep_Hours_Non_Schoolnight\n```\n:::\n\n\n::: {style=\"font-size: 30px\"}\nHowever, we like the `tidyverse` so why don't we use the `mutate()` function\n\nAnd I always overdo things, so I am going to make a new dataset that only has the variables that I'm interested in (`sleep_state_school`)\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsleep_state_school <- state_school %>%    \n  mutate(sleep_diff = Sleep_Hours_Schoolnight - Sleep_Hours_Non_Schoolnight) %>%   \n  select(id, Gender, Ageyears, Sleep_Hours_Schoolnight,          Sleep_Hours_Non_Schoolnight, sleep_diff)  \n\nhead(sleep_state_school)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 √ó 6\n     id Gender Ageyears Sleep_Hours_Schoolni‚Ä¶¬π Sleep_Hours_Non_Scho‚Ä¶¬≤ sleep_diff\n  <int> <chr>     <dbl>                  <dbl>                  <dbl>      <dbl>\n1     1 Female       16                    8                       13       -5  \n2     2 Male         17                    8                        9       -1  \n3     3 Female       19                    8                        7        1  \n4     4 Male         17                    8                        9       -1  \n5     5 Male         16                    8.5                      5        3.5\n6     6 Female       11                   11                       12       -1  \n# ‚Ñπ abbreviated names: ¬π‚ÄãSleep_Hours_Schoolnight, ¬≤‚ÄãSleep_Hours_Non_Schoolnight\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Visualizing\n\nNow that we have our variable of interest, let's take a look at it!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep_state_school %>%   \n  ggplot(aes(sleep_diff)) +    \n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](lec-6_mean_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n## Doing the test in R: One Sample\n\nSince we have calculated the difference scores, we can basically just do a one-sample t-test with the `lsr` library\n\n\n::: {.cell}\n\n```{.r .cell-code}\noneSampleTTest(sleep_state_school$sleep_diff, mu = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One sample t-test \n\nData variable:   sleep_state_school$sleep_diff \n\nDescriptive statistics: \n            sleep_diff\n   mean         -1.866\n   std dev.      2.741\n\nHypotheses: \n   null:        population mean equals 0 \n   alternative: population mean not equal to 0 \n\nTest results: \n   t-statistic:  -9.106 \n   degrees of freedom:  178 \n   p-value:  <.001 \n\nOther information: \n   two-sided 95% confidence interval:  [-2.27, -1.462] \n   estimated effect size (Cohen's d):  0.681 \n```\n\n\n:::\n:::\n\n\n## Doing the test in R: Paired Sample\n\nMaybe we want to keep things separate and don't want to calculate separate values. We can use `pairedSamplesTTest()` instead!\n\nBut this isn't working and is making me mad...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlsr::pairedSamplesTTest(\n  formula = ~ Sleep_Hours_Schoolnight + Sleep_Hours_Non_Schoolnight,\n  data = sleep_state_school\n)\n```\n:::\n\n\n## Doing the test in R: Classic Edition\n\nAs you Google around to figure things out, you will likely see folks using `t.test()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(x = sleep_state_school$Sleep_Hours_Schoolnight,    \n       y = sleep_state_school$Sleep_Hours_Non_Schoolnight,\n       paired = TRUE )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  sleep_state_school$Sleep_Hours_Schoolnight and sleep_state_school$Sleep_Hours_Non_Schoolnight\nt = -9.1062, df = 178, p-value < 0.00000000000000022\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.270281 -1.461563\nsample estimates:\nmean difference \n      -1.865922 \n```\n\n\n:::\n:::\n\n\n## Reporting $t$-test\n\nThe first sentence usually conveys some descriptive information about the sample you were comparing (e.g., pre & post test).\n\nThen you identify the type of test you conducted and what was determined (be sure to include the \"stat block\" here as well with the t-statistic, df, p-value, CI and Effect size).\n\nFinish it up by putting that into person words and saying what that means.\n\n# YOUR TURN üíª\n\n# ANOVA (Analysis of Variance)\n\n## What is ANOVA? ([LSR Ch. 14](https://learningstatisticswithr.com/book/anova.html))\n\n::: incremental\n-   ANOVA stands for [***An***]{.underline}alysis [***o***]{.underline}f [***Va***]{.underline}riance\n-   Comparing means between two or more groups (usually 3 or more)\n    -   Continuous outcome and grouping variable with 2 or more levels\n-   Under the larger umbrella of general linear models\n    -   ANOVA is basically a regression with only categorical predictors\n-   Likely the most widely used tool in Psychology\n:::\n\n## Different Types of ANOVA\n\n-   ::: {.fragment .highlight-green}\n    One-Way ANOVA\n    :::\n\n-   Two-Way ANOVA\n\n-   Repeated Measures ANOVA\n\n-   ANCOVA\n\n-   MANOVA\n\n## One-Way ANOVA\n\nGoal: Inform of differences among the levels of our variable of interest (Omnibus Test)\n\n-   But cannot tell us more than that...\n\nHypotheses:\n\n$$ H_0: it\\: is\\: true\\: that\\: \\mu_1 = \\mu_2 = \\mu_3 =\\: ...\\mu_k  \\\\  H_1: it\\: is\\: \\boldsymbol{not}\\: true\\: that\\: \\mu_1 = \\mu_2 = \\mu_3 =\\: ...\\mu_k $$\n\n## Wait...Means or Variance?\n\nWe are using the variance to create a ratio (within group versus between group variance) to determine differences in means\n\n-   We are not directly investigating variance, but operationalize variance to create the ratio:\n\n$$ F_{df_b, \\: df_w} = \\frac{MS_{between}}{MS_{within}} $$\n\n------------------------------------------------------------------------\n\n![](/images/ANOVA.png){width=\"729\"}\n\n------------------------------------------------------------------------\n\n![](/images/ANOVA.png){width=\"729\"}\n\n::::: columns\n::: {.column width=\"50%\"}\n$F = \\frac{MS_{between}}{MS_{within}} = \\frac{small}{large} < 1$\n:::\n\n::: {.column width=\"50%\"}\n$F = \\frac{MS_{between}}{MS_{within}} = \\frac{large}{small} > 1$\n:::\n:::::\n\n## ANOVA: Assumptions\n\n::: {style=\"font-size: 30px\"}\n**Independence**\n\n-   Observations between and within groups should be independent. No autocorrelation\n\n**Homogeneity of Variance**\n\n-   The variances within each group should be roughly equal\n    -   Levene's test --\\> Welch's ANOVA for unequal variances\n\n**Normality**\n\n-   The data within each group should follow a normal distribution\n    -   Shapiro-Wilk test --\\> can transform the data or use non-parametric tests\n:::\n\n# NHST with ANOVA\n\n## Review of the NHST process\n\n::: incremental\n1.  Collect Sample and define hypotheses\n\n2.  Set alpha level\n\n3.  Determine the sampling distribution (will be using $F$-distribution now)\n\n4.  Identify the critical value\n\n5.  Calculate test statistic for sample collected\n\n6.  Inspect & compare statistic to critical value; Calculate probability\n:::\n\n## Steps to calculating F-ratio\n\n1.  Capture variance both between and within groups\n2.  Variance to Sum of Squares\n3.  Degrees of Freedom\n4.  Mean squares values\n5.  F-Statistic\n\n## Capturing Variance\n\nWe have calculated variance before!\n\n$$ Var = \\frac{1}{N}\\sum(x_i - \\bar{x})^2 $$\n\nNow we have to take into account the variance between and within the groups:\n\n$$ Var(Y) = \\frac{1}{N} \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y})^2 $$\n\n::: {style=\"font-size: 20px; text-align: center\"}\nNotice that we have the summation across each group ( $G$ ) and the person in the group ( $N_k$ )\n:::\n\n------------------------------------------------------------------------\n\n## Variance to Sum of Squares\n\n**Total Sum of Squares -** Adding up the sum of squares instead of getting the average (notice the removal of $\\frac{1}{N}$)\n\n$$ SS_{total} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y})^2 $$\n\nCan be broken up to see what is the variation ***between*** the groups AND the variation ***within*** the groups\n\n$$ SS_{total}=SS_{between}+SS_{within} $$\n\n::: {style=\"font-size: 20px; text-align: center\"}\nThis gets us closer to understanding the difference between means\n:::\n\n# Sum of Squares\n\n------------------------------------------------------------------------\n\n### Sum of Squares\n\n$$ SS_{total}=SS_{between}+SS_{within} $$\n\n![](/images/var_ss.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Example Data\n\n![](/images/table1.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Example Data\n\n![](/images/table2.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Example Data\n\n![](/images/table3.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Example Data\n\n![](/images/table4.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Between\n\nThe difference between the *group mean* and *grand mean*\n\n$$ SS_{between} = \\sum^G_{k=1}N_k(\\bar{Y_k} - \\bar{Y})^2 $$\n\n| Group  | Group Mean $\\bar{Y_k}$ | Grand Mean $\\bar{Y}$ |\n|:------:|:----------------------:|:--------------------:|\n|  Cool  |           32           |         41.8         |\n| Uncool |          56.5          |         41.8         |\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Between\n\nThe difference between the *group mean* and *grand mean*\n\n$$ SS_{between} = \\sum^G_{k=1}N_k(\\bar{Y_k} - \\bar{Y})^2 $$\n\n| Group | Group Mean $\\bar{Y_k}$ | Grand Mean $\\bar{Y}$ | Sq. Dev. | N | Weighted Sq. Dev. |\n|:--:|:--:|:--:|:--:|:--:|:--:|\n| Cool | 32 | 41.8 | 96.04 | 3 | 288.12 |\n| Uncool | 56.5 | 41.8 | 216.09 | 2 | 432.18 |\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Between\n\nThe difference between the *group mean* and *grand mean*\n\n$$ SS_{between} = \\sum^G_{k=1}N_k(\\bar{Y_k} - \\bar{Y})^2 $$\n\nNow we can sum the Weighted Squared Deviations together to get our Sum of Squares Between:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nssb <- 288.12 + 432.18 \nssb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 720.3\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Within\n\nThe difference between the [*individual*]{.underline} and their [*group mean*]{.underline}\n\n$$ SS_{within} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y_k})^2 $$\n\n|    Name    | Grumpiness $Y_{ik}$ | Group Mean $\\bar{Y_K}$ |\n|:----------:|:-------------------:|:----------------------:|\n|   Frodo    |         20          |           32           |\n|    Sam     |         55          |           32           |\n|   Bandit   |         21          |           32           |\n| Dolores U. |         91          |          56.5          |\n|   Dustin   |         22          |          56.5          |\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Within\n\nThe difference between the [*individual*]{.underline} and their [*group mean*]{.underline}\n\n$$ SS_{within} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y_k})^2 $$\n\n|    Name    | Grumpiness $Y_{ik}$ | Group Mean $\\bar{Y_K}$ | Sq. Dev |\n|:----------:|:-------------------:|:----------------------:|---------|\n|   Frodo    |         20          |           32           | 144     |\n|    Sam     |         55          |           32           | 529     |\n|   Bandit   |         21          |           32           | 121     |\n| Dolores U. |         91          |          56.5          | 1190.25 |\n|   Dustin   |         22          |          56.5          | 1190.25 |\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nscore <- c(20, 55, 21, 91, 22) \ngroup_m <- c(32, 32, 32, 56.5, 56.5) \nsq_dev <- (score - group_m)^2\n```\n:::\n\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Within\n\nThe difference between the [*individual*]{.underline} and their [*group mean*]{.underline}\n\n$$ SS_{within} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y_k})^2 $$ Now we can sum the Squared Deviations together to get our Sum of Squares Within:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(sq_dev)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3174.5\n```\n\n\n:::\n:::\n\n\n## Sum of Squares\n\nCan start to have an idea of what this looks like\n\n$$ SS_{between} = \\sum^G_{k=1}N_k(\\bar{Y_k} - \\bar{Y})^2 = 720.3 $$\n\n$$ SS_{within} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y_k})^2 = 3174.5 $$\n\nNext we have to take into account the degrees of freedom\n\n# Degrees of Freedom - ANOVA\n\n------------------------------------------------------------------------\n\n## Degrees of Freedom\n\nSince we have 2 types of variations that we are examining, this needs to be reflected in the degrees of freedom\n\n1.  Take the number of groups and subtract 1\\\n    $df_{between} = G - 1$\n\n2.  Take the total number of observations and subtract the number of groups\n\n    $df_{within} = N - G$\n\n# Mean Squares\n\n------------------------------------------------------------------------\n\n## Calculating Mean Squares\n\nNext we convert our summed squares value into a \"mean squares\"\n\nThis is done by dividing by the respective degrees of freedom\n\n$$ MS_b = \\frac{SS_b}{df_b} $$\n\n$$ MS_W = \\frac{SS_w}{df_w} $$\n\n------------------------------------------------------------------------\n\n## Calculating Mean Squares - Example\n\nLet's take a look at how this applies to our example: $$ MS_b = \\frac{SS_b}{G-1} = \\frac{720.3}{2-1} = 720.3 $$\n\n$$ MS_W = \\frac{SS_w}{N-G} = \\frac{3174.5}{5-2} = 1058.167  $$\n\n# $F$-Statistic\n\n------------------------------------------------------------------------\n\n## Calculating the F-Statistic\n\n$$F = \\frac{MS_b}{MS_w}$$\n\nIf the null hypothesis is true, $F$ has an expected value close to 1 (numerator and denominator are estimates of the same variability)\n\nIf it is false, the numerator will likely be larger, because systematic, between-group differences contribute to the variance of the means, but not to variance within group.\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata.frame(F = c(0,8)) %>%   \n  ggplot(aes(x = F)) +   \n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196),                  geom = \"line\") +   \n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196),                  geom = \"area\", xlim = c(2.65, 8), fill = \"purple\") +   geom_vline(aes(xintercept = 2.65), color = \"purple\") +   scale_y_continuous(\"Density\") + scale_x_continuous(\"F statistic\", breaks = NULL) +   theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](lec-6_mean_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\nIf data are normally distributed, then the variance is $\\chi^2$ distributed\n\n$F$-distributions are one-tailed tests. Recall that we're interested in how far away our test statistic from the null $(F = 1).$\n\n------------------------------------------------------------------------\n\n## Calculating F-statistic: Example\n\n$$F = \\frac{MS_b}{MS_w} = \\frac{720.3}{1058.167} = 0.68$$\n\n[Link](http://courses.atlas.illinois.edu/spring2016/STAT/STAT200/pf.html) to probability calculator\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata.frame(F = c(0,8)) %>%   \n  ggplot(aes(x = F)) +   \n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196),                  geom = \"line\") +   \n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196),                  geom = \"area\", xlim = c(2.65, 8), fill = \"purple\") +   geom_vline(aes(xintercept = 2.65), color = \"purple\") +\n  geom_vline(aes(xintercept = 0.68), color = \"red\") +    \n  annotate(\"text\",            \n           label = \"F=0.68\",             \n           x = 1.1, y = 0.65, size = 8, color = \"red\") +\n  scale_y_continuous(\"Density\") + \n  scale_x_continuous(\"F statistic\", breaks = NULL) +   \n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](lec-6_mean_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\nWhat can we conclude?\n\n------------------------------------------------------------------------\n\n## Contrasts/Post-Hoc Tests\n\nPerformed when there is a significant difference among the groups to examine which groups are different\n\n1.  **Contrasts**: When we have *a priori* hypotheses\n2.  **Post-hoc Tests**: When we want to test everything\n\n# Reporting Results\n\n------------------------------------------------------------------------\n\n## Tables\n\nOften times the output will be in the form of a table and then it is often reported this way in the manuscript\n\n| Source of Variation | df | Sum of Squares | Mean Squares | F-statistic | p-value |\n|:--:|:--:|:--:|:--:|:--:|:--:|\n| Group | $G-1$ | $SS_b$ | $MS_b = \\frac{SS_b}{df_b}$ | $F = \\frac{MS_b}{MS_w}$ | $p$ |\n| Residual | $N-G$ | $SS_w$ | $MS_w = \\frac{SS_w}{df_w}$ |  |  |\n| Total | $N-1$ | $SS_{total}$ |  |  |  |\n\n------------------------------------------------------------------------\n\n## In-Text\n\n> A one-way analysis of variance was used to test for differences in the \\[variable of interest/outcome variable\\] as a function of \\[whatever the factor is\\]. Specifically, differences in \\[variable of interest\\] were assessed for the \\[list different levels and be sure to include (M= , SD= )\\] . The one-way ANOVA revealed a significant/nonsignificant effect of \\[factor\\] on scores on the \\[variable of interest\\] (F(dfb, dfw) = f-ratio, p = p-value, Œ∑2 = effect size).\n>\n> Planned comparisons were conducted to compare expected differences among the \\[however many groups\\] means. Planned contrasts revealed that participants in the \\[one of the conditions\\] had a greater/fewer \\[variable of interest\\] and then include the p-value. This same type of sentence is repeated for whichever contrasts you completed. Descriptive statistics were reported in Table 1.\n\n# One-Way ANOVA in R\n\n`Continuous_Variable ~ Group_Variable`\n\n------------------------------------------------------------------------\n\n## Books by Marital Status\n\nWe can examine how many books (continuous) by marital status (7 categories: Married, Divorced, In a relationship, Other, Separated, Widowed, Single)\n\nVISUALIZE!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggbetweenstats(\n  data  = cah,\n  x     = marital_status,\n  y     = books,\n  title = \"Distribution of books across marital status\"\n)\n```\n\n::: {.cell-output-display}\n![](lec-6_mean_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Running the ANOVA\n\nUse the same way we build a model and then get the summary of that model\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov_mar <- aov(books ~ marital_status, \n               data = cah)\n\nsummary(aov_mar)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Df  Sum Sq Mean Sq F value  Pr(>F)   \nmarital_status   6  124027   20671   3.624 0.00146 **\nResiduals      963 5492561    5704                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n30 observations deleted due to missingness\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Post-hoc Tests\n\n1.  Examine the basic summary statistics\n2.  Do pairwise comparisons between each of the groups, based on the model we created\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#get basic summary stats\ncah %>% \n  group_by(marital_status) %>% \n  summarise(mean = mean(books, na.rm = TRUE), \n            sd = sd(books, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 √ó 3\n  marital_status     mean    sd\n  <chr>             <dbl> <dbl>\n1 Divorced          56.9  165. \n2 In a relationship 13.6   25.8\n3 Married           18.3   59.5\n4 Other             50     NA  \n5 Separated          7.75  14.3\n6 Single            20.4   76.1\n7 Widowed           30.7   59.0\n8 <NA>              14     12.3\n```\n\n\n:::\n\n```{.r .cell-code}\n# conduct the comparisons\nemmeans(aov_mar, pairwise ~ marital_status, \n        adjust = \"none\") \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$emmeans\n marital_status    emmean    SE  df lower.CL upper.CL\n Divorced           56.87  8.29 963    40.61     73.1\n In a relationship  13.61  8.05 963    -2.19     29.4\n Married            18.28  3.36 963    11.68     24.9\n Other              50.00 75.50 963   -98.21    198.2\n Separated           7.75 18.90 963   -29.30     44.8\n Single             20.37  5.27 963    10.01     30.7\n Widowed            30.67  8.84 963    13.32     48.0\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate    SE  df t.ratio p.value\n Divorced - In a relationship     43.26 11.60 963   3.744  0.0002\n Divorced - Married               38.59  8.95 963   4.314  <.0001\n Divorced - Other                  6.87 76.00 963   0.090  0.9279\n Divorced - Separated             49.12 20.60 963   2.382  0.0174\n Divorced - Single                36.51  9.83 963   3.716  0.0002\n Divorced - Widowed               26.20 12.10 963   2.162  0.0308\n In a relationship - Married      -4.67  8.73 963  -0.535  0.5929\n In a relationship - Other       -36.39 76.00 963  -0.479  0.6320\n In a relationship - Separated     5.86 20.50 963   0.286  0.7752\n In a relationship - Single       -6.75  9.62 963  -0.702  0.4831\n In a relationship - Widowed     -17.06 12.00 963  -1.427  0.1540\n Married - Other                 -31.72 75.60 963  -0.420  0.6749\n Married - Separated              10.53 19.20 963   0.549  0.5831\n Married - Single                 -2.09  6.26 963  -0.333  0.7389\n Married - Widowed               -12.39  9.46 963  -1.310  0.1904\n Other - Separated                42.25 77.80 963   0.543  0.5874\n Other - Single                   29.63 75.70 963   0.391  0.6956\n Other - Widowed                  19.33 76.00 963   0.254  0.7994\n Separated - Single              -12.62 19.60 963  -0.644  0.5200\n Separated - Widowed             -22.92 20.80 963  -1.099  0.2718\n Single - Widowed                -10.31 10.30 963  -1.001  0.3170\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Family-wise error\n\nThese pairwise comparisons can quickly grow in number as the number of Groups increases. With 3 (k) Groups, we have k(k-1)/2 = 3 possible pairwise comparisons.\n\nAs the number of groups in the ANOVA grows, the number of possible pairwise comparisons increases dramatically.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec-6_mean_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nAs the number of tests grows, and assuming the null hypothesis is true, the probability that we will make one or more Type I errors increases. To approximate the magnitude of the problem, we can assume that the multiple pairwise comparisons are independent. The probability that we **don't** make a Type I error for [**one**]{.underline} test is:\n\n$$P(\\text{No Type I}, 1 \\text{ test}) = 1-\\alpha$$\n\n------------------------------------------------------------------------\n\nThe probability that we don't make a Type I error for [**two**]{.underline} tests is:\n\n$$P(\\text{No Type I}, 2 \\text{ test}) = (1-\\alpha)(1-\\alpha)$$\n\nFor C tests, the probability that we make **no** Type I errors is\n\n$$P(\\text{No Type I}, C \\text{ tests}) = (1-\\alpha)^C$$\n\nWe can then use the following to calculate the probability that we make one or more Type I errors in a collection of C independent tests.\n\n$$P(\\text{At least one Type I}, C \\text{ tests}) = 1-(1-\\alpha)^C$$\n\n------------------------------------------------------------------------\n\nThe Type I error inflation that accompanies multiple comparisons motivates the large number of \"correction\" procedures that have been developed.\n\n\n::: {.cell fight.height='6'}\n::: {.cell-output-display}\n![](lec-6_mean_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nMultiple comparisons, each tested with $\\alpha_{per-test}$, increases the family-wise $\\alpha$ level.\n\n$$\\large \\alpha_{family-wise} = 1 - (1-\\alpha_{per-test})^C$$ ≈†id√°k showed that the family-wise a could be controlled to a desired level (e.g., .05) by changing the $\\alpha_{per-test}$ to:\n\n$$\\large \\alpha_{per-wise} = 1 - (1-\\alpha_{family-wise})^{\\frac{1}{C}}$$\n\n------------------------------------------------------------------------\n\n### Bonferroni\n\nBonferroni (and Dunn, and others) suggested this simple approximation:\n\n$$\\large \\alpha_{per-test} = \\frac{\\alpha_{family-wise}}{C}$$\n\nThis is typically called the Bonferroni correction and is very often used even though better alternatives are available.\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(aov_mar, pairwise ~ marital_status,          adjust = \"bonferroni\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$emmeans\n marital_status    emmean    SE  df lower.CL upper.CL\n Divorced           56.87  8.29 963    40.61     73.1\n In a relationship  13.61  8.05 963    -2.19     29.4\n Married            18.28  3.36 963    11.68     24.9\n Other              50.00 75.50 963   -98.21    198.2\n Separated           7.75 18.90 963   -29.30     44.8\n Single             20.37  5.27 963    10.01     30.7\n Widowed            30.67  8.84 963    13.32     48.0\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate    SE  df t.ratio p.value\n Divorced - In a relationship     43.26 11.60 963   3.744  0.0040\n Divorced - Married               38.59  8.95 963   4.314  0.0004\n Divorced - Other                  6.87 76.00 963   0.090  1.0000\n Divorced - Separated             49.12 20.60 963   2.382  0.3654\n Divorced - Single                36.51  9.83 963   3.716  0.0045\n Divorced - Widowed               26.20 12.10 963   2.162  0.6478\n In a relationship - Married      -4.67  8.73 963  -0.535  1.0000\n In a relationship - Other       -36.39 76.00 963  -0.479  1.0000\n In a relationship - Separated     5.86 20.50 963   0.286  1.0000\n In a relationship - Single       -6.75  9.62 963  -0.702  1.0000\n In a relationship - Widowed     -17.06 12.00 963  -1.427  1.0000\n Married - Other                 -31.72 75.60 963  -0.420  1.0000\n Married - Separated              10.53 19.20 963   0.549  1.0000\n Married - Single                 -2.09  6.26 963  -0.333  1.0000\n Married - Widowed               -12.39  9.46 963  -1.310  1.0000\n Other - Separated                42.25 77.80 963   0.543  1.0000\n Other - Single                   29.63 75.70 963   0.391  1.0000\n Other - Widowed                  19.33 76.00 963   0.254  1.0000\n Separated - Single              -12.62 19.60 963  -0.644  1.0000\n Separated - Widowed             -22.92 20.80 963  -1.099  1.0000\n Single - Widowed                -10.31 10.30 963  -1.001  1.0000\n\nP value adjustment: bonferroni method for 21 tests \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThe Bonferroni procedure is conservative. Other correction procedures have been developed that control family-wise Type I error at .05 but that are more powerful than the Bonferroni procedure. The most common one is the Holm procedure.\n\nThe Holm procedure does not make a constant adjustment to each per-test $\\alpha$. Instead it makes adjustments in stages depending on the relative size of each pairwise p-value.\n\n------------------------------------------------------------------------\n\n### Holm correction\n\n1.  Rank order the p-values from largest to smallest.\n2.  Start with the smallest p-value. Multiply it by its rank.\n3.  Go to the next smallest p-value. Multiply it by its rank. If the result is larger than the adjusted p-value of next smallest rank, keep it. Otherwise replace with the previous step adjusted p-value.\n4.  Repeat Step 3 for the remaining p-values.\n5.  Judge significance of each new p-value against $\\alpha = .05$.\n\n------------------------------------------------------------------------\n\n\n| Original p value| Rank| Rank x p|   Holm| Bonferroni|\n|----------------:|----:|--------:|------:|----------:|\n|           0.0012|    6|   0.0072| 0.0072|     0.0072|\n|           0.0023|    5|   0.0115| 0.0115|     0.0138|\n|           0.0450|    4|   0.1800| 0.1800|     0.2700|\n|           0.0470|    3|   0.1410| 0.1800|     0.2820|\n|           0.0530|    2|   0.1060| 0.1800|     0.3180|\n|           0.2100|    1|   0.2100| 0.2100|     1.0000|\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(aov_mar, pairwise ~ marital_status,          adjust = \"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$emmeans\n marital_status    emmean    SE  df lower.CL upper.CL\n Divorced           56.87  8.29 963    40.61     73.1\n In a relationship  13.61  8.05 963    -2.19     29.4\n Married            18.28  3.36 963    11.68     24.9\n Other              50.00 75.50 963   -98.21    198.2\n Separated           7.75 18.90 963   -29.30     44.8\n Single             20.37  5.27 963    10.01     30.7\n Widowed            30.67  8.84 963    13.32     48.0\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate    SE  df t.ratio p.value\n Divorced - In a relationship     43.26 11.60 963   3.744  0.0038\n Divorced - Married               38.59  8.95 963   4.314  0.0004\n Divorced - Other                  6.87 76.00 963   0.090  1.0000\n Divorced - Separated             49.12 20.60 963   2.382  0.3132\n Divorced - Single                36.51  9.83 963   3.716  0.0041\n Divorced - Widowed               26.20 12.10 963   2.162  0.5244\n In a relationship - Married      -4.67  8.73 963  -0.535  1.0000\n In a relationship - Other       -36.39 76.00 963  -0.479  1.0000\n In a relationship - Separated     5.86 20.50 963   0.286  1.0000\n In a relationship - Single       -6.75  9.62 963  -0.702  1.0000\n In a relationship - Widowed     -17.06 12.00 963  -1.427  1.0000\n Married - Other                 -31.72 75.60 963  -0.420  1.0000\n Married - Separated              10.53 19.20 963   0.549  1.0000\n Married - Single                 -2.09  6.26 963  -0.333  1.0000\n Married - Widowed               -12.39  9.46 963  -1.310  1.0000\n Other - Separated                42.25 77.80 963   0.543  1.0000\n Other - Single                   29.63 75.70 963   0.391  1.0000\n Other - Widowed                  19.33 76.00 963   0.254  1.0000\n Separated - Single              -12.62 19.60 963  -0.644  1.0000\n Separated - Widowed             -22.92 20.80 963  -1.099  1.0000\n Single - Widowed                -10.31 10.30 963  -1.001  1.0000\n\nP value adjustment: holm method for 21 tests \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Tukey HSD\n\nUsed to compare **all groups** to each other (so all possible comparisons of 2 groups)\n\nTypically the most common\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(aov_mar, pairwise ~ marital_status,          adjust = \"tukey\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$emmeans\n marital_status    emmean    SE  df lower.CL upper.CL\n Divorced           56.87  8.29 963    40.61     73.1\n In a relationship  13.61  8.05 963    -2.19     29.4\n Married            18.28  3.36 963    11.68     24.9\n Other              50.00 75.50 963   -98.21    198.2\n Separated           7.75 18.90 963   -29.30     44.8\n Single             20.37  5.27 963    10.01     30.7\n Widowed            30.67  8.84 963    13.32     48.0\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate    SE  df t.ratio p.value\n Divorced - In a relationship     43.26 11.60 963   3.744  0.0036\n Divorced - Married               38.59  8.95 963   4.314  0.0004\n Divorced - Other                  6.87 76.00 963   0.090  1.0000\n Divorced - Separated             49.12 20.60 963   2.382  0.2071\n Divorced - Single                36.51  9.83 963   3.716  0.0040\n Divorced - Widowed               26.20 12.10 963   2.162  0.3173\n In a relationship - Married      -4.67  8.73 963  -0.535  0.9983\n In a relationship - Other       -36.39 76.00 963  -0.479  0.9991\n In a relationship - Separated     5.86 20.50 963   0.286  1.0000\n In a relationship - Single       -6.75  9.62 963  -0.702  0.9925\n In a relationship - Widowed     -17.06 12.00 963  -1.427  0.7874\n Married - Other                 -31.72 75.60 963  -0.420  0.9996\n Married - Separated              10.53 19.20 963   0.549  0.9981\n Married - Single                 -2.09  6.26 963  -0.333  0.9999\n Married - Widowed               -12.39  9.46 963  -1.310  0.8473\n Other - Separated                42.25 77.80 963   0.543  0.9982\n Other - Single                   29.63 75.70 963   0.391  0.9997\n Other - Widowed                  19.33 76.00 963   0.254  1.0000\n Separated - Single              -12.62 19.60 963  -0.644  0.9953\n Separated - Widowed             -22.92 20.80 963  -1.099  0.9283\n Single - Widowed                -10.31 10.30 963  -1.001  0.9538\n\nP value adjustment: tukey method for comparing a family of 7 estimates \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Tukey HSD another way\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(aov_mar)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = books ~ marital_status, data = cah)\n\n$marital_status\n                                  diff        lwr        upr     p adj\nIn a relationship-Divorced  -43.259858  -77.40177  -9.117947 0.0036187\nMarried-Divorced            -38.593732  -65.02603 -12.161436 0.0003541\nOther-Divorced               -6.873494 -231.34992 217.602937 1.0000000\nSeparated-Divorced          -49.123494 -110.04754  11.800548 0.2071415\nSingle-Divorced             -36.507640  -65.53787  -7.477414 0.0040209\nWidowed-Divorced            -26.202261  -62.00630   9.601774 0.3172699\nMarried-In a relationship     4.666126  -21.11337  30.445620 0.9983277\nOther-In a relationship      36.386364 -188.01414 260.786863 0.9991040\nSeparated-In a relationship  -5.863636  -66.50731  54.780036 0.9999557\nSingle-In a relationship      6.752217  -21.68491  35.189343 0.9925145\nWidowed-In a relationship    17.057597  -18.26725  52.382446 0.7873530\nOther-Married                31.720238 -191.63728 255.077755 0.9995816\nSeparated-Married           -10.529762  -67.19237  46.132847 0.9980605\nSingle-Married                2.086092  -16.39813  20.570309 0.9998901\nWidowed-Married              12.391471  -15.55206  40.335007 0.8472908\nSeparated-Other             -42.250000 -272.25359 187.753593 0.9981827\nSingle-Other                -29.634146 -253.31398 194.045687 0.9997201\nWidowed-Other               -19.328767 -243.98816 205.330626 0.9999778\nSingle-Separated             12.615854  -45.30425  70.535962 0.9953161\nWidowed-Separated            22.921233  -38.67352  84.515988 0.9283177\nWidowed-Single               10.305379  -20.10727  40.718024 0.9537681\n```\n\n\n:::\n:::\n\n\n# ANOVA is regression\n\nIn regression, we can accommodate categorical predictors. How does this compare to ANOVA?\n\n-   Same omnibus test of the model!\n\n\\*(Really the same model, but packaged differently.)\n\n-   When would you use one versus the other?\n\n------------------------------------------------------------------------\n\n::::: columns\n::: {.column width=\"50%\"}\n**ANOVA**\n\n-   More traditional for 3+ groups\n\n-   Comparing/controlling multiple categorical variables\n:::\n\n::: {.column width=\"50%\"}\n**Regression**\n\n-   Best for two groups\n\n-   Incorporating continuous predictors too\n\n-   Good for 3+ groups when you have more specific hypotheses (contrasts)\n:::\n:::::\n\n# Two-Way ANOVA\n\n------------------------------------------------------------------------\n\n## What is a Two-Way ANOVA?\n\nExamines the impact of ***2*** nominal/categorical variables on a continuous outcome\n\nWe can now examine:\n\n-   The impact of variable 1 on the outcome (Main Effect)\n\n-   The impact of variable 2 on the outcome (Main Effect)\n\n-   The *interaction*¬†of variable 1 & 2 on the outcome (Interaction Effect)\n\n    ::: incremental\n    -   The effect of variable 1 ***depends*** on the level of variable 2\n    :::\n\n------------------------------------------------------------------------\n\n------------------------------------------------------------------------\n\n## Main Effect & Interactions\n\nMain Effect: Basically a one-way ANOVA\n\n-   The effect of variable 1 is the same across all levels of variable 2\n\nInteraction:\n\n-   Able to examine the effect of variable 1 across different levels of variable 2\n\n-   Basically speaking, **the effect of variable 1 on our outcome *DEPENDS on the levels of variable 2***\n",
    "supporting": [
      "lec-6_mean_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
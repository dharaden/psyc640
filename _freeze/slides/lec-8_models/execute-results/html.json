{
  "hash": "7031178378b05a0cf9632df9d7fa5f5c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 08: Model Selection & Variability\"\nsubtitle: \"Date: October 19, 2025\"\nfooter:  \"[course-website](https://dharaden.github.io/psyc640/)\"\nlogo: \"images/640_hex.png\"\nformat: \n  revealjs:\n    theme: clean.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    overview: false\n    scrollable: true\n    code-line-numbers: true\neditor: visual\nexecute:\n  echo: true\n  freeze: auto\n---\n\n## Today...\n\nRegression & Model Selection\n\n![](/images/linear_regression_2x.png){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Don't know if I'm using all of these, but including theme here anyways\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(broom)\nlibrary(psych)\n\n\n#Remove Scientific Notation \noptions(scipen=999)\n```\n:::\n\n\n------------------------------------------------------------------------\n\n### Regressions\n\nWith regression, we are ***building a model*** that we think best represents the data, and the broader world\n\n$$\nData = Model + error\n$$\n\n------------------------------------------------------------------------\n\n### Regression Equation\n\nOverall, we are providing a model to give us a \"best guess\" on predicting our outcome\n\n$$\nY_i = b_0 + b_1X_i + e_i\n$$\n\nThis equation is capturing how we are able to calculate each observation ( $Y_i$ )\n\n| Term | Meaning |\n|:----:|---------|\n|  \\$  |         |\n\nb_0 \\$ \\| Intercept (when X = 0) \\| \\| $b_1$ \\| Slope - for every 1 unit change in X, there are $b_1$ unit change in Y \\| \\| $X_i$ \\| Predictor Variable \\| \\| $e_i$ \\| Error/Residuals \\|\n\n------------------------------------------------------------------------\n\n### Unstandardized vs. Standardized\n\n::: r-stack\n***What is the Difference?*** (other than the runes)\n:::\n\nUnstandardized:\n\n$$\nY_i = b_0 + b_1X_i + e_i\n$$\n\nStandardized:\n\n$$\nY_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\n$$\n\n$$\nZ_y = \\beta_0 + \\beta_1Z_x + \\epsilon_i\n$$\n\n------------------------------------------------------------------------\n\n## Example (by hand)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(gapminder)\n#gapminder %<>% filter(year == 2007 & continent == #\"Asia\") %>% \n#  mutate(log_gdp = log(gdpPercap))\n#\n#gapminder %>% \n#  select(log_gdp, lifeExp) %>% \n#  describe()\n#\n#cor(gapminder$log_gdp, gapminder$lifeExp)\n```\n:::\n\n\n------------------------------------------------------------------------\n\nIf we regress lifeExp onto log_gdp:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#r = cor(gapminder$log_gdp, gapminder$lifeExp)\n#m_log_gdp = mean(gapminder$log_gdp)\n#m_lifeExp = mean(gapminder$lifeExp)\n#s_log_gdp = sd(gapminder$log_gdp)\n#s_lifeExp = sd(gapminder$lifeExp)\n#\n#b1 = r*(s_lifeExp/s_log_gdp)\n#b0 = m_lifeExp - b1*m_log_gdp\n```\n:::\n\n\n------------------------------------------------------------------------\n\n## In `R`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#fit.1 <- lm(lifeExp ~ log_gdp, data = gapminder)\n#summary(fit.1)\n```\n:::\n\n\n::: notes\n**Things to discuss**\n\n-   Coefficient estimates\n-   Statistical tests (covered in more detail soon)\n:::\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n:::\n\n\n------------------------------------------------------------------------\n\n## Another Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschool <- read_csv(\"https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/data/example2-chisq.csv\") %>% \n  mutate(Sleep_Hours_Non_Schoolnight = as.numeric(Sleep_Hours_Non_Schoolnight)) %>% \n  filter(Sleep_Hours_Non_Schoolnight < 24) #removing impossible values\n```\n:::\n\n\n------------------------------------------------------------------------\n\n## Statistical Inference\n\n-   The way the world is = our model + error\n\n-   How good is our model? Does it \"fit\" the data well?\n\n. . .\n\nTo assess how well our model fits the data, we will examine the proportion of variance in our outcome variable that can be \"explained\" by our model.\n\nTo do so, we need to partition the variance into different categories. For now, we will partition it into two categories: the variability that is captured by (explained by) our model, and variability that is not.\n\n------------------------------------------------------------------------\n\n## Partitioning variation\n\nLet's start with the formula defining the relationship between observed $Y$ and fitted $\\hat{Y}$:\n\n$$Y_i = \\hat{Y}_i + e_i$$\n\n. . .\n\nOne of the properties that we love about variance is that variances are additive when two variables are independent. In other words, if we create some variable, C, by adding together two other variables, A and B, then the variance of C is equal to the sum of the variances of A and B.\n\n. . .\n\nWhy can we use that rule in this case?\n\n::: notes\nStudents must recognize that Y-hat and e are uncorrelated, they must be the way that we've built the OLS function.\n:::\n\n------------------------------------------------------------------------\n\n## Partitioning variation\n\n::::: columns\n::: {.column width=\"50%\"}\n[$\\hat{Y}_i$ and $e_i$ must be independent from each other. Thus, the variance of $Y$ is equal to the sum of the variance of $\\hat{Y}$ and $e$.]{style=\"font-size:80%\"}\n\n$$\\large s^2_Y = s^2_{\\hat{Y}} + s^2_{e}$$\n:::\n\n::: {.column width=\"50%\"}\n[Recall that variances are sums of squares divided by N-1. Thus, all variances have the same sample size, so we can also note the following:]{style=\"font-size:80%\"}\n\n$$\\large SS_Y = SS_{\\hat{Y}} + SS_{\\text{e}}$$\n:::\n:::::\n\n------------------------------------------------------------------------\n\n[A quick note about terminology: I demonstrated these calculations using $Y$, $\\hat{Y}$ and $e$. However, you may also see the same terms written differently, to more clearly indicate the source of the variance...]{style=\"font-size:80%\"}\n\n$$ SS_Y = SS_{\\hat{Y}} + SS_{\\text{e}}$$ $$ SS_Y = SS_{\\text{Model}} + SS_{\\text{Residual}}$$\n\n[The relative magnitudes of sums of squares provides a way of identifying particularly large and important sources of variability.]{style=\"font-size:80%\"}\n\n------------------------------------------------------------------------\n\n## Coefficient of Determination\n\n$$\\Large \\frac{SS_{Model}}{SS_{Y}} = \\frac{s_{Model}^2}{s_{y}^2} = R^2$$\n\n$R^2$ represents the proportion of variance in Y that is explained by the model.\n\n. . .\n\n$\\sqrt{R^2} = R$ is the correlation between the predicted values of Y from the model and the actual values of Y\n\n$$\\large \\sqrt{R^2} = r_{Y\\hat{Y}}$$\n\n------------------------------------------------------------------------\n\n### Example\n\n\n::: {.cell highlight-output='10'}\n\n```{.r .cell-code}\nfit.1 <- lm(Sleep_Hours_Non_Schoolnight ~ Ageyears, \n           data = school)\nsummary(fit.1) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Sleep_Hours_Non_Schoolnight ~ Ageyears, data = school)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3947 -0.7306  0.3813  1.2694  4.5974 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(>|t|)    \n(Intercept) 10.52256    0.90536  11.623 <0.0000000000000002 ***\nAgeyears    -0.11199    0.05887  -1.902              0.0585 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.204 on 204 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.01743,\tAdjusted R-squared:  0.01261 \nF-statistic: 3.619 on 1 and 204 DF,  p-value: 0.05854\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_info <- augment(fit.1)\nsummary(fit.1)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01742975\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschool %>% \n  ggplot(aes(x = Ageyears, y = Sleep_Hours_Non_Schoolnight)) + \n  geom_point() + geom_smooth(method = \"lm\", se = F)\n```\n\n::: {.cell-output-display}\n![](lec-8_models_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Example\n\nThe correlation between X and Y is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(school$Sleep_Hours_Non_Schoolnight, school$Ageyears, use = \"pairwise\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.1320218\n```\n\n\n:::\n:::\n\n\n. . .\n\nIf we square the correlation, we get:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(school$Sleep_Hours_Non_Schoolnight, school$Ageyears, use = \"pairwise\")^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01742975\n```\n\n\n:::\n:::\n\n\n. . .\n\nTa da!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit.1)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01742975\n```\n\n\n:::\n:::\n\n\n# Example in `R`\n\nTry some live coding! Also known as \"Another opportunity for Dr. Haraden to potentially embarrass himself\"\n\n<https://archive.ics.uci.edu/dataset/320/student+performance>\n\n<https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success>\n\n<https://datahub.io/collections>\n\n<https://www.kaggle.com/datasets>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}